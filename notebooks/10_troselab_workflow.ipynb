{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86cc1ca7",
   "metadata": {},
   "source": [
    "# Daily workflow - troselab (DRAFT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eaa674c",
   "metadata": {},
   "source": [
    "`TR TODO: \n",
    "\n",
    "4. Scan Curation\n",
    "    1. DIFFERENT NOTEBOOK (or different Notebook section)\n",
    "    2. Make intersection of folder with database\n",
    "        1. list _all_ scans\n",
    "        2. have all non-curated red - the others green\n",
    "        3. bhave non-processed scans grey\n",
    "        4. have 'CURATE LOCALLY WITH s2P' buttons behind all scans. This button should spawn a local suite2p GUI which directly loads the respective suite2p stats.npy file from the server. Folder settings for server need to be set in datajoint.json file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CURRENT TODO\n",
    "\n",
    "***FIX SAMESITE LOADING BUG AFTER SESSION INGEST***\n",
    "1. Find better way to prevent multi-extractions with scan sets (samesite and samesessions)\n",
    "2. AUX ingest and EVENTS!!!! https://github.com/SFB1089/adamacs/pull/24 https://troselab.slack.com/archives/C02GR5XE47R/p1661457553581559 \n",
    "3. remove main trigger from bpod and record tracking gate. That event can then be used to match timestamps globally (simple offset)\n",
    "4. check the surgery table. Maybe an optional GUI with at least virus injection would be helpful. But low prio.\n",
    "5. Write a cronjob that polls the population tables every XY minutes - ideally combine with backup script so the task can run on local data that is then pushed back to tatchu.\n",
    "6. SERVER: Database backup and Data folder backup\n",
    "    1. Have cronjob backup database to ana2 and isilon (midnight, incremental - only add, no delete. Increment ID to be reversible?)\n",
    "    2. Have cronjob backup data folders to ana2 and isilon (midnight, incremental - only add, no delete)\n",
    "    3. Use second server as worker\n",
    "7. Think of project splitting \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "634ec7b4",
   "metadata": {},
   "source": [
    "## Login\n",
    "\n",
    "Either log in via a local config file (see [01_pipeline](./01_pipeline.ipynb)), or enter login information manually. If you are don't have your login information, contact the administrator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b1bebf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-17 10:44:32,045][INFO]: Connecting tobiasr@172.26.128.53:3306\n",
      "[2023-05-17 10:44:32,092][INFO]: Connected tobiasr@172.26.128.53:3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# change to the upper level folder to detect dj_local_conf.json\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='adamacs', (\"Please move to the main directory\")\n",
    "from adamacs.pipeline import subject, session, equipment, surgery, event, trial, imaging\n",
    "from adamacs.ingest import session as isess\n",
    "from adamacs.helpers import stack_helpers as sh\n",
    "from adamacs.ingest import behavior as ibe\n",
    "import datajoint as dj\n",
    "from rspace_client.eln import eln\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dj.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7042be7d",
   "metadata": {},
   "source": [
    "### RSpace connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5db12e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'OK', 'rspaceVersion': '1.80.1'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL=dj.config['custom'].get('rspace_URL')\n",
    "API_KEY=dj.config['custom'].get('rspace_API_key')\n",
    "api = eln.ELNClient(URL, API_KEY)\n",
    "api.get_status()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d29453ed",
   "metadata": {},
   "source": [
    "## Activation\n",
    "Next, import from `adamacs.pipeline` to activate the relevant schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27dcb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamacs.utility import *\n",
    "# from adamacs.nbgui import *\n",
    "from adamacs.pipeline import subject, session, surgery, scan, equipment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cd8be94",
   "metadata": {},
   "source": [
    "Assign easy names for relevant tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30485290",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub, lab, protocol, line, mutation, user, project, subject_genotype, subject_death = (\n",
    "    subject.Subject(), subject.Lab(), subject.Protocol(), subject.Line(), \n",
    "    subject.Mutation(), subject.User(), subject.Project(), subject.SubjectGenotype(), \n",
    "    subject.SubjectDeath()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d21a66a",
   "metadata": {},
   "source": [
    "## Cleaning. Use with caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d06a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject.Subject.delete()\n",
    "# session.Session.delete()\n",
    "# imaging.Processing.delete()\n",
    "# imaging.Curation.delete()\n",
    "# event.Event.delete()\n",
    "# event.BehaviorRecording.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c9c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.Session.drop()\n",
    "# scan.Scan.drop()\n",
    "# imaging.Processing.drop()\n",
    "# imaging.Curation.drop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "807a36ef",
   "metadata": {},
   "source": [
    "## 1. Data upload\n",
    "(not implemented currently)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d83b5668",
   "metadata": {},
   "source": [
    "## 2. Session ingest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2fabde",
   "metadata": {},
   "source": [
    "### Define ingest GUI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8164fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tobias Rose 2023: Routine ingest helpers\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from natsort import natsorted, ns\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from adamacs.helpers import stack_helpers as sh\n",
    "\n",
    "def select_sessions(AvailableSessionDirB, do_population = False):\n",
    "    \n",
    "    # Personal default values\n",
    "    user_defaults = get_user_defaults(AvailableSessionDirB)\n",
    "\n",
    "    # extract lookup tables\n",
    "    Project = project.fetch('project')\n",
    "    Equipment = equipment.Equipment().fetch('scanner')\n",
    "    Recording_Location = surgery.AnatomicalLocation().fetch('anatomical_location')\n",
    "    SessionNotes = session.SessionNote.fetch('session_note')\n",
    "    s2pparm = imaging.ProcessingParamSet.fetch(\"paramset_idx\", \"paramset_desc\")\n",
    "    session_dirs_ingested = session.SessionDirectory.fetch('session_dir')\n",
    "    scan_dirs_ingested = scan.ScanPath.fetch('path')\n",
    "    IngestedSessionDirA = get_session_dir_key_from_dir(scan_dirs_ingested)\n",
    "    # ScanDirArrayingested = get_scan_dir_key_from_dir(scan_dirs_ingested)\n",
    "   \n",
    "    \n",
    "    # Define the widgets\n",
    "    session_dropdowns = []\n",
    "    session_checkboxes = []\n",
    "    for i, session_list in enumerate(AvailableSessionDirB):\n",
    "        # Dropdowns for Project, Recording Location, Equipment\n",
    "\n",
    "        current_session = get_session_key_from_dir([session_list])\n",
    "        current_subject = get_subject_key_from_dir([session_list])\n",
    "\n",
    "        # check if directory session is already ingested - if yes: populate GUI with table values. If no: use user defaults\n",
    "        query = session.Session() & f'session_id = \"{current_session[0]}\"'\n",
    "        count = len(query.fetch('session_id'))\n",
    "\n",
    "        # get all sessions of current animal that are already ingested\n",
    "        subject_sessions = session.SessionSameSite().proj(\"session_id\") * session.Session.proj(\"subject\") & f'subject = \"{current_subject[0]}\"'\n",
    "        subject_sessions_array = subject_sessions.fetch(\"session_id\")\n",
    "\n",
    "        if count > 0:\n",
    "            # POPULATE PRESELECTION FROM DATABASE\n",
    "            # get the project associated with a session\n",
    "            query = session.ProjectSession() & f'session_id = \"{current_session[0]}\"'\n",
    "            project_dropdown_value = query.fetch(\"project\")\n",
    "            # get the location associated with a session\n",
    "            query = session.Session() * scan.ScanLocation() & f'session_id = \"{current_session[0]}\"'\n",
    "            location_dropdown_value = query.fetch(\"anatomical_location\")\n",
    "            # get the equipment associated with a session\n",
    "            query = session.Session() * scan.Scan() & f'session_id = \"{current_session[0]}\"'\n",
    "            equipment_dropdown_value = query.fetch(\"scanner\")\n",
    "            # get the note associated with a session\n",
    "            query = session.SessionNote() & f'session_id = \"{current_session[0]}\"'\n",
    "            session_note_textbox_value = query.fetch(\"session_note\")\n",
    "            if len(session_note_textbox_value) == 0:\n",
    "                session_note_textbox_value = [\"none\"]\n",
    "            # get the current SessionSameSite\n",
    "            query = session.SessionSameSite() & f'session_id = \"{current_session[0]}\"'\n",
    "            subject_session_dropdown_value = query.fetch(\"same_site_id\")\n",
    "            # check if manually curated\n",
    "            key = dict(session_id = current_session)\n",
    "            try:\n",
    "                curated = (imaging.Curation & key).fetch1(\"manual_curation\").astype(\"bool\")\n",
    "            except:\n",
    "                curated = False\n",
    "\n",
    "        else:\n",
    "            # POPULATE PRESELECTION FROM USER DEFAULTS\n",
    "            project_dropdown_value = Project[user_defaults[i][0]]\n",
    "            location_dropdown_value = [Recording_Location[user_defaults[i][1]]]\n",
    "            equipment_dropdown_value = [Equipment[user_defaults[i][2]]]\n",
    "            subject_session_dropdown_value = current_session\n",
    "            subject_sessions_array = current_session\n",
    "            session_note_textbox_value = [\"none\"]\n",
    "            curated = False\n",
    "        \n",
    "\n",
    "        project_dropdown = widgets.Dropdown(options=Project, value=project_dropdown_value, description=\"Project:\")\n",
    "        location_dropdown = widgets.Dropdown(options=Recording_Location, value=location_dropdown_value[0], description=\"Location:\")\n",
    "        equipment_dropdown = widgets.Dropdown(options=Equipment, value=equipment_dropdown_value[0], description=\"Setup:\")\n",
    "        s2pparms_dropdown = widgets.Dropdown(options=s2pparm[1], value=s2pparm[1][user_defaults[i][3]], description=\"s2p parm:\")\n",
    "        try:\n",
    "            subject_session_dropdown = widgets.Dropdown(options=subject_sessions_array, value=subject_session_dropdown_value[0], description=\"same site as:\")\n",
    "        except:\n",
    "            subject_session_dropdown = widgets.Dropdown(options=current_session[0], value=current_session[0], description=\"same site as:\")\n",
    "\n",
    "        session_note_textbox = widgets.Text(value=session_note_textbox_value[0], description='Session comment:')\n",
    "\n",
    "        session_dropdowns.append((project_dropdown, location_dropdown, equipment_dropdown, s2pparms_dropdown, subject_session_dropdown, session_note_textbox))\n",
    "\n",
    "        # Checkbox for Process - check to commit for ingest and processing\n",
    "        session_checkbox = widgets.Checkbox(description='run?', layout=widgets.Layout(width='auto'))\n",
    "        # session_checkboxes.append(session_checkbox)\n",
    "\n",
    "        # Checkbox for Curation - check if manually curated\n",
    "        curation_checkbox = widgets.Checkbox(description='curated?', layout=widgets.Layout(width='auto'))\n",
    "        session_checkboxes.append((session_checkbox, curation_checkbox))\n",
    "    \n",
    "    # Display the widgets\n",
    "    output = widgets.Output()\n",
    "\n",
    "    with output:\n",
    "        # Display the Sessions labels and associated dropdowns and checkboxes\n",
    "        hbox_list = []\n",
    "        for i, session_list in enumerate(AvailableSessionDirB): #unique_directory_strings(SessionDirA, SessionDirB)\n",
    "            # Create an HBox to hold the label and associated dropdowns and checkbox\n",
    "            hbox = widgets.HBox()\n",
    "            hbox.children = [\n",
    "                widgets.Label(value=session_list + ':', layout=widgets.Layout(width='1800px')), \n",
    "                session_dropdowns[i][0],\n",
    "                session_dropdowns[i][1],\n",
    "                session_dropdowns[i][2],\n",
    "                session_dropdowns[i][3],\n",
    "                session_dropdowns[i][4],\n",
    "                session_dropdowns[i][5],\n",
    "                session_checkboxes[i][0],\n",
    "                session_checkboxes[i][1]\n",
    "            ]\n",
    "            if session_list in unique_directory_strings(IngestedSessionDirA, AvailableSessionDirB):\n",
    "                hbox.children[0].value = '*' + session_list + ':'\n",
    "                hbox.children[7].value = True\n",
    "                hbox.children[8].value = curated\n",
    "            else:\n",
    "                hbox.children[7].value = False\n",
    "                hbox.children[8].value = False\n",
    "                hbox.children[1].disabled = False\n",
    "                hbox.children[2].disabled = False\n",
    "                hbox.children[3].disabled = False\n",
    "                hbox.children[4].disabled = False\n",
    "                hbox.children[5].disabled = False\n",
    "                hbox.children[6].disabled = False\n",
    "                \n",
    "            hbox_list.append(hbox)\n",
    "\n",
    "        vbox = widgets.VBox(hbox_list, layout=widgets.Layout(flex='0 0 auto', overflow_y='scroll'))\n",
    "        # Display the commit button\n",
    "        commit_button = widgets.Button(description='Commit', layout=widgets.Layout(width='auto'))\n",
    "        display(vbox, commit_button)\n",
    "\n",
    "        # Define the callback function for the commit button\n",
    "        def commit_button_clicked(b):\n",
    "            selected_sessions = [AvailableSessionDirB[i] for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "            selected_projects = [session_dropdowns[i][0].value for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "            selected_locations = [session_dropdowns[i][1].value for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "            selected_equipment = [session_dropdowns[i][2].value for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "            selected_s2pparms = [session_dropdowns[i][3].index for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "            selected_same_site = [session_dropdowns[i][4].value for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "            entered_session_note = [session_dropdowns[i][5].value for i in range(len(AvailableSessionDirB)) if session_checkboxes[i][0].value]\n",
    "\n",
    "            selected_scans = get_scan_key_from_dir(selected_sessions)\n",
    "            selected_sessions = get_session_key_from_dir(selected_sessions)\n",
    "\n",
    "            selected_s2pparms_index = s2pparm[0][selected_s2pparms]\n",
    "\n",
    "            output.clear_output()\n",
    "\n",
    "            \n",
    "            with output:\n",
    "                # OUTPUT FUNCTION: PERFORM INGESTIONS AND JOBS BASED ON SELECTION\n",
    "\n",
    "                # Ingest selected sessions here\n",
    "                populate_settings = {'display_progress': True, 'suppress_errors': True, 'processes': 1}\n",
    "\n",
    "                # SESSION processing\n",
    "                for i, sessi in enumerate(tqdm(selected_sessions, desc='Current Session')):\n",
    "                    isess.ingest_session_scan(sessi, verbose=False, project_key=selected_projects[i], equipment_key=selected_equipment[i], location_key=selected_locations[i], software_key='ScanImage')\n",
    "                    # update / insert session info based on user choice from above\n",
    "                    try:\n",
    "                        session.SessionNote.insert1({'session_id': sessi, 'session_note': entered_session_note[i]})\n",
    "                    except:\n",
    "                        session.SessionNote.delete_quick({'session_id': sessi})\n",
    "                        session.SessionNote.insert1({'session_id': sessi, 'session_note': entered_session_note[i]})\n",
    "                    \n",
    "                    session.SessionSameSite.update1({'session_id': sessi, 'same_site_id': selected_same_site[i]})\n",
    "                    \n",
    "                    # update scaninfo based on user choice from above\n",
    "                   \n",
    "                    # get the scans associated with a session\n",
    "                    query = session.Session() * scan.Scan() & f'session_id = \"{sessi}\"'\n",
    "                    scans_to_process = query.fetch(\"scan_id\")\n",
    "                    # print(\"Scans to process: \" + scans_to_process)\n",
    "\n",
    "                    for j, scansi in enumerate(scans_to_process):\n",
    "                        try:\n",
    "                            scan.Scan.update1({'session_id': sessi, 'scan_id': scansi, 'scan_notes': entered_session_note[i]})\n",
    "                        except:\n",
    "                            scan.Scan.insert1({'session_id': sessi, 'scan_id': scansi, 'scan_notes': entered_session_note[i]})\n",
    "                        try:\n",
    "                            scan.ScanLocation.update1({'session_id': sessi, 'scan_id': scansi, 'anatomical_location': selected_locations[i]})\n",
    "                        except: \n",
    "                            scan.ScanLocation.insert1({'session_id': sessi, 'scan_id': scansi, 'anatomical_location': selected_locations[i]})\n",
    "                        \n",
    "                        query = scan.ScanPath() & 'scan_id = \"' + scansi + '\"'\n",
    "                        dir_proc = query.fetch('path')[0]\n",
    "\n",
    "                        # print(dir_proc)\n",
    "\n",
    "                         # push scan to ProcessingTask\n",
    "                         # TODO: handle multiscan concatenation from here?\n",
    "                        imaging.ProcessingTask.insert1((sessi, scansi, selected_s2pparms_index[i], dir_proc, 'trigger'), skip_duplicates=True)\n",
    "\n",
    "                        print('- - - -')\n",
    "                        print('Ingesting AUX data for scan:', scansi)\n",
    "                        try:\n",
    "                            ibe.ingest_aux(sessi, scansi, verbose=True)                       # print('Ingested and process-queued Scan:', scansi)\n",
    "                        except:\n",
    "                            print(f'Failed to ingest aux for scan = \"{scansi}\"')\n",
    "                    # SCAN Processing \n",
    "                    scan.ScanInfo.populate(**populate_settings) \n",
    "\n",
    "                    ## Make RSpace entries \n",
    "                    query = session.Session() * subject.User() & f'session_id = \"{sessi}\"'\n",
    "                    animalID = query.fetch(\"subject\")[0]\n",
    "                    date = query.fetch(\"session_datetime\")[0].strftime(\"%Y-%m-%d\")\n",
    "                    userID = query.fetch(\"initials\")[0]\n",
    "                    sessionID = query.fetch(\"session_id\")[0]\n",
    "                    fetchtable = session.Session() * scan.ScanPath() * session.SessionUser() * session.ProjectSession() * session.SessionNote() * session.SessionSameSite() * scan.Scan() *scan.ScanInfo() & f'session_id = \"{sessi}\"'\n",
    "\n",
    "                    make_rspace_session_document(animalID, sessi, date, userID, fetchtable)\n",
    "                    # fetchtable = event.BehaviorRecording() * event.BehaviorRecording.File() & f'session_id = \"{sessi}\"'\n",
    "                    # make_rspace_session_document(animalID, sessi, date, userID, fetchtable)               # print('Ingested Sessions:', set(selected_sessions))\n",
    "                # print('Ingested Scans:', scans_to_process)\n",
    "\n",
    "                # RUN PRIMARY PROCESSING TASKS (S2P)\n",
    "\n",
    "                if do_population:\n",
    "\n",
    "                    print(\"\\n---- Populate imported and computed tables ----\")\n",
    "                    try:\n",
    "                        imaging.Processing.populate(**populate_settings)\n",
    "\n",
    "                        # try:\n",
    "                        for i, sessi in enumerate(set(selected_sessions)):\n",
    "                            # get the scans associated with a session\n",
    "                            query = session.Session() * scan.Scan() & f'session_id = \"{sessi}\"'\n",
    "                            scans_to_process = query.fetch(\"scan_id\")\n",
    "                            for j, scansi in enumerate(scans_to_process):\n",
    "                                imaging.Curation().create1_from_processing_task({'session_id': sessi, 'scan_id': scansi, \"paramset_idx\": selected_s2pparms_index[i], \"manual_curation\": session_checkboxes[i][1].value})\n",
    "                        # except:\n",
    "                        #     print(f'no curation file found for {sessi}')                      \n",
    "\n",
    "                        imaging.MotionCorrection.populate(**populate_settings)\n",
    "\n",
    "                        imaging.Segmentation.populate(**populate_settings)\n",
    "\n",
    "                        imaging.MaskClassification.populate(**populate_settings)\n",
    "\n",
    "                        imaging.Fluorescence.populate(**populate_settings)\n",
    "\n",
    "                        imaging.Activity.populate(**populate_settings)\n",
    "\n",
    "                        print(\"\\n---- Successfully completed workflow ----\")\n",
    "\n",
    "                        ## Append RSpace Template figures\n",
    "                        # TODO: make movies and upload as well \n",
    "                        for i, sessi in enumerate(set(selected_sessions)):\n",
    "                            # get the scans associated with a session\n",
    "                            query = session.Session() * scan.Scan() & f'session_id = \"{sessi}\"'\n",
    "                            scans_to_process = query.fetch(\"scan_id\")\n",
    "                            for j, scansi in enumerate(scans_to_process):\n",
    "                                session_key = (session.Session & f'session_id = \"{sessi}\"').fetch('KEY')[0]\n",
    "                                scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "                                curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')\n",
    "                                print(curation_key)\n",
    "                                rspace_id = (session.SessionRspace & f'session_id = \"{sessi}\"').fetch('rspace_id')[0]\n",
    "                        \n",
    "                                make_overview_figures_rspace(curation_key, rspace_id)\n",
    "                                print('making movie')\n",
    "                                make_overview_movies_rspace(curation_key, rspace_id)\n",
    "                    except: \n",
    "                        print(f'POPULATION DID NOT RUN FOR {sessi}')\n",
    "    # Attach the callback function to the commit button\n",
    "    commit_button.on_click(commit_button_clicked)\n",
    "\n",
    "    # Display the output\n",
    "    display(output)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_rspace_session_document(animalID, sessionID, date, userID, fetchtable):\n",
    "    # find 'Experiments' folder and get ID\n",
    "    folders = api.list_folder_tree()\n",
    "    experiments_ids = []\n",
    "    for record in folders['records']:\n",
    "        if record['name'] == 'Experiments':\n",
    "            experiments_ids.append(record['id'])\n",
    "\n",
    "    # find folders under 'Experiments' folder\n",
    "    names_and_ids = []\n",
    "    subfolders = api.list_folder_tree(experiments_ids[0])\n",
    "    names_and_ids.append([(record['name'], record['id']) for record in subfolders['records']])\n",
    "\n",
    "    # find matches with animal IDs\n",
    "    match_found = any(animalID in item[0] for sublist in names_and_ids for item in sublist)\n",
    "\n",
    "    # if found, find folder id matching that animal\n",
    "    if match_found:\n",
    "        save_folder_id = [x[1] for sublist in names_and_ids for x in sublist if animalID in x[0]][0]\n",
    "    else:\n",
    "        # if not generate the directory\n",
    "        new_folder = api.create_folder(name=f\"{userID}_{animalID}\", parent_folder_id=experiments_ids[0], notebook=True)\n",
    "        save_folder_id = new_folder['id']\n",
    "\n",
    "\n",
    "    # find matches with document IDs\n",
    "\n",
    "    # find all documents in the animal folder\n",
    "    all_animal_documents = api.list_folder_tree(save_folder_id)\n",
    "    doc_names_and_ids = []\n",
    "    doc_names_and_ids.append([(record['name'], record['id']) for record in all_animal_documents['records']])\n",
    "    # find matches with sessionID\n",
    "    match_found = any(sessionID in item[0] for sublist in doc_names_and_ids for item in sublist)\n",
    "\n",
    "    if match_found:\n",
    "        ids = [id for string, id in doc_names_and_ids[0] if sessionID in string]\n",
    "        new_doc = {\"id\": ids[0]}\n",
    "    else:\n",
    "        new_doc = api.create_document(name=f\"{date}_{sessionID}\", parent_folder_id=save_folder_id)\n",
    "\n",
    "    # create RSpace document\n",
    "    df = fetchtable.fetch(format='frame')\n",
    "    html_table = df.to_html()\n",
    "    content = html_table\n",
    "    api.append_content(new_doc['id'], content)\n",
    "\n",
    "    # ingest document in rspace table\n",
    "    session.SessionRspace.insert1({'session_id': sessionID, 'rspace_id': new_doc['id']}, skip_duplicates=True)\n",
    "    # ingest animal folder id in subject Rspace table\n",
    "    subject.SubjectRspace.insert1({'subject': animalID, 'rspace_subject_id': save_folder_id}, skip_duplicates=True)\n",
    "    # session.SessionRspace.insert1({'rspace_id': new_doc['id'], 'rspace_url': new_doc['_links'][0]['link']})\n",
    "\n",
    "## DEPRECATED\n",
    "# def live_update_s2pparms(sessi, scansi, s2pparms_index):\n",
    "\n",
    "#     key = dict(paramset_idx = s2pparms_index)\n",
    "#     params_dict = (imaging.ProcessingParamSet() & key).fetch1('params')\n",
    "\n",
    "#     key = dict(session_id = sessi, scan_id = scansi)\n",
    "\n",
    "#     fs = (scan.ScanInfo() & key).fetch1(\"fps\")\n",
    "#     nchannels = (scan.ScanInfo() & key).fetch1(\"nchannels\")\n",
    "#     nplanes = (scan.ScanInfo() & key).fetch1(\"nplanes\")\n",
    "    \n",
    "#     params_dict['nchannels'] = nchannels\n",
    "#     params_dict['fs'] = fs\n",
    "#     params_dict['nplanes'] = nplanes\n",
    "\n",
    "#     params_dict = imaging.ProcessingParamSet().update1({'paramset_idx': s2pparms_index, 'params': params_dict})\n",
    "\n",
    "\n",
    "def get_session_dir_key_from_dir(directory):\n",
    "    return [path.split('/')[-1] for path in directory]\n",
    "     \n",
    "def get_scan_dir_key_from_dir(directory):\n",
    "    return [path.split('/')[-1] for path in directory]\n",
    "\n",
    "\n",
    "\n",
    "def get_session_key_from_dir(string):\n",
    "    result = [re.search(r'sess\\S+', item).group(0) for item in string]\n",
    "    return result\n",
    "\n",
    "def get_user_initials_from_dir(string):\n",
    "    result = [name[:2] for name in string]\n",
    "    return result\n",
    "\n",
    "def get_subject_key_from_dir(string):\n",
    "    result = [item.split(\"_\")[1] for item in string]\n",
    "    return result\n",
    "\n",
    "def get_date_key_from_dir(directory):\n",
    "    return directory.split(\"_\")[-1]\n",
    "\n",
    "def get_scan_key_from_dir(string):\n",
    "    result = [re.search(r'scan\\S+_', item).group(0)[:-1] for item in string]\n",
    "    return result\n",
    "\n",
    "def unique_directory_strings(dirs1, dirs2):\n",
    "    set1 = set(dirs1)\n",
    "    set2 = set(dirs2)\n",
    "    common_dirs = list(set1.intersection(set2))\n",
    "    unique_dirs = list(set(set1.union(set2)) - set(common_dirs))\n",
    "    return unique_dirs\n",
    "\n",
    "def get_user_defaults(directory):\n",
    "    RN = [2, 2, 0, 1]\n",
    "    JJ = [4, 8, 7, 2]\n",
    "    TR = [2, 2, 0, 2]\n",
    "    LK = [7, 1, 0, 2]\n",
    "    DB = [4, 0, 1, 2]\n",
    "    NK = [6, 5, 7, 4]\n",
    "    LE = [6, 5, 7, 4]\n",
    "    \n",
    "    user_initials = get_user_initials_from_dir(directory)\n",
    "\n",
    "    user_arrays = {}\n",
    "    for initial in user_initials:\n",
    "        values = locals()[initial]\n",
    "        user_arrays[initial] = values\n",
    "\n",
    "    new_array = [user_arrays[initial] for initial in user_initials]\n",
    "    return new_array\n",
    "\n",
    "def make_overview_figures_rspace(curation_key, rspace_id):\n",
    "    # Figure Style settings for notebook.\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    mpl.rcParams.update({\n",
    "        'axes.spines.left': False,\n",
    "        'axes.spines.bottom': False,\n",
    "        'axes.spines.top': False,\n",
    "        'axes.spines.right': False,\n",
    "        'legend.frameon': False,\n",
    "        'figure.subplot.wspace': .01,\n",
    "        'figure.subplot.hspace': .01,\n",
    "        'figure.figsize': (18, 13),\n",
    "        'ytick.major.left': False,\n",
    "        'xtick.major.bottom': False\n",
    "    })\n",
    "    jet = mpl.cm.get_cmap('jet')\n",
    "    jet.set_bad(color='k')\n",
    "\n",
    "    ref_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('ref_image')\n",
    "    average_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('average_image')\n",
    "    correlation_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('correlation_image')\n",
    "    max_proj_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('max_proj_image')\n",
    "    \n",
    "    plt.ioff()\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.imshow(ref_image, cmap='gray', )\n",
    "    plt.title(\"Reference Image for Registration\");\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.imshow(average_image, cmap='gray')\n",
    "    plt.title(\"Registered Image, Mean Projection\");\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.imshow(max_proj_image, cmap='gray')\n",
    "    plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.imshow(correlation_image, cmap='gray')\n",
    "    plt.title(\"Registered Image, Correlation Map\")\n",
    "\n",
    "    tmpdir = dj.config['custom'].get('suite2p_fast_tmp')[0]\n",
    "    session_id = curation_key['session_id']\n",
    "    scan_id = curation_key['scan_id']\n",
    "    save_path = os.path.join(tmpdir, f'{session_id}_{scan_id}_templates.png')\n",
    "    plt.savefig(save_path)\n",
    "    plt.clf()\n",
    "\n",
    "    # print(f'rspace_id' {rspace_id})\n",
    "    # print(f'save_path' {save_path})\n",
    "    # print(f'session_id' {session_id})\n",
    "    \n",
    "    append_rspace_session_image(rspace_id, save_path, captionimage='Suite2p Templates ' + session_id)   \n",
    "    plt.ion()\n",
    "\n",
    "    \n",
    "def make_overview_movies_rspace(curation_key, rspace_id):\n",
    "    # params_key = (imaging.ProcessingParamSet & 'paramset_idx = \"4\"').fetch('KEY')\n",
    "    # reg_tiffs_available = (imaging.ProcessingParamSet & params_key).fetch(\"params\")[0]['reg_tif']\n",
    "    from scipy.ndimage import mean\n",
    "    import tifffile\n",
    "    path = (scan.ScanPath & curation_key).fetch1(\"path\") + (\"/suite2p/plane0/reg_tif\")\n",
    "\n",
    "    # path = '/datajoint-data/data/jisooj/RN_OPI-1681_2023-02-15_scan9FGLEFJ3_sess9FGLEFJ3/suite2p_exp9FGLEFJ3/suite2p/plane0/reg_tif'\n",
    "    # Get a list of all tiff files in the folder\n",
    "    tiff_files = [os.path.join(path, f) for f in natsorted(os.listdir(path)) if f.endswith('.tif')]\n",
    "\n",
    "    # print(tiff_files)\n",
    "\n",
    "    # Load each tiff stack into a list of numpy arrays\n",
    "    stacks = []\n",
    "    for f in tiff_files:\n",
    "        with tifffile.TiffFile(f) as tif:\n",
    "            # Get the number of pages in the file\n",
    "            num_pages = len(tif.pages)\n",
    "            \n",
    "            # Create a numpy array to store all pages\n",
    "            stack = np.zeros((num_pages,) + tif.pages[0].shape, dtype=tif.pages[0].dtype)\n",
    "            \n",
    "            # Iterate over the pages and store them in the array\n",
    "            for i, page in enumerate(tif.pages):\n",
    "                stack[i] = page.asarray()\n",
    "\n",
    "        stacks.append(stack)\n",
    "\n",
    "    # Concatenate the stacks into a single numpy array along the z-axis\n",
    "    volume = np.concatenate(stacks, axis=0)\n",
    "\n",
    "    # delete registration tiff\n",
    "    for f in tiff_files:\n",
    "        os.remove(f) \n",
    "    \n",
    "    ### moving average filter\n",
    "    # Create a running Z mean projection of the volume\n",
    "\n",
    "    runav = 40\n",
    "    # running_z_projection = uniform_filter_mt(volume, size=(runav,xyrunav,xyrunav))\n",
    "    running_z_projection = sh.rolling_average_filter(volume, runav)\n",
    "\n",
    "    session_id = curation_key['session_id']\n",
    "    scan_id = curation_key['scan_id']\n",
    "\n",
    "    filename = os.path.join(path, 'registered_movie_' + session_id + '_' + scan_id + '_' + str(runav) + '_frame_runningaverage2' + '.mp4')\n",
    "\n",
    "    fps = 120   # frames per second - 120 default\n",
    "    p1 = 2       # percentile scaling low - 1 default\n",
    "    p2 = 99.998  # percentile scaling high - 99.995 default\n",
    "\n",
    "    rescaled_image_8bit = sh.make_stack_movie(running_z_projection, filename, fps, p1, p2)\n",
    "\n",
    "    tmpdir = dj.config['custom'].get('suite2p_fast_tmp')[0]\n",
    "\n",
    "\n",
    "    ### movie uload does not work yet\n",
    "    # append_rspace_session_image(rspace_id, filename, captionimage='registered movie ' + scan_id)   \n",
    "\n",
    "def append_rspace_session_image(rspace_id, filename, captionimage='no caption'):\n",
    "    with open(filename, 'rb') as f:\n",
    "        uploaded_image = api.upload_file(f, caption=captionimage)\n",
    "        \n",
    "        content = f\"\"\"\n",
    "        <p>Suite2p templates of dataset {filename}\n",
    "        <p>\n",
    "        <fileId={uploaded_image['id']}>\n",
    "        \"\"\"\n",
    "        api.append_content(rspace_id, content)\n",
    "        \n",
    "        print(f\"uploaded image id = {uploaded_image['id']}\") \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b049a5a7",
   "metadata": {},
   "source": [
    "#### Parse user data root dir and sort by date-string in directory name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "297a65e2",
   "metadata": {},
   "source": [
    "### Ingest GUI\n",
    "\n",
    "Please follow these instructions to add new recording sessions using the provided GUI:\n",
    "\n",
    "**Step 1: Add new recording sessions**\n",
    "\n",
    "Use the GUI to add new recording sessions and link them to projects, brain regions, suite2p options, and previous sessions of the same animal that you have imaged repeatedly.\n",
    "\n",
    "The GUI does the following:\n",
    "\n",
    "- it parses your primary data storage folder on the analysis server and lists all session directories\n",
    "- it marks all *new* folders with an asterisk and a pre-checked \"run\" checkbox\n",
    "- all session folders that are already in the database are unchecked - but the metadata can still be modified and committed when you check the checkbox\n",
    "\n",
    "What you need to do:\n",
    "\n",
    "1. Upload your folders to the server under your own credentials and folder name\n",
    "2. In your `dj.config` file you can store the default values for the selectable metadata (not fully implemented)\n",
    "3. Edit the selectable metadata, select the appropriate s2p config file and select the appropriate first session of a chronic experiment with that animal. Finally: Add some comments if needed. \n",
    "4. To queue for processing check checkbox \n",
    "5. Press **commit**\n",
    "\n",
    "This will \n",
    "- pull all animal information from the pyrat database to populate the `subject` tables\n",
    "- ingest the session, generate a new `session` table entry, pull and store all metadata from the scanimage tiff to populate the `scan` tables\n",
    "- put all necessary data into the processing queue for image registration, source extraction etc. Currently this is triggered immediately - in the future our two servers will poll the processing queue regularly to work in parallel on the data\n",
    "\n",
    "Importantly, the ingest will generate (or write into if existing) the `animal_ID` folder of **RSPACE**. It will generate a session document and post all important session information right there. If available it will also automatically upload the scanimage template images. Feel free to add information (bloodvessel overviews, session comments, intermediate analyses etc.) to this rspace session document. \n",
    "\n",
    "Furthermore, this will\n",
    "- read in all 'h5' wavesurfer files to populate the `event` tables\n",
    "\n",
    "**Step 2: Manual Curation**\n",
    "\n",
    "If you have manually curated the data, tick the manual curation checkbox. This will trigger another source extraction based on that curation. You can add as many s2p settings to the tables as you wish.\n",
    "\n",
    "## ToDo\n",
    "1. integrate BPOD ingestion (needs new example file! Bpod data needs to include Main gate as events!)\n",
    "2. integrate HARP ingestion (check consistence of frame numbers to be expected from cam rect and 2p rec)\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4832c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content of user directory\n",
    "dataroot = dj.config['custom']['exp_root_data_dir'][0]\n",
    "dirs_root = [d for d in os.listdir(dataroot) if os.path.isdir(os.path.join(dataroot, d)) and 'NK' in d]\n",
    "sorted_dirs_root = natsorted(dirs_root, key=get_date_key_from_dir, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523b82d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINGEST GUI\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3f9f64219d46aa92ace8655ffed2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate GUI\n",
    "print(\"\\033[1m\" + 'INGEST GUI' + \"\\033[0m\" )\n",
    "selected_data = select_sessions(sorted_dirs_root, do_population = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e35dee3",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "614a275c",
   "metadata": {},
   "source": [
    "## Run imaging processing tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cc9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Curation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b79a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_settings = {'display_progress': True, 'suppress_errors': False, 'processes': 1}\n",
    "imaging.Processing.populate(**populate_settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de369dfc",
   "metadata": {},
   "source": [
    "### Do manual suite2p curation now!\n",
    "### Then: populate curation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Curation().create1_from_processing_task({'session_id': \"sess9FHELAYA\", 'scan_id': \"scan9FHELAYA\", \"paramset_idx\":0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "047e7c16",
   "metadata": {},
   "source": [
    "Populate imaging.MotionCorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_settings = {'display_progress': True, 'suppress_errors': False, 'processes': 60}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de44a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.MotionCorrection.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.MotionCorrection.NonRigidMotionCorrection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.MotionCorrection.NonRigidMotionCorrection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Segmentation.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55087028",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.MaskClassification.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Fluorescence.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Activity.populate(**populate_settings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf7dbbf",
   "metadata": {},
   "source": [
    "## Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426785d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining queries\n",
    "\n",
    "sessi = \"sess9FHIRPBE\"\n",
    "scansi = \"scan9FHIRPBE\"\n",
    "\n",
    "session_key = (session.Session & f'session_id = {sessi}').fetch('KEY')\n",
    "scan_key = (scan.Scan & f'scan_id = {scansi}').fetch('KEY')\n",
    "curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "758b3383",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "853d36e8",
   "metadata": {},
   "source": [
    "### Some images / plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce41ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make helper function\n",
    "\n",
    "# Figure Style settings for notebook.\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .01,\n",
    "    'figure.subplot.hspace': .01,\n",
    "    'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': False,\n",
    "    'xtick.major.bottom': False\n",
    "})\n",
    "jet = mpl.cm.get_cmap('jet')\n",
    "jet.set_bad(color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('ref_image')\n",
    "average_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('average_image')\n",
    "correlation_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('correlation_image')\n",
    "max_proj_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('max_proj_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d314b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir = dj.config['custom'].get('suite2p_tif_tmp')[0]\n",
    "session_id = curation_key['session_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prevent showing and just save. Make function\n",
    "\n",
    "plt.ion()\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(ref_image, cmap='gray', )\n",
    "plt.title(\"Reference Image for Registration\");\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(average_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Mean Projection\");\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(max_proj_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(correlation_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Correlation Map\")\n",
    "plt.show(block=False)\n",
    "# plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cad261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prevent showing and just save. Make function\n",
    "plt.ion()\n",
    "mask_xpix, mask_ypix = (imaging.Segmentation.Mask * imaging.MaskClassification.MaskType & curation_key & 'mask_center_z=0').fetch('mask_xpix','mask_ypix')\n",
    "\n",
    "mask_image = np.zeros(np.shape(average_image), dtype=bool)\n",
    "for xpix, ypix in zip(mask_xpix, mask_ypix):\n",
    "    mask_image[ypix, xpix] = True\n",
    "\n",
    "plt.imshow(average_image);\n",
    "plt.contour(mask_image, colors='white', linewidths=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5789ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: prevent showing and just save. Make function\n",
    "query_cells = (imaging.Segmentation.Mask * imaging.MaskClassification.MaskType & curation_key & 'mask_center_z=0' & 'mask_npix > 80').proj()\n",
    "\n",
    "fluorescence_traces = (imaging.Fluorescence.Trace & query_cells).fetch('fluorescence', order_by='mask')\n",
    "\n",
    "activity_traces = (imaging.Activity.Trace & query_cells).fetch('activity_trace', order_by='mask')\n",
    "\n",
    "sampling_rate = (scan.ScanInfo & curation_key).fetch1('fps') # [Hz]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 4))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "for f, a in zip(fluorescence_traces, activity_traces):\n",
    "    ax.plot(np.r_[:f.size] * 1/sampling_rate, f, 'w', label='fluorescence trace')    \n",
    "    ax2.plot(np.r_[:a.size] * 1/sampling_rate, a, 'r', alpha=0.5, label='deconvolved trace')\n",
    "    \n",
    "    break\n",
    "\n",
    "ax.tick_params(labelsize=14)\n",
    "ax2.tick_params(labelsize=14)\n",
    "\n",
    "ax.legend(loc='upper left', prop={'size': 14})\n",
    "ax2.legend(loc='upper right', prop={'size': 14})\n",
    "\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Activity (a.u.)')\n",
    "ax2.set_ylabel('Activity (a.u.)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ece37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Activity.Trace() & query_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ace9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Fluorescence.Trace() * imaging.Segmentation.Mask() & query_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddd7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Segmentation.Mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap import Rastermap\n",
    "from scipy import stats \n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "ops = {'n_components': 1, 'n_X': 100, 'alpha': 1., 'K': 1.,\n",
    "            'nPC': 200, 'constraints': 2, 'annealing': True, 'init': 'pca',\n",
    "            'start_time': 0, 'end_time': -1}\n",
    "\n",
    "S = np.vstack(fluorescence_traces)\n",
    "# we run rastermap the same way that the other scikit-learn embedding algorithms work\n",
    "# model = Rastermap(n_components=1, n_X=100, verbose = True).fit(S) \n",
    "model = Rastermap(n_components=ops['n_components'], n_X=ops['n_X'], nPC=ops['nPC'],\n",
    "                          init=ops['init'], alpha=ops['alpha'], K=ops['K'], constraints=ops['constraints'],\n",
    "                          annealing=ops['annealing'])\n",
    "model.fit(S)\n",
    "def running_average(X, nbin = 10):\n",
    "    Y = np.cumsum(X, axis=0)\n",
    "    Y = Y[nbin:, :] - Y[:-nbin, :]\n",
    "    return Y\n",
    "# the manifold embedding is in model.embedding\n",
    "isort = np.argsort(model.embedding[:,0])\n",
    "\n",
    "# sort by embedding and smooth over neurons\n",
    "# Sfilt = running_average(S[isort], 0)\n",
    "Sfilt = gaussian_filter1d(S[isort].T, np.minimum(8,np.maximum(1,int(S.shape[0]*0.005))),axis=1)\n",
    "Sfilt = stats.zscore(Sfilt, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80780c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.imshow(Sfilt, vmin = -0.1, vmax=3, aspect='auto', cmap='gray_r')\n",
    "plt.xlabel('time points')\n",
    "plt.ylabel('sorted neurons')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e778b48b",
   "metadata": {},
   "source": [
    "also see https://github.com/trose-neuro/mini2p---moser-analysis-datajoint/blob/master/Analysis%20notebooks/PCA%20Stringer%20Pacchitariu.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67be652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamacs.helpers import stack_helpers as sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import mean\n",
    "import tifffile\n",
    "path = (scan.ScanPath & curation_key).fetch1(\"path\") + (\"/suite2p/plane0/reg_tif\")\n",
    "\n",
    "# path = '/datajoint-data/data/jisooj/RN_OPI-1681_2023-02-15_scan9FGLEFJ3_sess9FGLEFJ3/suite2p_exp9FGLEFJ3/suite2p/plane0/reg_tif'\n",
    "# Get a list of all tiff files in the folder\n",
    "tiff_files = [os.path.join(path, f) for f in natsorted(os.listdir(path)) if f.endswith('.tif')]\n",
    "\n",
    "print(tiff_files)\n",
    "\n",
    "# Load each tiff stack into a list of numpy arrays\n",
    "stacks = []\n",
    "for f in tiff_files:\n",
    "    with tifffile.TiffFile(f) as tif:\n",
    "        # Get the number of pages in the file\n",
    "        num_pages = len(tif.pages)\n",
    "        \n",
    "        # Create a numpy array to store all pages\n",
    "        stack = np.zeros((num_pages,) + tif.pages[0].shape, dtype=tif.pages[0].dtype)\n",
    "        \n",
    "        # Iterate over the pages and store them in the array\n",
    "        for i, page in enumerate(tif.pages):\n",
    "            stack[i] = page.asarray()\n",
    "\n",
    "    stacks.append(stack)\n",
    "\n",
    "# Concatenate the stacks into a single numpy array along the z-axis\n",
    "volume = np.concatenate(stacks, axis=0)\n",
    "\n",
    "# # delete registration tiffd\n",
    "# for f in tiff_files:\n",
    "#     os.remove(f) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9419eae5",
   "metadata": {},
   "source": [
    "### moving average filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b16342",
   "metadata": {},
   "outputs": [],
   "source": [
    "### moving average filter\n",
    "# Create a running Z mean projection of the volume\n",
    "\n",
    "runav = 40\n",
    "# running_z_projection = uniform_filter_mt(volume, size=(runav,xyrunav,xyrunav))\n",
    "running_z_projection = sh.rolling_average_filter_mt(volume, runav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh.display_volume_z(running_z_projection,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb328175",
   "metadata": {},
   "source": [
    "### mp4 movie generaltion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(path, 'registered_movie' + str(runav) + '_frame_runningaverage' + '.mp4')\n",
    "\n",
    "fps = 240   # frames per second - 120 default\n",
    "p1 = 1      # percentile scaling low - 1 default\n",
    "p2 = 99.995  # percentile scaling high - 99.995 default\n",
    "\n",
    "rescaled_image_8bit = sh.make_stack_movie(running_z_projection, filename, fps, p1, p2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a6c4674",
   "metadata": {},
   "source": [
    "## Exploring updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2021e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(scan) + dj.Diagram(imaging.Processing) + dj.Diagram(imaging.Curation) + dj.Diagram(imaging.Curation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(scan) + dj.Diagram(imaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.MotionCorrection.Summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a7d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2888206d04306eefe963095373d1dcce5cd11176f3d6ee4ec3f64dd9e65fd8a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

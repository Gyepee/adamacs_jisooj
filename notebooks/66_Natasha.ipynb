{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.1\n",
      "tobiasrext_\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# change to the upper level folder to detect dj_local_conf.json\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='adamacs', (\"Please move to the main directory\")\n",
    "from adamacs.pipeline import subject, session, equipment, surgery, event, trial, imaging, behavior, scan, model\n",
    "from adamacs.ingest import session as isess\n",
    "from adamacs.ingest.harp import CamLoader_sync\n",
    "from adamacs.helpers import stack_helpers as sh\n",
    "from adamacs.helpers import trace_helpers as th\n",
    "from adamacs.helpers import dj_helpers as djh\n",
    "from adamacs.ingest import behavior as ibe\n",
    "from adamacs.paths import get_experiment_root_data_dir\n",
    "import datajoint as dj\n",
    "from rspace_client.eln import eln\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from element_interface.utils import find_full_path\n",
    "print(dj.__version__)\n",
    "print(dj.config['custom']['database.prefix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some functions used here (will be hidden later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "\n",
    "def get_closest_timestamps(series, target_timestamp):\n",
    "    # List to store the indices\n",
    "    indices = []\n",
    "\n",
    "    # For each timestamp in series1, find the closest timestamp in series2 and get its index\n",
    "    for t1 in series:\n",
    "        closest_index = closest_timestamp(target_timestamp, t1)\n",
    "        indices.append(closest_index)\n",
    "    return indices\n",
    "\n",
    "# Function to find closest timestamp\n",
    "def closest_timestamp(series, target_timestamp):\n",
    "    index = bisect.bisect_left(series, target_timestamp)\n",
    "    if index == 0:\n",
    "        return 0\n",
    "    if index == len(series):\n",
    "        return len(series)-1\n",
    "    before = series[index - 1]\n",
    "    after = series[index]\n",
    "    if after - target_timestamp < target_timestamp - before:\n",
    "       return index\n",
    "    else:\n",
    "       return index-1\n",
    "\n",
    "\n",
    "def resize_movie(movie, new_height, new_width):\n",
    "    # Get the number of frames and color channels\n",
    "    num_frames, _, _, num_channels = movie.shape\n",
    "    \n",
    "    # Initialize an empty array for the scaled movie\n",
    "    scaled_movie = np.empty((num_frames, new_height, new_width, num_channels), dtype=np.uint8)\n",
    "    \n",
    "    # Iterate through each frame\n",
    "    for i in tqdm(range(num_frames), desc=\"Resizing frames\"):\n",
    "        # Resize the frame and store it in the new array\n",
    "        scaled_movie[i] =  cv2.resize(movie[i], (new_width, new_height), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Return the scaled movie\n",
    "    return scaled_movie\n",
    "\n",
    "\n",
    "def resize_frame(frame, new_height, new_width):\n",
    "    return cv2.resize(frame, (new_width, new_height), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def resize_movie_mt(movie, new_height, new_width):\n",
    "    num_frames, _, _, num_channels = movie.shape\n",
    "    scaled_movie = np.empty((num_frames, new_height, new_width, num_channels), dtype=np.uint8)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for i, resized_frame in tqdm(enumerate(executor.map(resize_frame, movie, [new_height]*num_frames, [new_width]*num_frames)), total=num_frames, desc=\"Resizing frames\"):\n",
    "            scaled_movie[i] = resized_frame\n",
    "\n",
    "    return scaled_movie\n",
    "\n",
    "\n",
    "\n",
    "def create_snippets(videodata, indices, fps):\n",
    "    snippets = []\n",
    "    for index in indices:\n",
    "        start = max(index - starter, 0)\n",
    "        end = min(index + ender, len(videodata))\n",
    "        snippet = videodata[start:end]\n",
    "        snippets.append(snippet)\n",
    "    return snippets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def equalize_snippets(snippets):\n",
    "    # Find the maximum number of frames in any snippet\n",
    "    max_frames = max(snippet.shape[0] for snippet in snippets)\n",
    "\n",
    "    # Equalize the number of frames in each snippet\n",
    "    equalized_snippets = []\n",
    "    for snippet in snippets:\n",
    "        if snippet.shape[0] < max_frames:\n",
    "            # Pad the snippet with blank frames or repeat the last frame\n",
    "            padding = np.zeros((max_frames - snippet.shape[0],) + snippet.shape[1:])\n",
    "            snippet_padded = np.concatenate([snippet, padding], axis=0)\n",
    "            equalized_snippets.append(snippet_padded)\n",
    "        else:\n",
    "            equalized_snippets.append(snippet)\n",
    "    \n",
    "    return equalized_snippets\n",
    "\n",
    "# Figure Style settings for notebook.\n",
    "\n",
    "def concatenate_to_grid(snippets):\n",
    "    # Determine the grid size\n",
    "    num_snippets = len(snippets)\n",
    "    grid_size = int(math.ceil(math.sqrt(num_snippets)))\n",
    "\n",
    "    # Initialize placeholders for rows and the final grid\n",
    "    rows = []\n",
    "    final_grid = None\n",
    "\n",
    "    # Concatenate snippets into rows\n",
    "    for i in range(0, num_snippets, grid_size):\n",
    "        row = snippets[i:i + grid_size]\n",
    "        while len(row) < grid_size:  # Pad the row if necessary\n",
    "            row.append(np.zeros_like(snippets[0]))\n",
    "        concatenated_row = np.concatenate(row, axis=2)  # Concatenate along width\n",
    "        rows.append(concatenated_row)\n",
    "\n",
    "    # Concatenate rows to form the grid\n",
    "    final_grid = np.concatenate(rows, axis=1)  # Concatenate along height\n",
    "\n",
    "    return final_grid\n",
    "\n",
    "\n",
    "def resize_video_frames(grid_data, target_width=3840, target_height=2160):\n",
    "    resized_video = []\n",
    "\n",
    "    for frame in grid_data:\n",
    "        # Resize frame to 4K resolution\n",
    "        resized_frame = cv2.resize(frame, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "        resized_video.append(resized_frame)\n",
    "\n",
    "    return np.array(resized_video)\n",
    "\n",
    "\n",
    "plot_params = {\n",
    "    'axes.facecolor': 'white',\n",
    "    'figure.facecolor': 'white',\n",
    "    'font.family': 'sans-serif',\n",
    "    # 'font.sans-serif': 'Helvetica Neue',\n",
    "    'font.size': 16,\n",
    "    'lines.color': 'black',\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.spines.top': True,\n",
    "    'axes.spines.right': True,\n",
    "    'axes.edgecolor': 'black',  \n",
    "    # 'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .5,\n",
    "    'figure.subplot.hspace': .5,\n",
    "    # 'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': True,\n",
    "    'xtick.major.bottom': True\n",
    "}\n",
    "\n",
    "map_params = {\n",
    "    'axes.facecolor': 'white',\n",
    "    'figure.facecolor': 'white',\n",
    "    'font.family': 'sans-serif',\n",
    "    # 'font.sans-serif': 'Helvetica Neue',\n",
    "    'font.size': 12,\n",
    "    'lines.color': 'black',\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.edgecolor': 'black',  \n",
    "    # 'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .5,\n",
    "    'figure.subplot.hspace': .5,\n",
    "    # 'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': False,\n",
    "    'xtick.major.bottom': False\n",
    "}\n",
    "\n",
    "\n",
    "img_params = {\n",
    "    'axes.titlecolor': 'white',\n",
    "    'axes.facecolor': 'black',\n",
    "    'figure.facecolor': 'black',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .01,\n",
    "    'figure.subplot.hspace': .01,\n",
    "    'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': False,\n",
    "    'xtick.major.bottom': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recordings and example analysis for BonnBrain - NATASHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natasha - example scans\n",
    "\n",
    "# mini2p\n",
    "# sess9FIG39RU\n",
    "# sess9FHS7Y22\n",
    "\n",
    "# bench2p\n",
    "# sessi = sess9FJ66OT2\n",
    "# scan9FGLE1FN\n",
    "\n",
    "# hese are the recordings for the poster. But I am not sure if the recording is good enough. If not, it would be with translucent arena, posting in the next message\n",
    "# x2 5min + x1 10 mins recordings\n",
    "# LED flickering\n",
    "# IMU on\n",
    "# arena is covered with rubber walls and floor, upper IR light\n",
    "# no task\n",
    "# power 70% tLens -110\n",
    "# threshold for the blob tracking 15\n",
    "# /datajoint-data/data/nataliak/NK_WEZ-8869_2023-06-07_scan9FIG39RU_sess9FIG39RU\n",
    "# /datajoint-data/data/nataliak/NK_WEZ-8869_2023-06-07_scan9FIG3MU8_sess9FIG39RU\n",
    "# /datajoint-data/data/nataliak/NK_WEZ-8869_2023-06-07_scan9FIG3GCJ_sess9FIG39RU\n",
    "# 9:32\n",
    "# x2 10min recordings\n",
    "# LED flickering\n",
    "# IMU on\n",
    "# translucent arena\n",
    "# no task\n",
    "# power 70% tLens -80\n",
    "# /datajoint-data/data/nataliak/NK_ROS-1485_2023-04-28_scan9FHS7Y22_sess9FHS7Y22\n",
    "# /datajoint-data/data/nataliak/NK_ROS-1485_2023-04-28_scan9FHS845A_sess9FHS7Y22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench2p figures and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a key to be used across multiple tables\n",
    "\n",
    "scansi = \"scan9FIG39RU\"\n",
    "# scansi = \"scan9FKNRW9Y\"\n",
    "\n",
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "# curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')\n",
    "sessi = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('session_id')[0]\n",
    "aux_setup_typestr = (scan.ScanInfo() & scan_key).fetch(\"userfunction_info\")[0] # check setup type (not needed)\n",
    "print(aux_setup_typestr)\n",
    "print((scan.ScanPath & scan_key).fetch(\"path\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and show overview images from suite2p registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('ref_image')\n",
    "average_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('average_image')\n",
    "correlation_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('correlation_image')\n",
    "max_proj_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('max_proj_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some scaling values from pixel distribution\n",
    "scalemin = 2\n",
    "scalemax = 100\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin)  \n",
    "cmax = np.percentile(average_image,scalemax)\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "# Make figure with all templates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(ref_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(f\"Reference Image - {scansi}\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Registered Image, Mean Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(max_proj_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(correlation_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Correlation Map\")\n",
    "\n",
    "plt.show(block=False)\n",
    "\n",
    "# plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just mean image\n",
    "scalemin = 2\n",
    "scalemax = 100\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin)  \n",
    "cmax = np.percentile(average_image,scalemax)\n",
    "\n",
    "\n",
    "# plt.subplot(1, 4, 1)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot treadmill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract scan treadmill data from database using the scan_key from above\n",
    "treadmill = (behavior.TreadmillRecording.Channel() & scan_key).fetch(\"data\")[0]\n",
    "auxtime = (behavior.TreadmillRecording.Channel() & scan_key).fetch(\"time\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing window size (ms)\n",
    "window = 5000\n",
    "\n",
    "# convert voltage to degree\n",
    "treadmillnorm = (treadmill-np.min(treadmill)) / np.max(treadmill) * 360\n",
    "\n",
    "# compute running speed (see function above)\n",
    "angular_velocity_smoothed, unwrapped_angle_smoothed = ibe.compute_angular_velocity(auxtime, treadmillnorm, window)\n",
    "\n",
    "# get some values to scale running speed plot \n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "offset = 10\n",
    "ymin = np.percentile(angular_velocity_smoothed,scalemin)  - offset\n",
    "ymax = np.percentile(angular_velocity_smoothed,scalemax)  + offset\n",
    "\n",
    "# load plot styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15, 10))\n",
    "# plt.rcParams['agg.path.chunksize'] = 10000  # Add this line if it does not rende\n",
    "\n",
    "# Plotting the time series\n",
    "axes[0].plot(auxtime, treadmillnorm)\n",
    "axes[0].set_ylim([-10, 370])\n",
    "axes[0].set_ylabel(\"Wheel position \\n[degree]\")\n",
    "axes[0].set_xlabel(\"Time [s]\")\n",
    "\n",
    "axes[1].plot(auxtime[:-window+1],unwrapped_angle_smoothed )\n",
    "# axes[1].set_ylim([-10000, 10000])\n",
    "axes[1].set_ylabel(\"Unwrapped wheel position \\n[cumulative degree]\")\n",
    "axes[1].set_xlabel(\"Time [s]\")\n",
    "\n",
    "axes[2].plot(auxtime[:-window],angular_velocity_smoothed)\n",
    "axes[2].set_ylim([ymin, ymax])\n",
    "axes[2].set_ylabel(\"Running speed \\n[degree / s]\")\n",
    "axes[2].set_xlabel(\"Time [s]\")\n",
    "\n",
    "fig.suptitle(scan_key[\"scan_id\"], fontsize=16)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the fluorescence traces of this recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask positions of masks that are classified as cells and that are larger than a certain pixel size\n",
    "mask_xpix, mask_ypix = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & scan_key\n",
    "    & \"mask_npix > 1\"\n",
    ").fetch(\"mask_xpix\", \"mask_ypix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this query, we've fetched the coordinates of segmented masks. We can overlay these\n",
    "masks onto our average image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_image = np.zeros(np.shape(average_image), dtype=bool)\n",
    "for xpix, ypix in zip(mask_xpix, mask_ypix):\n",
    "    mask_image[ypix, xpix] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.contour(mask_image, colors=\"red\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example using queries - plot fluorescence and deconvolved activity\n",
    "traces:\n",
    "\n",
    "Here we fetch the primary key attributes of the entry with `curation_id=0` for the\n",
    "current session in the `imaging.Curation` table. \n",
    "\n",
    "Then, we fetch all cells that fit the\n",
    "restriction criteria from `imaging.Segmentation.Mask` and\n",
    "`imaging.MaskClassification.MaskType` as a `projection`. \n",
    "\n",
    "We then use this projection as\n",
    "a restriction to fetch and plot fluorescence and deconvolved activity traces from the\n",
    "`imaging.Fluorescence.Trace` and `imaging.Activity.Trace` tables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key = (imaging.Curation & scan_key & \"curation_id=1\").fetch1(\"KEY\")\n",
    "query_cells = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & curation_key\n",
    "    & \"mask_center_z=0\"\n",
    "    & \"mask_npix > 1\"\n",
    ").proj()\n",
    "\n",
    "# query_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropilcorr = True\n",
    "\n",
    "fluorescence_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "neuropil_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"neuropil_fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "if neuropilcorr:\n",
    "    print(\"DOING VANILLA NEUROPIL CORRECTION NOW!\")\n",
    "    fluorescence_traces = fluorescence_traces - 0.7 * neuropil_traces\n",
    "\n",
    "activity_traces = (imaging.Activity.Trace & query_cells).fetch(\n",
    "    \"activity_trace\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "sampling_rate = (scan.ScanInfo & curation_key).fetch1(\"fps\")\n",
    "\n",
    "# timebase_2p = np.r_[: fluorescence_traces[0].size] * 1 / sampling_rate\n",
    "\n",
    "timebase_2p = np.linspace(0, fluorescence_traces[0].size / sampling_rate, fluorescence_traces[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap import Rastermap\n",
    "from scipy import stats \n",
    "from scipy.stats import zscore\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# stack fluorescence for rastermap\n",
    "fluos = np.vstack(fluorescence_traces)\n",
    "\n",
    "nan_mask = np.isnan(fluos).any(axis=1)\n",
    "S = fluos[~nan_mask]\n",
    "S = zscore(S, axis=1)\n",
    "\n",
    "rmmodel = Rastermap(n_clusters=None, # None turns off clustering and sorts single neurons \n",
    "                  n_PCs=80, # use fewer PCs than neurons\n",
    "                  locality=0.15, # some locality in sorting (this is a value from 0-1)\n",
    "                  time_lag_window=15, # use future timepoints to compute correlation\n",
    "                  grid_upsample=0, # 0 turns off upsampling since we're using single neurons\n",
    "                ).fit(S)\n",
    "\n",
    "\n",
    "y = rmmodel.embedding # neurons x 1\n",
    "isort = rmmodel.isort\n",
    "\n",
    "# sort by embedding and smooth over neurons (uncomment)\n",
    "\n",
    "# Sfilt = gaussian_filter1d(S[isort], np.minimum(1,np.maximum(1,int(S.shape[0]*0.001))),axis=0)\n",
    "Sfilt = S[isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sorted data\n",
    "# load plot styles for display\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(Sfilt, vmin = -0.1, vmax=1, extent= [timebase_2p[0], timebase_2p[-1], 0, Sfilt.shape[0]], aspect='auto', cmap='gray_r')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('sorted neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "offset_scaler = 10 # We want to plot every cell with a little offset to the last one\n",
    "for no,trace in enumerate(Sfilt):\n",
    "    if no == 80: break # not more than 80\n",
    "\n",
    "    # get the neuropil corrected values for that trace:\n",
    "    # trace = Sfilt\n",
    "    ax.plot(timebase_2p,trace + (no*offset_scaler),lw=1,c='k',alpha=.8)\n",
    "\n",
    "ax.set_xlim(0,timebase_2p[-1])\n",
    "\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.set_title('Sorted z-scored traces')    \n",
    "\n",
    "ax.set_ylabel('Cells')\n",
    "ax.set_xlabel('Time [s]')\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='bench2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "aligned_wheel_indices = get_closest_timestamps(twoptimestamps,auxtime[:-window]) #smoothing windwo from above\n",
    "\n",
    "# use this to index into the wheelspeed\n",
    "angular_velocity_smoothed_2pref = angular_velocity_smoothed[aligned_wheel_indices]\n",
    "\n",
    "# both arrays have same shape now - now 2pdata and wheel speed can be plotted together on the 2ptimestamps\n",
    "print(np.shape(twoptimestamps))\n",
    "print(np.shape(angular_velocity_smoothed_2pref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_colors = np.array([[0.55,0.55,0.55]])\n",
    "\n",
    "\n",
    "# timepoints to visualize\n",
    "tstart = 0\n",
    "tend =  timebase_2p[-1] - 10\n",
    "\n",
    "xmin = int(np.floor(tstart * sampling_rate))\n",
    "xmax = int(np.floor(tend * sampling_rate))\n",
    "\n",
    "# make figure with grid for easy plotting\n",
    "fig = plt.figure(figsize=(8,5), dpi=200)\n",
    "grid = plt.GridSpec(9, 20, figure=fig, wspace = 0.05, hspace = 0.3)\n",
    "\n",
    "# plot running speed\n",
    "ax = plt.subplot(grid[:2, :-1])\n",
    "ax.plot(angular_velocity_smoothed_2pref,  color=kp_colors[0])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "# plot superneuron activity\n",
    "ax = plt.subplot(grid[2:, :-1])\n",
    "ax.imshow(Sfilt[:, xmin:xmax], cmap=\"gray_r\", vmin=-0.1, vmax=0.7,  extent= [timebase_2p[xmin], timebase_2p[xmax], 0, Sfilt.shape[0]], aspect=\"auto\")\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"sorted boutons\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ax = plt.subplot(grid[1:, -1])\n",
    "# ax.imshow(np.arange(0, len(sn))[:,np.newaxis], cmap=\"gist_ncar\", aspect=\"auto\")\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini2p figures and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini2p1_openfield\n",
      "/datajoint-data/data/tobiasr/NK_ROS-1629_2023-10-19_scan9FKNRW9Y_sess9FKNRW9Y\n"
     ]
    }
   ],
   "source": [
    "# first define a key to be used across multiple tables\n",
    "\n",
    "scansi = \"scan9FKNRW9Y\"\n",
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')\n",
    "# curation_key = (imaging.Curation & scan_key & 'curation_id=10').fetch1('KEY') # SET THE CURATION ID OF MANUAL CURATION HERE!\n",
    "sessi = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('session_id')[0]\n",
    "aux_setup_typestr = (scan.ScanInfo() & scan_key).fetch(\"userfunction_info\")[0] # check setup type (not needed)\n",
    "print(aux_setup_typestr)\n",
    "print((scan.ScanPath & scan_key).fetch(\"path\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and show overview images from suite2p registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('ref_image')\n",
    "average_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('average_image')\n",
    "correlation_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('correlation_image')\n",
    "max_proj_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('max_proj_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some scaling values from pixel distribution\n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "offset = 0\n",
    "\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin)  \n",
    "cmax = np.percentile(average_image,scalemax) + offset\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "# Make figure with all templates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(ref_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Reference Image for Registration\");\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Registered Image, Mean Projection\");\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(max_proj_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(correlation_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Correlation Map\")\n",
    "plt.show(block=False)\n",
    "# plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just mean image\n",
    "scalemin = 5\n",
    "scalemax = 100\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin) \n",
    "cmax = np.percentile(average_image,scalemax) #+ offset\n",
    "\n",
    "\n",
    "# plt.subplot(1, 4, 1)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make cam-synchronized movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import skvideo.io\n",
    "\n",
    "# get the movie file name from the database\n",
    "topfile = (model.VideoRecording.File & scan_key).fetch('file_path')[0]\n",
    "\n",
    "#load to array\n",
    "videodata = skvideo.io.vread(str(topfile))\n",
    "# videodata = np.asarray([skvideo.io.vshape(frame)[0] for frame in videodata], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get / make DLC overlay video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import skvideo.io\n",
    "\n",
    "key =  (model.PoseEstimationTask & f'recording_id=\"{scansi}\"').fetch1('KEY')\n",
    "destfolder = (model.PoseEstimationTask & key).fetch1('pose_estimation_output_dir')\n",
    "\n",
    "labeled_videofile = glob.glob(f\"{destfolder}/*.mp4\")\n",
    "\n",
    "# DEEPLABCUT OVERLAY - CURRENTLY ONLY WORKING IN MY ENVIRONMENT. NEED TO CHECK VERSIONS\n",
    "\n",
    "# key = (model.VideoRecording & scan_key).fetch1('KEY')\n",
    "# key.update({'model_name': 'Head_orientation-NK', 'task_mode': 'trigger'})\n",
    "\n",
    "\n",
    "# from deeplabcut.utils.make_labeled_video import create_labeled_video\n",
    "# import yaml\n",
    "# from element_interface.utils import find_full_path\n",
    "# from adamacs.paths import get_dlc_root_data_dir\n",
    "\n",
    "\n",
    "# destfolder = model.PoseEstimationTask.infer_output_dir(scan_key)\n",
    "\n",
    "# config_paths = sorted( # Of configs in the project path, defer to the datajoint-saved\n",
    "#     list(\n",
    "#         find_full_path(\n",
    "#             get_dlc_root_data_dir(), ((model.Model & key).fetch1(\"project_path\"))\n",
    "#         ).glob(\"*.y*ml\")\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# create_labeled_video( # Pass strings to label the video\n",
    "#     config=str(config_paths[-1]),\n",
    "#     videos=str(topfile),\n",
    "#     destfolder=str(destfolder),\n",
    "# )\n",
    "\n",
    "\n",
    "# labeled_videofile = '/datajoint-data/data/tobiasr/RN_OPI-1681_2023-04-05_scan9FHELAYA_sess9FHELAYA/device_mini2p1_top_recording_scan9FHELAYA_model_Head_orientation-NK/scan9FHELAYA_top_video_2023-04-05T15_19_53DLC_resnet50_Head_orientationJul17shuffle1_90000_labeled.mp4'\n",
    "\n",
    "print(destfolder)\n",
    "\n",
    "# labeled_videodata = skvideo.io.vread(str(labeled_videofile[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load moving average registered Ca2+ imaging movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the registered moving average (blinking) movie data of the specified scanID\n",
    "\n",
    "scandir = (scan.ScanPath & scan_key).fetch('path')[0]\n",
    "\n",
    "directory = Path(scandir + \"/suite2p/plane0/reg_tif\")\n",
    "pattern = '*20_frame*.mp4'\n",
    "files = list(directory.glob(pattern))\n",
    "blinkvideodata = skvideo.io.vread(str(files[0]))\n",
    "blinkvideodata = np.asarray([skvideo.io.vshape(frame)[0] for frame in blinkvideodata], dtype=np.uint8)\n",
    "print(files[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display camaravideo with slider\n",
    "sh.display_volume_z(videodata,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 2pstackvideo with slider\n",
    "sh.display_volume_z(blinkvideodata,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 2pstackvideo with slider\n",
    "sh.display_volume_z(labeled_videodata,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionts of the original movie (frames, x,y,rgb)\n",
    "print(videodata.shape)\n",
    "print(blinkvideodata.shape)\n",
    "print(labeled_videodata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the timestamp data and gate / offset cameraframes\n",
    "\n",
    "# from the event table get the main recording gate start / end timestamps.\n",
    "auxgatetimestamp_end = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_end_time')\n",
    "auxgatetimestamp_start = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# Then return camera start timestamps within the recording gate only (not necessary: he important bit is to subtract the offsetframes after alignment)\n",
    "cameratimestamps = (event.Event()  &  \"event_type='aux_cam'\" & f\"event_start_time>{auxgatetimestamp_start[0]}\" & f\"event_start_time<{auxgatetimestamp_end[0]}\" & scan_key).fetch('event_start_time')\n",
    "\n",
    "cameraoffsetframes =len((event.Event()  &  \"event_type='aux_cam'\" & f\"event_start_time<{auxgatetimestamp_start[0]}\" & scan_key).fetch('event_start_time'))\n",
    "\n",
    "#  and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='mini2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# align the two recordings by finding the indices of the closest camera timestamp that fits the 2p frame timestamps by sorted list insertion (\"bisect\"). Be aware: camera frames can be double.\n",
    "aligned_cameraframes = np.array(get_closest_timestamps(twoptimestamps,cameratimestamps)) - cameraoffsetframes\n",
    "\n",
    "# this should have the same shape as the 2p frames:\n",
    "print(np.shape(aligned_cameraframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoptimestamps =  twoptimestamps[:np.shape(blinkvideodata)[0]] # truncating 2p timestamps to number of 2p videoframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now use this camara frame indices to reslice the video (which now is aligned to the 2p frames on a frame-by-frame level)\n",
    "# resliced_cam_video = videodata[aligned_cameraframes]\n",
    "resliced_cam_video = labeled_videodata[aligned_cameraframes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display synchronized movie\n",
    "sh.display_volume_z(resliced_cam_video,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale camera movie to fit size of 2p movie (can take a lot of time and memory)\n",
    "rescaled_cam_movie = resize_movie(resliced_cam_video, np.shape(blinkvideodata)[1],np.shape(blinkvideodata)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(rescaled_cam_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate and display movies\n",
    "concatmovie = np.concatenate((blinkvideodata,rescaled_cam_movie), axis = 2)\n",
    "sh.display_volume_z(concatmovie,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as new movie (without rescaling)\n",
    "\n",
    "\n",
    "filename = str(directory) + '/aligned_stack_cam_movie.mp4'\n",
    "fps = (scan.ScanInfo & scan_key).fetch('fps')\n",
    "# p1 = 0\n",
    "# p2 = 100\n",
    "# trash = sh.make_stack_movie(concatmovie, filename, fps[0], p1, p2)\n",
    "\n",
    "codecset = 'libx264'\n",
    "import imageio\n",
    "import imageio.plugins.ffmpeg as ffmpeg\n",
    "\n",
    "# Create an imageio VideoWriter object to write the video\n",
    "writer = imageio.get_writer(filename, fps=fps[0], codec=codecset, output_params=['-crf', '19'])\n",
    "\n",
    "# # Calculate the 1st and 99th percentile\n",
    "# p1, p99 = np.percentile(running_z_projection[:500,:,:], (p1set, p2set))\n",
    "\n",
    "# # rescale to 8 bit\n",
    "# rescaled_image_8bit = rescale_image_multithreaded(running_z_projection, p1, p99)\n",
    "\n",
    "for page in concatmovie:\n",
    "    writer.append_data(page)\n",
    "\n",
    "# Close the video writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed up, add timestamps etc - all with fast ffmpeg operations\n",
    "\n",
    "import os\n",
    "\n",
    "spedby = 5\n",
    "setpts_value = 1/spedby # change this to your desired value\n",
    "newfps = fps[0]*spedby\n",
    "\n",
    "input_filename = filename\n",
    "# 2. Add timestamps\n",
    "\n",
    "output_filename = str(directory) + '/' + scansi + '_top_video_concatenated' + 'withtimestamps.mp4'\n",
    "command = f\"\"\"ffmpeg -y -i {input_filename} -vf \"drawtext=fontfile=/Library/Fonts/Arial.ttf:timecode='00\\\\\\\\:00\\\\\\\\:00\\\\\\\\:00':rate={fps[0]}:text='':fontsize=20:fontcolor=white:x=530:y=20:box=1:boxcolor=0x00000000@1\" -f mp4 {output_filename}\"\"\"\n",
    "\n",
    "os.system(command)\n",
    "\n",
    "\n",
    "input_filename = output_filename  # 'sped_up_video.mp4'\n",
    "output_filename = str(directory) + '/' +  scansi + '_top_video_concatenated_spedup_' + str(spedby) + 'fold_withtimestamps_labels.mp4'\n",
    "\n",
    "command = f'ffmpeg -y -i {input_filename} -vf \"setpts={setpts_value}*PTS\" -r {newfps}  {output_filename}'\n",
    "\n",
    "\n",
    "\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the fluorescence traces of this recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask positions of masks that are classified as cells and that are larger than a certain pixel size\n",
    "mask_xpix, mask_ypix = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & scan_key\n",
    "    & \"mask_npix > 10\"\n",
    "    & \"curation_id = 10\" #SET CURATION ID OF MANUAL CURATION HERE\n",
    ").fetch(\"mask_xpix\", \"mask_ypix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this query, we've fetched the coordinates of segmented masks. We can overlay these\n",
    "masks onto our average image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_image = np.zeros(np.shape(average_image), dtype=bool)\n",
    "for xpix, ypix in zip(mask_xpix, mask_ypix):\n",
    "    mask_image[ypix, xpix] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.contour(mask_image, colors=\"red\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example using queries - plot fluorescence and deconvolved activity\n",
    "traces:\n",
    "\n",
    "Here we fetch the primary key attributes of the entry with `curation_id=0` for the\n",
    "current session in the `imaging.Curation` table. \n",
    "\n",
    "Then, we fetch all cells that fit the\n",
    "restriction criteria from `imaging.Segmentation.Mask` and\n",
    "`imaging.MaskClassification.MaskType` as a `projection`. \n",
    "\n",
    "We then use this projection as\n",
    "a restriction to fetch and plot fluorescence and deconvolved activity traces from the\n",
    "`imaging.Fluorescence.Trace` and `imaging.Activity.Trace` tables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key = (imaging.Curation & scan_key & \"curation_id=10\").fetch1(\"KEY\")\n",
    "query_cells = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & curation_key\n",
    "    & \"mask_center_z=0\"\n",
    "    & \"mask_npix > 10\"\n",
    ").proj()\n",
    "\n",
    "# query_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Fluorescence.Trace & scan_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropilcorr = True\n",
    "\n",
    "fluorescence_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "neuropil_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"neuropil_fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "if neuropilcorr:\n",
    "    print(\"DOING VANILLA NEUROPIL CORRECTION NOW!\")\n",
    "    fluorescence_traces = fluorescence_traces - 0.7 * neuropil_traces\n",
    "\n",
    "activity_traces = (imaging.Activity.Trace & query_cells).fetch(\n",
    "    \"activity_trace\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "sampling_rate = (scan.ScanInfo & curation_key).fetch1(\"fps\")\n",
    "\n",
    "# timebase_2p = np.r_[: fluorescence_traces[0].size] * 1 / sampling_rate\n",
    "\n",
    "timebase_2p = np.linspace(0, fluorescence_traces[0].size / sampling_rate, fluorescence_traces[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap import Rastermap\n",
    "from scipy import stats \n",
    "from scipy.stats import zscore\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "\n",
    "\n",
    "# stack fluorescence for rastermap\n",
    "fluos = np.vstack(fluorescence_traces)\n",
    "\n",
    "nan_mask = np.isnan(fluos).any(axis=1)\n",
    "\n",
    "# Create a mask for rows containing only zeros\n",
    "zero_rows = np.all(fluos == 0, axis=1)\n",
    "\n",
    "# Create a mask for rows containing only inf\n",
    "inf_rows = np.all(np.isinf(fluos), axis=1)\n",
    "\n",
    "# Create a mask for rows containing only NaN\n",
    "nan_rows = np.all(np.isnan(fluos), axis=1)\n",
    "\n",
    "# Combine the masks using logical OR\n",
    "mask_to_remove = zero_rows | inf_rows | nan_rows | nan_mask\n",
    "\n",
    "S = fluos[~mask_to_remove]\n",
    "S = zscore(S, axis=1)\n",
    "\n",
    "rmmodel = Rastermap(n_clusters=None, # None turns off clustering and sorts single neurons \n",
    "                  n_PCs=24, # use fewer PCs than neurons\n",
    "                  locality=0.15, # some locality in sorting (this is a value from 0-1)\n",
    "                  time_lag_window=15, # use future timepoints to compute correlation\n",
    "                  grid_upsample=0, # 0 turns off upsampling since we're using single neurons\n",
    "                ).fit(S)\n",
    "\n",
    "\n",
    "y = rmmodel.embedding # neurons x 1\n",
    "isort = rmmodel.isort\n",
    "\n",
    "# sort by embedding and smooth over neurons (uncomment)\n",
    "\n",
    "# Sfilt = gaussian_filter1d(S[isort], np.minimum(1,np.maximum(1,int(S.shape[0]*0.001))),axis=0)\n",
    "Sfilt = S[isort]\n",
    "Sfilt_backup = Sfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sorted data\n",
    "# load plot styles for display\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(Sfilt, vmin = -0.1, vmax=2, extent= [timebase_2p[0], timebase_2p[-1], 0, Sfilt.shape[0]], aspect='auto', cmap='gray_r', interpolation='none')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('sorted neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "offset_scaler = 10 # We want to plot every cell with a little offset to the last one\n",
    "for no,trace in enumerate(Sfilt):\n",
    "    # if no == 25: break # not more than 80\n",
    "\n",
    "    # get the neuropil corrected values for that trace:\n",
    "    # trace = Sfilt\n",
    "    ax.plot(timebase_2p,trace + (no*offset_scaler),lw=1,c='k',alpha=.8)\n",
    "\n",
    "ax.set_xlim(0,timebase_2p[-1])\n",
    "\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.set_title('Sorted z-scored traces')    \n",
    "\n",
    "ax.set_ylabel('Cells')\n",
    "ax.set_xlabel('Time [s]')\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarized event visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "\n",
    "method = \"tank\"\n",
    "# method = \"sd\"\n",
    "# method = \"poor\"\n",
    "\n",
    "cutoff_std = float(2)\n",
    "min_transient_length = .1\n",
    "\n",
    "filtered_event_mask = []\n",
    "\n",
    "for cell in range(len(activity_traces)):  \n",
    "    # Horst's event filtering \n",
    "    re = th.FilterEvents(activity_traces[cell])\n",
    "    if method == \"tank\":\n",
    "        transient_dict = re.transients(fluorescence_traces[cell],\n",
    "                    np.ones_like(fluorescence_traces[cell], dtype=bool),\n",
    "                    sampling_rate, cutoff_std, min_transient_length, plot=False)\n",
    "    elif method == \"sd\":\n",
    "        transient_dict = re.robust(cutoff_std)  \n",
    "    filtered_event_mask.append(transient_dict['mask_events'])\n",
    "\n",
    "event_matrix = np.array(filtered_event_mask)\n",
    "\n",
    "\n",
    "if method == \"poor\":\n",
    "    event_matrix = np.array([item > cutoff_std for item in Sfilt], dtype=bool).astype(int)\n",
    "\n",
    "\n",
    "# broaden events to generate non-event baseline for SNR calculation and second pass event detection with proper f0 baseline\n",
    "\n",
    "cutoff_std = float(2)\n",
    "min_transient_length = .250 # s\n",
    "\n",
    "broaden_by = 1 # s\n",
    "\n",
    "# Structuring element for one-second dilation\n",
    "structure = np.ones(int(broaden_by * sampling_rate))\n",
    "\n",
    "filtered_event_mask_2ndpass =[]\n",
    "broadened_events = []\n",
    "filtered_events = []\n",
    "SNR = []\n",
    "dFF0_traces = []\n",
    "\n",
    "if method == \"tank\":\n",
    "    for cell in range(len(activity_traces)):\n",
    "        re = th.FilterEvents(activity_traces[cell])\n",
    "        \n",
    "        # make dF/F0 from first pass events\n",
    "        F0mean = np.mean(fluorescence_traces[cell][np.logical_not(event_matrix[cell])])\n",
    "        dFF0 = (fluorescence_traces[cell] - F0mean) / F0mean\n",
    "        \n",
    "        # Apply binary dilation on previous events and invert to get baseline\n",
    "        \n",
    "        broadened_events = binary_dilation(filtered_event_mask[cell], structure=structure) \n",
    "        \n",
    "        transient_dict = re.transients(dFF0,\n",
    "                    np.logical_not(broadened_events),\n",
    "                    sampling_rate, cutoff_std, min_transient_length, plot=False)\n",
    " \n",
    "        filtered_event_mask_2ndpass.append(transient_dict['mask_events'])\n",
    "        filtered_events.append(transient_dict['filtered_events'])      \n",
    "        \n",
    "        SNR.append(np.mean(filtered_events[cell] / np.nanstd(dFF0[np.logical_not(broadened_events)])))\n",
    "        dFF0_traces.append(dFF0)\n",
    "        \n",
    "    event_matrix_2ndpass = np.array(filtered_event_mask_2ndpass)\n",
    "\n",
    "\n",
    "\n",
    "# sort events based on previous rastermap embedding\n",
    "event_matrix = event_matrix[isort]\n",
    "event_matrix_2ndpass = event_matrix_2ndpass[isort]\n",
    "dFF0_traces = np.array(dFF0_traces)[isort]\n",
    "\n",
    "\n",
    "# plot events and non-event epochs (with traces)\n",
    "\n",
    "figure = plt.figure(figsize=(15,20))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "\n",
    "offset_scaler = 1.5 # We want to plot every cell with a little offset to the last one\n",
    "for no, (trace, trace2) in enumerate(zip(event_matrix, event_matrix_2ndpass)):\n",
    "    if no == 80: break # not more than 80\n",
    "\n",
    "    ax.plot(timebase_2p, trace + (no * offset_scaler), lw=1, c='k', alpha=.8)\n",
    "    ax.plot(timebase_2p, trace2 + (no * offset_scaler), lw=2, c='r', alpha=.8)\n",
    "\n",
    "\n",
    "ax.set_xlim(0,timebase_2p[-1])\n",
    "\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.set_title('Sorted binarized deconvolved traces')    \n",
    "\n",
    "ax.set_ylabel('Cells')\n",
    "ax.set_xlabel('Time [s]')\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#set used event_matrix\n",
    "event_matrix = event_matrix_2ndpass\n",
    "event_matrix_backup = event_matrix_2ndpass\n",
    "SNR_matrix = np.array(SNR)[isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "SNRthresh = .5\n",
    "\n",
    "offset_scaler = 12 # We want to plot every cell with a little offset to the last one\n",
    "for no, (trace,evnts) in enumerate(zip(dFF0_traces[SNR_matrix > SNRthresh,:], event_matrix[SNR_matrix > SNRthresh,:] * 1)):\n",
    "    if no == 15: break # not more than 25\n",
    "\n",
    "    # get the neuropil corrected values for that trace:\n",
    "    # trace = Sfilt\n",
    "    ax.plot(timebase_2p,trace + (no*offset_scaler),lw=1,c='k',alpha=.8)\n",
    "    ax.plot(timebase_2p, evnts + (no*offset_scaler - 2),lw=2,c='r',alpha=.8)\n",
    "\n",
    "ax.set_xlim(0,timebase_2p[-1])\n",
    "\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.set_title('Sorted, SNR-filtered dF/F0 traces')    \n",
    "\n",
    "ax.set_ylabel('Cells')\n",
    "ax.set_xlabel('Time [s]')\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit data to SNR threshold from hereon\n",
    "Sfilt = Sfilt_backup[SNR_matrix > SNRthresh]\n",
    "event_matrix = event_matrix[SNR_matrix > SNRthresh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26500,)\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "## Get the timestamp data and gate / offset cameraframes\n",
    "\n",
    "# from the event table get the main recording gate start / end timestamps.\n",
    "auxgatetimestamp_end = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_end_time')\n",
    "auxgatetimestamp_start = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# Then return camera start timestamps within the recording gate only \n",
    "cameratimestamps = (event.Event()  &  \"event_type='aux_cam'\" & f\"event_start_time>{auxgatetimestamp_start[0]}\" & f\"event_start_time<{auxgatetimestamp_end[0]}\" & scan_key).fetch('event_start_time')\n",
    "cameratimestamps_end = (event.Event()  &  \"event_type='aux_cam'\"  & f\"event_start_time<{auxgatetimestamp_end[0]}\" & scan_key).fetch('event_start_time')\n",
    "cameraoffsetframes =len((event.Event()  &  \"event_type='aux_cam'\" & f\"event_start_time<{auxgatetimestamp_start[0]}\" & scan_key).fetch('event_start_time'))\n",
    "\n",
    "cameratimestamps_low = (event.Event()  &  \"event_type='aux_cam'\"  & f\"event_start_time<{auxgatetimestamp_end[0]}\" & scan_key).fetch('event_start_time')\n",
    "cameratimestamps_all = (event.Event()  &  \"event_type='aux_cam'\"   & scan_key).fetch('event_start_time')\n",
    "\n",
    "\n",
    "#  and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='mini2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# align the two recordings by finding the indices of the closest camera timestamp that fits the 2p frame timestamps by sorted list insertion (\"bisect\"). Be aware: camera frames can be double.\n",
    "aligned_cameraframes = np.array(get_closest_timestamps(twoptimestamps,cameratimestamps)) - cameraoffsetframes\n",
    "\n",
    "# this should have the same shape as the 2p frames:\n",
    "print(np.shape(aligned_cameraframes))\n",
    "print(cameraoffsetframes)\n",
    "\n",
    "# cameraoffsetframes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cameratimestamps))\n",
    "print(len(cameratimestamps_low))\n",
    "print(cameraoffsetframes)\n",
    "# np.shape(labeled_videodata)\n",
    "np.shape(videodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for camera framedrops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamacs.paths import get_experiment_root_data_dir\n",
    "behavior_path_relative = (event.BehaviorRecording.File & scan_key &  \"filepath LIKE '%.mat%'\").fetch1(\"filepath\")\n",
    "camera_timestamp_paths = list(find_full_path(\n",
    "    get_experiment_root_data_dir(), behavior_path_relative\n",
    ").parent.glob(\"*top_video_timestamps*.csv\"))\n",
    "\n",
    "camera_timestamp_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allchans_sync = CamLoader_sync(camera_timestamp_paths[0]).data_for_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allchans_sync[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(allchans_sync[0][\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(np.diff(allchans_sync[0][\"time\"]) > 30)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin width (1 ms in this case)\n",
    "bin_width = .1\n",
    "\n",
    "data = np.diff(new_timestamps)\n",
    "\n",
    "# Calculate the number of bins\n",
    "num_bins = int((max(data) - min(data)) / bin_width)\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(data, bins=num_bins, edgecolor='black')\n",
    "# plt.hist(data, edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram with 1ms Bin Width')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_frame_drops(timestamps, threshold):\n",
    "    # Calculate time intervals between consecutive frames\n",
    "    intervals = np.diff(timestamps)\n",
    "    \n",
    "    # Identify indices where the interval exceeds the threshold\n",
    "    drop_indices = np.where(intervals > threshold)[0]\n",
    "    \n",
    "    return drop_indices\n",
    "\n",
    "def insert_frames(timestamps, drop_indices):\n",
    "    new_timestamps = np.copy(timestamps)\n",
    "    \n",
    "    for idx in drop_indices:\n",
    "        # Insert a new timestamp halfway between the adjacent frames\n",
    "        new_timestamp = (timestamps[idx] + timestamps[idx + 1]) / 2.0\n",
    "        new_timestamps = np.insert(new_timestamps, idx + 1, new_timestamp)\n",
    "    \n",
    "    return new_timestamps\n",
    "\n",
    "# Example usage:\n",
    "timestamps = allchans_sync[0][\"time\"]\n",
    "threshold = 25  # Adjust this threshold based on your application\n",
    "\n",
    "drop_indices = detect_frame_drops(timestamps, threshold)\n",
    "new_timestamps = insert_frames(timestamps, drop_indices)\n",
    "\n",
    "print(\"Original Timestamps:\", timestamps)\n",
    "print(\"Indices of Frame Drops:\", drop_indices)\n",
    "print(\"New Timestamps with Inserted Frames:\", new_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### align movie to individual events and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cricket data aligned\n",
    "leftcricket_start = (trial.Trial()  &  \"trial_type='BPOD: SubCricket; 60'\" &  scan_key ).fetch('trial_start_time')\n",
    "rightcricket_start = (trial.Trial()  &  \"trial_type='BPOD: SubCricket; -60'\" &  scan_key ).fetch('trial_start_time')\n",
    "\n",
    "allcricket_triggered = (event.Event()  &  \"event_type='aux_bpod_visual'\" &  scan_key ).fetch('event_start_time')\n",
    "reward_triggered = (event.Event()  &  \"event_type='aux_bpod_reward'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "target_triggered = (event.Event()  &  \"event_type='bpod_at_target'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# rightcricket_start = (trial.Trial()  &  \"trial_type='BPOD: SubCricket; -60'\" &  scan_key ).fetch('trial_start_time')\n",
    "\n",
    "\n",
    "aligned_cameraframes_leftcricket_start = np.array(get_closest_timestamps(leftcricket_start,cameratimestamps)) - cameraoffsetframes\n",
    "\n",
    "aligned_cameraframes_rightcricket_start = np.array(get_closest_timestamps(rightcricket_start,cameratimestamps)) - cameraoffsetframes\n",
    "\n",
    "\n",
    "aligned_cameraframes_allcricket_triggered = np.array(get_closest_timestamps(allcricket_triggered,cameratimestamps)) - cameraoffsetframes\n",
    "aligned_cameraframes_reward_triggered = np.array(get_closest_timestamps(reward_triggered,cameratimestamps)) - cameraoffsetframes\n",
    "\n",
    "aligned_cameraframes_target_triggered = np.array(get_closest_timestamps(target_triggered,cameratimestamps)) - cameraoffsetframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_cameraframes_target_triggered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data from the event.Event and trial.Trial tables\n",
    "event_data = (event.Event & scan_key).fetch('event_type', 'event_start_time', as_dict=True)\n",
    "trial_data = (trial.Trial & scan_key).fetch('trial_id', 'trial_type', 'trial_start_time', as_dict=True)\n",
    "\n",
    "pltn = djh.plot_event_trial_start_times(event_data, trial_data)\n",
    "pltn.xlim([0, 20])\n",
    "pltn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with even-aligned videosnippets \n",
    "\n",
    "# Assuming videodata and cameratimestamps are defined\n",
    "cameraspeed = 1 / (np.mean(np.diff(cameratimestamps)))\n",
    "frames_per_second = int(cameraspeed)\n",
    "starter = int(.1 * frames_per_second)\n",
    "ender = int(2 * frames_per_second)\n",
    "\n",
    "def create_snippets(videodata, indices, fps):\n",
    "    snippets = []\n",
    "    for index in indices:\n",
    "        start = max(index - starter, 0)\n",
    "        end = min(index + ender, len(videodata))\n",
    "        snippet = videodata[start:end]\n",
    "        snippets.append(snippet)\n",
    "    return snippets\n",
    "\n",
    "snippets = create_snippets(labeled_videodata, aligned_cameraframes_rightcricket_start, frames_per_second)\n",
    "# vertical_concat = np.concatenate(snippets, axis=1)  # Vertical concatenation\n",
    "# horizontal_concat = np.concatenate(snippets, axis=2)  # Horizontal concatenation\n",
    "# overview_video = np.concatenate([vertical_concat, horizontal_concat], axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh.display_volume_z(snippets[10],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equalize the snippet length before concatenating\n",
    "\n",
    "import math\n",
    "\n",
    "def equalize_snippets(snippets):\n",
    "    # Find the maximum number of frames in any snippet\n",
    "    max_frames = max(snippet.shape[0] for snippet in snippets)\n",
    "\n",
    "    # Equalize the number of frames in each snippet\n",
    "    equalized_snippets = []\n",
    "    for snippet in snippets:\n",
    "        if snippet.shape[0] < max_frames:\n",
    "            # Pad the snippet with blank frames or repeat the last frame\n",
    "            padding = np.zeros((max_frames - snippet.shape[0],) + snippet.shape[1:])\n",
    "            snippet_padded = np.concatenate([snippet, padding], axis=0)\n",
    "            equalized_snippets.append(snippet_padded)\n",
    "        else:\n",
    "            equalized_snippets.append(snippet)\n",
    "    \n",
    "    return equalized_snippets\n",
    "\n",
    "# Equalize the snippets before concatenating\n",
    "equalized_snippets = equalize_snippets(snippets)\n",
    "# vertical_concat = np.concatenate(equalized_snippets, axis=1)\n",
    "# horizontal_concat = np.concatenate(equalized_snippets, axis=2)\n",
    "# overview_video = np.concatenate([vertical_concat, horizontal_concat], axis=0)\n",
    "\n",
    "\n",
    "# Assuming equalized_snippets is a list of video snippets of equal frame counts\n",
    "def concatenate_to_grid(snippets):\n",
    "    # Determine the grid size\n",
    "    num_snippets = len(snippets)\n",
    "    grid_size = int(math.ceil(math.sqrt(num_snippets)))\n",
    "\n",
    "    # Initialize placeholders for rows and the final grid\n",
    "    rows = []\n",
    "    final_grid = None\n",
    "\n",
    "    # Concatenate snippets into rows\n",
    "    for i in range(0, num_snippets, grid_size):\n",
    "        row = snippets[i:i + grid_size]\n",
    "        while len(row) < grid_size:  # Pad the row if necessary\n",
    "            row.append(np.zeros_like(snippets[0]))\n",
    "        concatenated_row = np.concatenate(row, axis=2)  # Concatenate along width\n",
    "        rows.append(concatenated_row)\n",
    "\n",
    "    # Concatenate rows to form the grid\n",
    "    final_grid = np.concatenate(rows, axis=1)  # Concatenate along height\n",
    "\n",
    "    return final_grid\n",
    "\n",
    "# Concatenate the snippets into a grid\n",
    "grid_video = concatenate_to_grid(equalized_snippets[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh.display_volume_z(grid_video,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to 4k movie\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_video_frames(grid_data, target_width=5504, target_height=5008):\n",
    "    resized_video = []\n",
    "\n",
    "    for frame in grid_data:\n",
    "        # Resize frame to 4K resolution\n",
    "        resized_frame = cv2.resize(frame, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
    "        resized_video.append(resized_frame)\n",
    "\n",
    "    return np.array(resized_video)\n",
    "\n",
    "# Assuming grid_data is a numpy array of video frames\n",
    "grid_data_4k = resize_video_frames(grid_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write movie\n",
    "\n",
    "import imageio\n",
    "import imageio.plugins.ffmpeg as ffmpeg\n",
    "\n",
    "#save as new movie (without rescaling)\n",
    "scandir = (scan.ScanPath & scan_key).fetch('path')[0]\n",
    "\n",
    "directory = Path(scandir + \"/\")\n",
    "\n",
    "filename = str(directory) + '/concatenated_peri_rightcricket_movie2.mp4'\n",
    "fps = frames_per_second\n",
    "# p1 = 0\n",
    "# p2 = 100\n",
    "# trash = sh.make_stack_movie(concatmovie, filename, fps[0], p1, p2)\n",
    "\n",
    "codecset = 'libx264'\n",
    "\n",
    "\n",
    "# Create an imageio VideoWriter object to write the video\n",
    "writer = imageio.get_writer(filename, fps=fps, codec=codecset, output_params=['-crf', '19'])\n",
    "\n",
    "# # Calculate the 1st and 99th percentile\n",
    "# p1, p99 = np.percentile(running_z_projection[:500,:,:], (p1set, p2set))\n",
    "\n",
    "# # rescale to 8 bit\n",
    "# rescaled_image_8bit = rescale_image_multithreaded(running_z_projection, p1, p99)\n",
    "\n",
    "for page in grid_data_4k:\n",
    "    writer.append_data(page)\n",
    "\n",
    "# Close the video writer\n",
    "writer.close()\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftcricket_start = (trial.Trial()  &  \"trial_type='BPOD: SubCricket; 60'\" &  scan_key ).fetch('trial_start_time')\n",
    "rightcricket_start = (trial.Trial()  &  \"trial_type='BPOD: SubCricket; -60'\" &  scan_key ).fetch('trial_start_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the timestamps of the video synchronization from above are the one to use for synchronized plotting of positions etc: aligned_cameraframes\n",
    "print(np.shape(cameratimestamps))\n",
    "print(np.shape(aligned_cameraframes))\n",
    "print(np.shape(twoptimestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now do some positional plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/datajoint-data/data/tobiasr/NK_ROS-1629_2023-10-19_scan9FKNRW9Y_sess9FKNRW9Y/scan9FKNRW9Y_mini2p1_top_video_2023-10-19T10_44_15.mp4'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlc_scan_key = (model.PoseEstimation & f'recording_id = \"{scan_key[0][\"scan_id\"]}\"').fetch('KEY')\n",
    "path = (model.VideoRecording.File & scan_key).fetch(\"file_path\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recording_id = \"scan9FKNRW9Y\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'recording_id = \"{scan_key[0][\"scan_id\"]}\"'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PoseEstimation' has no attribute 'task'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPoseEstimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dj_pure/lib/python3.8/site-packages/datajoint/user_tables.py:56\u001b[0m, in \u001b[0;36mTableMeta.__getattribute__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# trigger instantiation for supported class attrs\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28mcls\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m supported_class_attrs\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'PoseEstimation' has no attribute 'task'"
     ]
    }
   ],
   "source": [
    "model.PoseEstimation.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#reduce dataframe to xy coordinates\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dlc_scan_key =dlc_scan_key[0]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPoseEstimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdlc_scan_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df_xy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:,df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m])][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopcam_2bin_without_scope\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# df_xy.mean()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df_xy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dj_pure/lib/python3.8/site-packages/element_deeplabcut/model.py:769\u001b[0m, in \u001b[0;36mPoseEstimation.get_trajectory\u001b[0;34m(cls, key, body_parts)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trajectory\u001b[39m(\u001b[38;5;28mcls\u001b[39m, key: \u001b[38;5;28mdict\u001b[39m, body_parts: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    758\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a pandas dataframe of coordinates of the specified body_part(s)\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m             output of DLC dataframe. If 2D, z is set of zeros\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m body_parts \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    772\u001b[0m         body_parts \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mBodyPartPosition \u001b[38;5;241m&\u001b[39m key)\u001b[38;5;241m.\u001b[39mfetch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody_part\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "#reduce dataframe to xy coordinates\n",
    "# dlc_scan_key =dlc_scan_key[0]\n",
    "df=model.PoseEstimation.get_trajectory(dlc_scan_key)\n",
    "df_xy = df.iloc[:,df.columns.get_level_values(2).isin([\"x\",\"y\"])]['Topcam_2bin_without_scope']\n",
    "# df_xy.mean()\n",
    "# df_xy\n",
    "df_xy.plot().legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_xy.copy()\n",
    "df_flat.columns = df_flat.columns.map('_'.join)\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "# df_flat.plot(x='body_middle_x',y='body_middle_y',ax=ax)\n",
    "df_flat.plot(x='head_middle_x',y='head_middle_y', ax=ax)\n",
    "# df_flat.plot(x='tail_x',y='tail_y', ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.title(scan_key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = df_flat[['body_middle_x', 'body_middle_y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot events over position\n",
    "\n",
    "position = df_flat[['body_middle_x', 'body_middle_y']].values\n",
    "position = position[aligned_cameraframes].T/10 # synchronize to 2pframes and translate for opexebo - THIS IS A GUESSTIMATE!  pretending 1px = 1mm NEEDS CALIBRATION - tracking needs to be in xy real-world coordinates (in cm)\n",
    "\n",
    "\n",
    "total_cells = np.shape(event_matrix)[0] # Change this to the desired number of cells\n",
    "\n",
    "# Determine the grid dimensions (for a roughly square arrangement)\n",
    "nrows = int(np.ceil(np.sqrt(total_cells)))\n",
    "ncols = int(np.ceil(total_cells / nrows))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(30, 30))\n",
    "fig.subplots_adjust(hspace=0.1) # Add some space between the subplots\n",
    "\n",
    "# If axs is not already a 2D array (e.g., if total_cells is a perfect square), make it one\n",
    "# If axs is not already a 2D array (e.g., if total_cells is a perfect square), make it one\n",
    "if total_cells != nrows * ncols:\n",
    "    axs = axs.reshape(-1)\n",
    "else:\n",
    "    axs = axs.flatten()\n",
    "\n",
    "\n",
    "\n",
    "# load image styles for display\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(map_params)\n",
    "\n",
    "for cell in range(total_cells):\n",
    "    # try:    \n",
    "    ax = axs[cell]\n",
    "    # Plotting the line plot first\n",
    "    ax.plot(position[0], position[1], color='grey')\n",
    "\n",
    "    # spike events at position\n",
    "    spikes_at_pos = np.vstack((position[0, event_matrix[cell].astype(\"bool\")], position[1, event_matrix[cell].astype(\"bool\")]))\n",
    "    \n",
    "    # Then plotting the scatter plot so that it's on top of the line\n",
    "    ax.scatter(spikes_at_pos[0], spikes_at_pos[1], color='red', alpha = 0.2, zorder=2)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(scan_key[0][\"scan_id\"] + \"_\" + str(cell+1))\n",
    "    # except:\n",
    "    #     print(f'error at cell{cell}')\n",
    "# Remove any extra subplots\n",
    "for cell in range(total_cells, nrows * ncols):\n",
    "    fig.delaxes(axs[cell])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make masked spatial occupancy map - OPEXEBO\n",
    "\n",
    "import opexebo\n",
    "\n",
    "arena_size = 100 # in cm - NEEDS CALIBRATED TRACKING COORDS!\n",
    "arena_shape = \"circle\"\n",
    "bin_width =  4 # cm\n",
    "\n",
    "masked_occupancy_map, coverage, bin_edges = opexebo.analysis.spatial_occupancy(timebase_2p, position, arena_size, arena_shape = arena_shape, bin_width = bin_width)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(np.flipud(masked_occupancy_map))\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('time / bin [s]')\n",
    "plt.title(f'spatial occupancy - {scan_key[0][\"scan_id\"]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rate maps - OPEXEBO\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "# Determine the grid dimensions (for a roughly square arrangement)\n",
    "nrows = int(np.ceil(np.sqrt(total_cells)))\n",
    "ncols = int(np.ceil(total_cells / nrows))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace=0.1) # Add some space between the subplots\n",
    "\n",
    "# If axs is not already a 2D array (e.g., if total_cells is a perfect square), make it one\n",
    "if total_cells != nrows * ncols:\n",
    "    axs = axs.reshape(-1)\n",
    "else:\n",
    "    axs = axs.flatten()\n",
    "\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(map_params)\n",
    "\n",
    "for cell in range(total_cells):\n",
    "    ax = axs[cell]\n",
    "    # print(cell)\n",
    "    try:\n",
    "        # spike events at position\n",
    "        spikes_at_pos = np.vstack((position[0, event_matrix[cell].astype(\"bool\")], position[1, event_matrix[cell].astype(\"bool\")]))\n",
    "        \n",
    "        # time at postiion\n",
    "        time_at_pos = (timebase_2p[event_matrix[cell].astype(\"bool\")])\n",
    "        \n",
    "        # spikes_tracking [t,x,y]\n",
    "        spikes_tracking = np.vstack((time_at_pos, spikes_at_pos))\n",
    "        \n",
    "        # make ratemap\n",
    "        rate_map = opexebo.analysis.rate_map(masked_occupancy_map, spikes_tracking, arena_size, arena_shape = arena_shape, bin_width = bin_width)\n",
    "        rate_map = np.flipud(rate_map)\n",
    "        # filtered_rate_map = gaussian_filter(rate_map, sigma = 0.5)\n",
    "        \n",
    "        \n",
    "        im = ax.imshow(rate_map, vmin = 0, vmax = 2)\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(scan_key[0][\"scan_id\"] + \"-\" + str(cell+1))\n",
    "        # cbar = plt.colorbar(im, ax=ax) # Pass the image object and the ax to plt.colorbar\n",
    "        # cbar.set_label('events / bin [s]')\n",
    "    except:\n",
    "        print(f'error at cell{cell}')\n",
    "\n",
    "# Remove any extra subplots\n",
    "for cell in range(total_cells, nrows * ncols):\n",
    "    fig.delaxes(axs[cell])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speed tuning - freely moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get running speed - OPEXEBO\n",
    "new_speed = opexebo.analysis.calc_speed(timebase_2p, position[0], position[1], moving_average = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "# get some scaling values from pixel distribution\n",
    "scalemin = 0\n",
    "scalemax = 99.7\n",
    "\n",
    "cmin = np.percentile(new_speed,scalemin)  \n",
    "cmax = np.percentile(new_speed,scalemax)\n",
    "\n",
    "kp_colors = np.array([[0.55,0.55,0.55]])\n",
    "\n",
    "\n",
    "# timepoints to visualize\n",
    "tstart = 0\n",
    "tend =  timebase_2p[-1] -1\n",
    "\n",
    "xmin = int(np.floor(tstart * sampling_rate))\n",
    "xmax = int(np.floor(tend * sampling_rate))\n",
    "\n",
    "# make figure with grid for easy plotting\n",
    "fig = plt.figure(figsize=(8,5), dpi = 200)\n",
    "grid = plt.GridSpec(9, 20, figure=fig, wspace = 0.05, hspace = 0.3)\n",
    "\n",
    "# plot running speed\n",
    "ax = plt.subplot(grid[:2, :-1])\n",
    "ax.plot(new_speed,  color=kp_colors[0])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([cmin, cmax])\n",
    "\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"freely moving running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "\n",
    "# plot superneuron activity\n",
    "ax = plt.subplot(grid[2:, :-1])\n",
    "ax.imshow(Sfilt[:, xmin:xmax], cmap=\"gray_r\", vmin=-0.1, vmax=1,  extent= [timebase_2p[xmin], timebase_2p[xmax], 0, Sfilt.shape[0]], aspect=\"auto\", interpolation='none')\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"sorted cells\")\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()\n",
    "\n",
    "# ax = plt.subplot(grid[1:, -1])\n",
    "# ax.imshow(np.arange(0, len(sn))[:,np.newaxis], cmap=\"gist_ncar\", aspect=\"auto\")\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET IMU data\n",
    "\n",
    "accelerometer = (behavior.HarpRecording.Channel() & scan_key & \"channel_name LIKE 'IMU accelerometer %'\").fetch(\"data\")\n",
    "gyroscope = (behavior.HarpRecording.Channel() & scan_key & \"channel_name LIKE 'IMU gyroscope %'\").fetch(\"data\")\n",
    "magnetometer = (behavior.HarpRecording.Channel() & scan_key & \"channel_name LIKE 'IMU magnetometer %'\").fetch(\"data\")\n",
    "IMU_twopframes = (behavior.HarpRecording.Channel() & scan_key & \"channel_name LIKE '2p %'\").fetch(\"data\")\n",
    "IMU_time = (behavior.HarpRecording.Channel() & scan_key & \"channel_name LIKE '2p %'\").fetch(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYNC IMU data\n",
    "\n",
    "propersync = np.max(IMU_twopframes[0]).astype(\"bool\")\n",
    "\n",
    "## Get the timestamp data\n",
    "\n",
    "# from the event table get the main recording gate start / end HARP gate timestamps.\n",
    "harpgatetimestamp_end = (event.Event()  &  \"event_type='HARP_gate'\" &  scan_key ).fetch('event_end_time')\n",
    "harpgatetimestamp_start = (event.Event()  &  \"event_type='HARP_gate'\" &  scan_key ).fetch('event_start_time')\n",
    "#\n",
    "# if propersync:\n",
    "    # print(\"2p timestamps detected\")\n",
    "# else:\n",
    "print(\"No 2p timestamps in IMU rec detected - poor man's single-point sync\")\n",
    "IMU_time = IMU_time - harpgatetimestamp_start\n",
    "# HARP and AUX not in sync!\n",
    "print(IMU_time[0][-1]/1000)  \n",
    "print(harpgatetimestamp_end[0])\n",
    "\n",
    "# Therefore: space number of HARP samples evenly between HARP gate timestampa.\n",
    "harpgate_sync_timestamps = np.squeeze(np.linspace(harpgatetimestamp_start, harpgatetimestamp_end, np.shape(magnetometer[0])[0]))\n",
    "\n",
    "#  and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='mini2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "\n",
    "# get indices\n",
    "aligned_IMU_indices = get_closest_timestamps(twoptimestamps,harpgate_sync_timestamps) #smoothing windwo from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: use bidirectional filtering to prevent phase shifts between original and filtered signal\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# Design the Butterworth filter for accelerometer and magnetometer\n",
    "N = 6 # Order of the filter\n",
    "Wn = 0.03 # Cutoff frequency (example value, should be chosen based on your specific needs)\n",
    "b, a = butter(N, Wn, btype='low')\n",
    "\n",
    "# Design the Butterworth filter for gyroscope\n",
    "N = 6 # Order of the filter\n",
    "Wn = 0.005 # Cutoff frequency (example value, should be chosen based on your specific needs)\n",
    "d, c = butter(N, Wn, btype='low')\n",
    "\n",
    "filtered_accelerometer = [filtfilt(b, a, array) for array in accelerometer]\n",
    "filtered_gyroscope = [filtfilt(d, c, array) for array in gyroscope]\n",
    "filtered_magnetometer = [filtfilt(b, a, array) for array in magnetometer]\n",
    "filtered_newspeed = filtfilt(b, a, new_speed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate event trace for light stim\n",
    "\n",
    "# from the event table get the main recording gate start / end HARP gate timestamps.\n",
    "flash_timestamp_end = (event.Event()  &  \"event_type='arena_LED'\" &  scan_key ).fetch('event_end_time')\n",
    "flash_timestamp_start = (event.Event()  &  \"event_type='arena_LED'\" &  scan_key ).fetch('event_start_time')\n",
    "twoptimestamps = (event.Event()  &  \"event_type='mini2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# get indices\n",
    "aligned_flash_timestamp_end  = get_closest_timestamps(flash_timestamp_end, twoptimestamps) #smoothing windwo from above\n",
    "aligned_flash_timestamp_start  = get_closest_timestamps(flash_timestamp_start, twoptimestamps) #smoothing windwo from above\n",
    "\n",
    "# Create a linear array of zeros of length 10\n",
    "array_length = np.shape(twoptimestamps)[0]\n",
    "flash_array = np.zeros(array_length)\n",
    "\n",
    "# Iterate through the start and stop times and set the corresponding elements to 1\n",
    "for start, stop in zip(aligned_flash_timestamp_start, aligned_flash_timestamp_end):\n",
    "    flash_array[start:stop] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "plotdata = Sfilt_backup\n",
    "# plotdata = Sfilt\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "# get some scaling values from pixel distribution\n",
    "# SPEED \n",
    "scalemin = 0\n",
    "scalemax = 99.7\n",
    "\n",
    "cmin = np.percentile(new_speed,scalemin)  \n",
    "cmax = np.percentile(new_speed,scalemax)\n",
    "\n",
    "# SPEED threshold\n",
    "# Calculate the 50 th percentile on the lowpass filtered speed trace to obtain thresholds for (smooth) division into running / not running \n",
    "cropped_speed = filtered_newspeed[filtered_newspeed <= 500]\n",
    "perc = 50\n",
    "speed_thresh = np.percentile(cropped_speed, perc)\n",
    "\n",
    "# ACC \n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "\n",
    "acc_cmin = np.percentile(np.concatenate(filtered_accelerometer),scalemin)  \n",
    "acc_cmax = np.percentile(np.concatenate(filtered_accelerometer),scalemax)\n",
    "\n",
    "# GYR \n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "\n",
    "gyr_cmin = np.percentile(np.concatenate(filtered_gyroscope),scalemin)  \n",
    "gyr_cmax = np.percentile(np.concatenate(filtered_gyroscope),scalemax)\n",
    "\n",
    "# MAG \n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "\n",
    "mag_cmin = np.percentile(np.concatenate(filtered_magnetometer),scalemin)  \n",
    "mag_cmax = np.percentile(np.concatenate(filtered_magnetometer),scalemax)\n",
    "\n",
    "\n",
    "\n",
    "kp_colors = np.array([[0,0,0], [0.55,0.55,0.55], [0,0.9,0.9]])\n",
    "\n",
    "imu_colors = np.array([\n",
    "    [[1.0, 0.75, 0.8], [0.8, 0.65, 0.68], [0.55, 0.55, 0.55]],\n",
    "    [[0.95, 0.6, 0.95], [0.75, 0.58, 0.75], [0.55, 0.55, 0.55]],\n",
    "    [[0.65, 0.95, 0.95], [0.6, 0.75, 0.75], [0.55, 0.55, 0.55]]\n",
    "])\n",
    "\n",
    "# timepoints to visualize\n",
    "tstart = 0\n",
    "tend =  timebase_2p[-1] -1\n",
    "\n",
    "xmin = int(np.floor(tstart * sampling_rate))\n",
    "xmax = int(np.floor(tend * sampling_rate))\n",
    "\n",
    "# make figure with grid for easy plotting\n",
    "fig = plt.figure(figsize=(16,10), dpi = 200)\n",
    "grid = plt.GridSpec(20, 20, figure=fig, wspace = 0.5, hspace = 0.3)\n",
    "\n",
    "# plot running speed\n",
    "ax = plt.subplot(grid[:2, :-1])\n",
    "ax.plot(new_speed,  color=kp_colors[0])\n",
    "# ax.plot(filtered_newspeed,  color=kp_colors[2])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([cmin, cmax])\n",
    "ax.tick_params(left=False, right=False, bottom=False, top=False,\n",
    "               labelleft=False, labelbottom=False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "    \n",
    "ax.fill_between(range(len(new_speed)), scalemax, where=filtered_newspeed>speed_thresh, color='red', alpha=0.1)\n",
    "\n",
    "ax.set_ylabel(\"speed\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "# ax.set_title(\"freely moving running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "# plot accelerometer\n",
    "sliced_accelerometer = [array[aligned_IMU_indices] for array in filtered_accelerometer]\n",
    "ax = plt.subplot(grid[2:4, :-1])\n",
    "for i, arr in enumerate(sliced_accelerometer):\n",
    "    ax.plot(arr, label=f'accelerometer {i+1}', color=imu_colors[0][i])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([acc_cmin, acc_cmax])\n",
    "ax.tick_params(left=False, right=False, bottom=False, top=False,\n",
    "               labelleft=False, labelbottom=False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.set_ylabel(\"acc\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "# ax.set_title(\"freely moving running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "# plot gyroscope\n",
    "sliced_gyroscope = [array[aligned_IMU_indices] for array in filtered_gyroscope]\n",
    "ax = plt.subplot(grid[4:6, :-1])\n",
    "for i, arr in enumerate(sliced_gyroscope):\n",
    "    ax.plot(arr, label=f'gyroscope {i+1}', color=imu_colors[1][i])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([gyr_cmin, gyr_cmax])\n",
    "ax.tick_params(left=False, right=False, bottom=False, top=False,\n",
    "               labelleft=False, labelbottom=False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.set_ylabel(\"gyr\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "\n",
    "# plot magnetometer\n",
    "sliced_magnetometer= [array[aligned_IMU_indices] for array in filtered_magnetometer]\n",
    "ax = plt.subplot(grid[6:8, :-1])\n",
    "for i, arr in enumerate(sliced_magnetometer):\n",
    "    ax.plot(arr, label=f'magnetometer {i+1}', color=imu_colors[2][i])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([mag_cmin, mag_cmax])\n",
    "ax.tick_params(left=False, right=False, bottom=False, top=False,\n",
    "               labelleft=False, labelbottom=False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.set_ylabel(\"mag\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "# ax.set_title(\"freely moving running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "# plot LIGHT flash\n",
    "ax = plt.subplot(grid[8:10, :-1])\n",
    "ax.plot(flash_array,  color=kp_colors[1])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([-.1, 1.1])\n",
    "ax.tick_params(left=False, right=False, bottom=False, top=False,\n",
    "               labelleft=False, labelbottom=False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.set_ylabel(\"flash\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.fill_between(range(len(new_speed)), 1, where=filtered_newspeed>speed_thresh, color='red', alpha=0.2)\n",
    "\n",
    "\n",
    "# plot neuronal activity\n",
    "ax = plt.subplot(grid[10:, :-1])\n",
    "ax.imshow(plotdata[:, xmin:xmax], cmap=\"gray_r\", vmin=0.2, vmax=1,  extent= [timebase_2p[xmin], timebase_2p[xmax], 0, plotdata.shape[0]], aspect=\"auto\", interpolation = \"none\")\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"sorted cells\")\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()\n",
    "\n",
    "# ax = plt.subplot(grid[1:, -1])\n",
    "# ax.imshow(np.arange(0, len(sn))[:,np.newaxis], cmap=\"gist_ncar\", aspect=\"auto\")\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-aligned plotting - Flash onsets / Movement onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PSTH function\n",
    "def plot_aligned_data(data, eventarray, color, colortrace, label, plot_average=False):\n",
    "    if len(data.shape) == 1:\n",
    "        data = data[np.newaxis, :]\n",
    "    \n",
    "    segments = []\n",
    "    for row in data:\n",
    "        for event in eventarray:\n",
    "            start = int(event - pre_event_window)\n",
    "            end = int(event + post_event_window)\n",
    "            segment_length = end - start\n",
    "            time_range = np.linspace(-pre_event_window, post_event_window, segment_length) / sampling_rate\n",
    "\n",
    "            if len(time_range) != len(row[start:end]):\n",
    "                # print(f\"Skipping segment due to mismatched length: {len(time_range)} vs {len(row[start:end])}\")\n",
    "                continue\n",
    "            segments.append(row[start:end])\n",
    "\n",
    "    if plot_average:\n",
    "        avg_segment = np.mean(segments, axis=0)\n",
    "        std_dev = np.std(segments, axis=0)\n",
    "        plt.fill_between(time_range, avg_segment - std_dev, avg_segment + std_dev, color=color, alpha=0.3)\n",
    "        plt.plot(time_range, avg_segment, color = colortrace, lw=2, label='Average ' + label)\n",
    "        plt.axvline(0, color='black', linestyle='--')\n",
    "        plt.xlabel(\"Time [s]\")\n",
    "        plt.ylabel(r\"$\\Delta$F/F$_0$ [%]\")\n",
    "        plt.ylim([cmin, cmax])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running indices\n",
    "running_indices = filtered_newspeed > speed_thresh\n",
    "\n",
    "# PSTH\n",
    "pre = .5  # s\n",
    "post = 2  # s\n",
    "\n",
    "cmin = -100\n",
    "cmax = 250\n",
    "\n",
    "# Window around events to plot: 2 seconds before and 3 seconds after\n",
    "pre_event_window = np.floor(pre * sampling_rate)\n",
    "post_event_window = np.floor(post * sampling_rate)\n",
    "aligned_flash_timestamp_start = np.array(aligned_flash_timestamp_start)[(aligned_flash_timestamp_start > pre_event_window)]\n",
    "\n",
    "\n",
    "# Get the total number of cells to plot - HERE ONLY  CROSSING SNRthresh\n",
    "num_cells = dFF0_traces[SNR_matrix > SNRthresh, :].shape[0]\n",
    "# num_cells = dFF0_traces.shape[0]\n",
    "\n",
    "# Determine the layout for the subplots\n",
    "cols = int(math.ceil(math.sqrt(num_cells)))\n",
    "rows = int(math.ceil(num_cells / cols))\n",
    "\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(8,10))\n",
    "\n",
    "for cell in range(num_cells):\n",
    "    row = cell // cols\n",
    "    col = cell % cols\n",
    "\n",
    "    timeseries_data = dFF0_traces[SNR_matrix > SNRthresh, :][cell] * 100\n",
    "    # timeseries_data = dFF0_traces[cell] * 100\n",
    "\n",
    "\n",
    "    plt.sca(axarr[row, col])  # Set the current subplot\n",
    "\n",
    "    plot_aligned_data(timeseries_data[:], aligned_flash_timestamp_start, 'grey', 'red', 'Cell ' + str(cell), plot_average=True)\n",
    "    plt.title(\"Cell \" + str(cell))\n",
    "\n",
    "    \n",
    "plt.suptitle(\"all data\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Running indices\n",
    "running_indices = filtered_newspeed > speed_thresh\n",
    "\n",
    "running_event_indices = np.array(sorted(list(set(aligned_flash_timestamp_start) & set(np.where(running_indices)[0]))))\n",
    "not_running_event_indices = np.array(sorted(list(set(aligned_flash_timestamp_start) - set(np.where(running_indices)[0]))))\n",
    "\n",
    "\n",
    "# PSTH\n",
    "pre = .5  # s\n",
    "post = 2  # s\n",
    "\n",
    "cmin = -100\n",
    "cmax = 250\n",
    "\n",
    "# Window around events to plot: 2 seconds before and 3 seconds after\n",
    "pre_event_window = np.floor(pre * sampling_rate)\n",
    "post_event_window = np.floor(post * sampling_rate)\n",
    "aligned_flash_timestamp_start = np.array(aligned_flash_timestamp_start)[(aligned_flash_timestamp_start > pre_event_window)]\n",
    "\n",
    "# Adjust the event timestamps for the running condition\n",
    "adjusted_events = []\n",
    "for event in aligned_flash_timestamp_start:\n",
    "    adjustment = np.sum(running_indices[:event])\n",
    "    adjusted_events.append(event + adjustment)\n",
    "\n",
    "# Get the total number of cells to plot - HERE ONLY  CROSSING SNRthresh\n",
    "num_cells = dFF0_traces[SNR_matrix > SNRthresh, :].shape[0]\n",
    "# num_cells = dFF0_traces.shape[0]\n",
    "\n",
    "\n",
    "# Determine the layout for the subplots\n",
    "cols = int(math.ceil(math.sqrt(num_cells)))\n",
    "rows = int(math.ceil(num_cells / cols))\n",
    "\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(8,10))\n",
    "\n",
    "for cell in range(num_cells):\n",
    "    row = cell // cols\n",
    "    col = cell % cols\n",
    "\n",
    "    # timeseries_data = dFF0_traces[cell] * 100\n",
    "    timeseries_data = dFF0_traces[SNR_matrix > SNRthresh, :][cell] * 100\n",
    "    plt.sca(axarr[row, col])  # Set the current subplot\n",
    "\n",
    "    plot_aligned_data(timeseries_data[:], running_event_indices, 'red', 'red', 'Cell' + str(cell), plot_average=True)\n",
    "    plot_aligned_data(timeseries_data[:], not_running_event_indices, 'grey', 'black',  'Cell ' + str(cell), plot_average=True)\n",
    "    plt.title(\"Cell \" + str(cell))\n",
    "\n",
    "plt.suptitle(\"all data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_flash_timestamp_start[np.where(running_indices)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(aligned_flash_timestamp_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape([np.where(running_indices)[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set((aligned_flash_timestamp_start, np.where(running_indices)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_flash_timestamp_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_event_indices = np.array(sorted(list(set(aligned_flash_timestamp_start) & set(np.where(running_indices)[0]))))\n",
    "not_running_event_indices = np.array(sorted(list(set(aligned_flash_timestamp_start) - set(np.where(running_indices)[0]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_intersecting_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running indices\n",
    "running_indices = filtered_newspeed > speed_thresh\n",
    "\n",
    "# PSTHdd\n",
    "pre = .5  # s\n",
    "post = 2  # s\n",
    "\n",
    "cmin = -100\n",
    "cmax = 250\n",
    "\n",
    "# Window around events to plot: 2 seconds before and 3 seconds after\n",
    "pre_event_window = np.floor(pre * sampling_rate)\n",
    "post_event_window = np.floor(post * sampling_rate)\n",
    "aligned_flash_timestamp_start = np.array(aligned_flash_timestamp_start)[(aligned_flash_timestamp_start > pre_event_window)]\n",
    "\n",
    "\n",
    "# Get the total number of cells to plot\n",
    "# num_cells = dFF0_traces[SNR_matrix > SNRthresh, :].shape[0]\n",
    "num_cells = dFF0_traces.shape[0]\n",
    "\n",
    "# Determine the layout for the subplots\n",
    "cols = int(math.ceil(math.sqrt(num_cells)))\n",
    "rows = int(math.ceil(num_cells / cols))\n",
    "\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(12,15))\n",
    "\n",
    "for cell in range(num_cells):\n",
    "    row = cell // cols\n",
    "    col = cell % cols\n",
    "\n",
    "    # timeseries_data = dFF0_traces[SNR_matrix > SNRthresh, :][cell] * 100\n",
    "    timeseries_data = dFF0_traces[running_indices][cell] * 100\n",
    "\n",
    "\n",
    "    plt.sca(axarr[row, col])  # Set the current subplot\n",
    "\n",
    "    plot_aligned_data(timeseries_data[:], 'red', 'Cell ' + str(cell), plot_average=True)\n",
    "    plt.title(\"Cell \" + str(cell))\n",
    "\n",
    "    \n",
    "plt.suptitle(\"running\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some behavior analyis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data['custom']['database.prefix']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

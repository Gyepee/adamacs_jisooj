{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual ingest workflow, bypassing GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login\n",
    "\n",
    "Either log in via a local config file (see [01_pipeline](./01_pipeline.ipynb)), or enter login information manually. If you are don't have your login information, contact the administrator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-18 12:29:27,784][INFO]: Connecting tobiasr@172.26.128.53:3306\n",
      "[2023-11-18 12:29:27,828][INFO]: Connected tobiasr@172.26.128.53:3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# change to the upper level folder to detect dj_local_conf.json\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='adamacs', (\"Please move to the main directory\")\n",
    "from adamacs.pipeline import subject, session, equipment, surgery, event, trial, imaging, behavior, model\n",
    "from adamacs.ingest import session as isess\n",
    "from adamacs.helpers import stack_helpers as sh\n",
    "from adamacs.ingest import behavior as ibe\n",
    "import pathlib\n",
    "from natsort import natsorted, ns\n",
    "import datajoint as dj\n",
    "from rspace_client.eln import eln\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dj.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSpace connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'OK', 'rspaceVersion': '1.93.0'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL=dj.config['custom'].get('rspace_URL')\n",
    "API_KEY=dj.config['custom'].get('rspace_API_key')\n",
    "api = eln.ELNClient(URL, API_KEY)\n",
    "api.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation\n",
    "Next, import from `adamacs.pipeline` to activate the relevant schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamacs.utility import *\n",
    "# from adamacs.nbgui import *\n",
    "from adamacs.pipeline import subject, session, surgery, scan, equipment, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign easy names for relevant tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub, lab, protocol, line, mutation, user, project, subject_genotype, subject_death = (\n",
    "    subject.Subject(), subject.Lab(), subject.Protocol(), subject.Line(), \n",
    "    subject.Mutation(), subject.User(), subject.Project(), subject.SubjectGenotype(), \n",
    "    subject.SubjectDeath()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_session_dir_key_from_dir(directory):\n",
    "    return [path.split('/')[-1] for path in directory]\n",
    "     \n",
    "def get_scan_dir_key_from_dir(directory):\n",
    "    return [path.split('/')[-1] for path in directory]\n",
    "\n",
    "def get_session_key_from_dir(string):\n",
    "    result = [re.search(r'sess\\S+', item).group(0) for item in string]\n",
    "    return result\n",
    "\n",
    "def get_user_initials_from_dir(string):\n",
    "    result = [name[:2] for name in string]\n",
    "    return result\n",
    "\n",
    "def get_subject_key_from_dir(string):\n",
    "    result = [item.split(\"_\")[1] for item in string]\n",
    "    return result\n",
    "\n",
    "def get_date_key_from_dir(directory):\n",
    "    return directory.split(\"_\")[-1]\n",
    "\n",
    "def get_scan_key_from_dir(string):\n",
    "    result = [re.search(r'scan\\S+_', item).group(0)[:-1] for item in string]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content of user directory\n",
    "import fnmatch\n",
    "dataroot = dj.config['custom']['exp_root_data_dir'][0]\n",
    "# dirs_root = [d for d in os.listdir(dataroot) if os.path.isdir(os.path.join(dataroot, d)) and '_' in d]\n",
    "dirs_root = [d for d in os.listdir(dataroot) if os.path.isdir(os.path.join(dataroot, d)) and fnmatch.fnmatch(d, 'NK*') and fnmatch.fnmatch(d, '*10-*')]\n",
    "sorted_dirs_root = natsorted(dirs_root, key=get_date_key_from_dir, reverse = True)\n",
    "sorted_dirs_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sessi = \"sess9FKW82R3\"\n",
    "# scansi = \"scan9FKW82R3\"\n",
    "sessi = \"sess9FKOI7ZY\"\n",
    "scansi = \"scan9FKOI7ZY\"\n",
    "\n",
    "# sessi = \"sess9FKSMYYS\"\n",
    "# scansi = \"scan9FKSMYYS\"\n",
    "\n",
    "# scan9FJ842C3\n",
    "# scan9FB2LN5C\n",
    "# scan_key      \n",
    "\n",
    "# scansi = \"scan9FKSMYYS\"\n",
    "\n",
    "# scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "# curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')\n",
    "# sessi = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('session_id')[0]\n",
    "# session_key = (session.Session & f'session_id = \"{sessi}\"').fetch('KEY')[0]\n",
    "# aux_setup_typestr = (scan.ScanInfo() & scan_key).fetch(\"userfunction_info\")[0] # check setup type (not needed)\n",
    "# print(aux_setup_typestr)\n",
    "# print((scan.ScanPath & scan_key).fetch(\"path\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Project = project.fetch('project')\n",
    "Equipment = equipment.Equipment().fetch('scanner')\n",
    "Recording_Location = surgery.AnatomicalLocation().fetch('anatomical_location')\n",
    "s2pparm = imaging.ProcessingParamSet.fetch(\"paramset_idx\", \"paramset_desc\")\n",
    "DLCModels = model.Model.fetch(\"model_name\")\n",
    "\n",
    "print(\"Project\")\n",
    "print(project)\n",
    "\n",
    "print(\"Equipment\")\n",
    "print(Equipment)\n",
    "\n",
    "print(\"Recording_Location\")\n",
    "print(Recording_Location)\n",
    "\n",
    "print(\"s2pparm\")\n",
    "print(s2pparm)\n",
    "\n",
    "print(\"DLCModels\")\n",
    "print(DLCModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Session and Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isess.ingest_session_scan(sessi, verbose=True, project_key=\"rsc-functop\", equipment_key=\"mini2p_01\", location_key=\"RSCa\", software_key='ScanImage')\n",
    "session.SessionSameSite.update1({'session_id': sessi, 'same_site_id': sessi})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = scan.ScanPath() & 'scan_id = \"' + scansi + '\"'\n",
    "dir_proc = query.fetch('path')[0]\n",
    "print(dir_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POPULATE!\n",
    "populate_settings = {'display_progress': True, 'suppress_errors': False, 'processes': 1}\n",
    "scan.ScanInfo.populate(**populate_settings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scan.Scan * scan.ScanInfo * session.SessionSameSite * session.Session() & f'session_id = \"{sessi}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push scan to ProcessingTask\n",
    "# TODO: handle multiscan concatenation from here?\n",
    "selected_s2pparms_index = 0\n",
    "imaging.ProcessingTask.insert1((sessi, scansi, selected_s2pparms_index, dir_proc, 'trigger'), skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingTask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest AUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_setup_typestr = (scan.ScanInfo() & 'scan_id = \"' + scansi + '\"').fetch(\"userfunction_info\")[0]    \n",
    "print(aux_setup_typestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibe.ingest_aux(sessi,scansi,verbose=True, aux_setup_type=aux_setup_typestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%frames%\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_differences = np.diff((event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%reward%\"').fetch(\"event_start_time\"))\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(time_differences, bins=range(min(time_differences.astype(int)), max(time_differences.astype(int)) + 10, 1), edgecolor='black')\n",
    "# plt.title('Histogram of Time Differences Between Consecutive Events')\n",
    "# plt.xlabel('Time Difference (seconds)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest BPOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibe.ingest_bpod(sessi,scansi,verbose=False, aux_setup_type=aux_setup_typestr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get BPOD object to play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamacs.paths import get_imaging_root_data_dir, get_experiment_root_data_dir\n",
    "from adamacs.ingest import bpod\n",
    "from element_interface.utils import find_full_path\n",
    "\n",
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "bpod_path_relative = (scan.ScanPath & scan_key).fetch(\"path\")[0]\n",
    "\n",
    "bpod_path_full = list(find_full_path(\n",
    "        get_experiment_root_data_dir(), bpod_path_relative\n",
    "        ).glob(\"*mat\"))[0]\n",
    "\n",
    "print(bpod_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_object = bpod.Bpodfile(bpod_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_object.session_data[\"Info\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_object.session_data[\"MousePos\"]['PreStimDLC_live'][10]['right_ear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_object.session_data['RawEvents']['Trial'][4]['States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_object.trial_data[4]['States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_object.trial_data[3]['Events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = bpod_object.trial_data\n",
    "SoftCode10_events = [(i, trial['Events'].get('SoftCode10')) for i, trial in enumerate(trials) if 'SoftCode10' in trial['Events']] # get all trials that have a BNC1Low event. Returns a list of tuples (trial number, event time)\n",
    "\n",
    "SoftCode10_events_times = [x[1] for x in SoftCode10_events]\n",
    "\n",
    "plt.hist(SoftCode10_events_times, edgecolor='black')\n",
    "\n",
    "plt.title('Histogram of softcode 10 event-to-trial-beginning time')\n",
    "plt.xlabel('[s]')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.xlim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = bpod_object.trial_data\n",
    "SoftCode15_events = [(i, trial['Events'].get('SoftCode15')) for i, trial in enumerate(trials) if 'SoftCode15' in trial['Events']] # get all trials that have a BNC1Low event. Returns a list of tuples (trial number, event time)\n",
    "\n",
    "SoftCode15_events_times = [x[1] for x in SoftCode15_events]\n",
    "\n",
    "plt.hist(SoftCode15_events_times, edgecolor='black')\n",
    "\n",
    "plt.title('Histogram of softcode 15 event-to-trial-beginning time')\n",
    "plt.xlabel('[s]')\n",
    "plt.ylabel('Frequency')\n",
    "# plt.xlim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUX_shutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Fetch trial data\n",
    "trials = bpod_object.trial_data\n",
    "\n",
    "# Fetch trial start timestamps\n",
    "bpod_trial = bpod_object.session_data[\"TrialStartTimestamp\"]\n",
    "\n",
    "WaitForPosTriggerSoftCode_events = [(trial['States'].get('WaitForPosTriggerSoftCode') if trial['States'].get('WaitForPosTriggerSoftCode') is not None else 0) + bpod_trial[i] for i, trial in enumerate(trials) if 'WaitForPosTriggerSoftCode' in trial['States']]\n",
    "WaitForPosTriggerSoftCode = [event[1] for event in WaitForPosTriggerSoftCode_events]\n",
    "\n",
    "CueDelayTrialCricket_events = [(trial['States'].get('CueDelayTrialCricket') if trial['States'].get('CueDelayTrialCricket') is not None else 0) + bpod_trial[i] for i, trial in enumerate(trials) if 'CueDelayTrialCricket' in trial['States']]\n",
    "CueDelayTrialCricket = [event[0] for event in CueDelayTrialCricket_events]\n",
    "\n",
    "CueDelay_events = [(trial['States'].get('CueDelay') if trial['States'].get('CueDelay') is not None else 0) + bpod_trial[i] for i, trial in enumerate(trials) if 'CueDelay' in trial['States']]\n",
    "CueDelay  = [event[0] for event in CueDelay_events]\n",
    "\n",
    "Drinking_events = [(trial['States'].get('Drinking') if trial['States'].get('Drinking') is not None else 0) + bpod_trial[i] for i, trial in enumerate(trials) if 'Drinking' in trial['States']]\n",
    "DrinkingResponse = [event[0] for event in Drinking_events]\n",
    "\n",
    "\n",
    "# Fetch all trials that have a BNC1Low event. Returns a list of tuples (trial number, event time)\n",
    "BNC1Low_events = [(i, trial['Events'].get('BNC1Low')) for i, trial in enumerate(trials) if 'BNC1Low' in trial['Events']]\n",
    "\n",
    "# Fetch shutter event start times\n",
    "AUX_shutter = (event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%shutter%\"').fetch(\"event_start_time\")\n",
    "\n",
    "# Fetch event start times for visual and tone events\n",
    "aux_vis = (event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%vis%\"').fetch(\"event_start_time\") -  AUX_shutter[0]\n",
    "aux_tone = (event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%tone%\"').fetch(\"event_start_time\") -  AUX_shutter[0]\n",
    "aux_reward = (event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%reward%\"').fetch(\"event_start_time\") -  AUX_shutter[0]\n",
    "\n",
    "# Adjust bpod_trial times\n",
    "\n",
    "bpodoffset = (BNC1Low_events[0][1]  + bpod_trial[0]) \n",
    "\n",
    "WaitForPosTriggerSoftCode = WaitForPosTriggerSoftCode - bpodoffset\n",
    "CueDelay = CueDelay - bpodoffset \n",
    "CueDelayTrialCricket = CueDelayTrialCricket - bpodoffset \n",
    "DrinkingResponse = DrinkingResponse - bpodoffset \n",
    "bpod_trial = bpod_trial - bpodoffset \n",
    "\n",
    "# Convert aux_tone, bpod_trial, and aux_vis values from seconds to deciseconds\n",
    "tone_timestamps = (aux_tone * 100).astype(int)\n",
    "trial_timestamps = (bpod_trial * 100).astype(int)\n",
    "vis_timestamps = (aux_vis * 100).astype(int)\n",
    "reward_timestamps = (aux_reward * 100).astype(int)\n",
    "WaitForPosTriggerSoftCode_timestamps = (np.array(WaitForPosTriggerSoftCode) * 100).astype(int)\n",
    "CueDelay_timestamps = (np.array(CueDelay) * 100).astype(int)\n",
    "CueDelayTrialCricket_timestamps = (np.array(CueDelayTrialCricket) * 100).astype(int)\n",
    "DrinkingResponse_timestamps = (np.array(DrinkingResponse) * 100).astype(int)\n",
    "\n",
    "# Create separate timebases for each event type\n",
    "timebase_tone = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "timebase_trial = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "timebase_vis = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "timebase_reward = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "timebase_WaitForPosTriggerSoftCode = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "timebase_CueDelay = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "timebase_CueDelayTrialCricket = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "DrinkResponse = np.zeros(int(max(tone_timestamps.max(), trial_timestamps.max(), vis_timestamps.max(), reward_timestamps.max(), WaitForPosTriggerSoftCode_timestamps.max(), CueDelay_timestamps.max()) + 1))\n",
    "\n",
    "\n",
    "# Set the timebase samples that correspond to the tone onsets, trial starts, and visual cue onsets to one\n",
    "# Each event has a duration of 100ms\n",
    "for timestamp in tone_timestamps:\n",
    "    timebase_tone[timestamp:timestamp+1] = 2\n",
    "for timestamp in trial_timestamps:\n",
    "    timebase_trial[timestamp:timestamp+1] = 3\n",
    "for timestamp in vis_timestamps:\n",
    "    timebase_vis[timestamp:timestamp+1] = 1\n",
    "for timestamp in reward_timestamps:\n",
    "    timebase_reward[timestamp:timestamp+1] = 4\n",
    "for timestamp in WaitForPosTriggerSoftCode_timestamps:\n",
    "    timebase_WaitForPosTriggerSoftCode[timestamp:timestamp+1] = 5\n",
    "for timestamp in CueDelay_timestamps:\n",
    "    timebase_CueDelay[timestamp:timestamp+1] = 6\n",
    "for timestamp in CueDelayTrialCricket_timestamps:\n",
    "    timebase_CueDelayTrialCricket[timestamp:timestamp+1] = 7  \n",
    "for timestamp in DrinkingResponse_timestamps:\n",
    "    DrinkResponse[timestamp:timestamp+1] = 8   \n",
    "\n",
    "# Set the style to 'dark_background'\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Create a color palette\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "# Create a figure with a width that fits the screen\n",
    "plt.figure(figsize=(50, 10))\n",
    "\n",
    "# Create tick plots with colors from the palette\n",
    "linelength = 0.7\n",
    "\n",
    "plt.eventplot(np.where(timebase_tone == 2)[0], lineoffsets=1, colors=colors[0], linelengths=linelength, label='AUX Tone onsets')\n",
    "plt.eventplot(np.where(timebase_vis == 1)[0], lineoffsets=2, colors=colors[1], linelengths=linelength, label='AUX Visual cue onsets')\n",
    "plt.eventplot(np.where(timebase_reward == 4)[0], lineoffsets=3, colors=colors[2], linelengths=linelength, label='AUX Reward onsets')\n",
    "plt.eventplot(np.where(timebase_trial == 3)[0], lineoffsets=4, colors=colors[3], linelengths=linelength, label='BPOD Trial starts')\n",
    "plt.eventplot(np.where(timebase_WaitForPosTriggerSoftCode == 5)[0], lineoffsets=5, colors=colors[4], linelengths=linelength, label='WaitForPosTriggerSoftCode')\n",
    "plt.eventplot(np.where(timebase_CueDelay == 6)[0], lineoffsets=6, colors=colors[5], linelengths=linelength, label='CueDelay')\n",
    "plt.eventplot(np.where(timebase_CueDelayTrialCricket == 7)[0], lineoffsets=7, colors=colors[6], linelengths=linelength, label='CueDelayTrialCricket') \n",
    "plt.eventplot(np.where(DrinkResponse == 8)[0], lineoffsets=8, colors=colors[7], linelengths=linelength, label='DrinkingResponse')\n",
    "\n",
    "plt.yticks([1, 2, 3, 4, 5, 6, 7, 8], ['AUX Tone onsets', 'AUX Visual cue_triggered', 'AUX Reward onsets', 'BPOD Trial starts', 'WaitForPosTriggerSoftCode', 'CueDelay', 'CueDelayTrialCricket', 'Drinking'])\n",
    "\n",
    "# Draw vertical lines where timebase_trial == 3\n",
    "\n",
    "for i in np.where(timebase_trial == 3)[0]:\n",
    "    plt.axvline(x=i, color='w', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "# Get the y-tick labels\n",
    "labels = plt.gca().get_yticklabels()\n",
    "\n",
    "# Find the labels to change\n",
    "for label in labels:\n",
    "    if label.get_text() == 'WaitForPosTriggerSoftCode' or label.get_text() == 'AUX Visual cue_triggered':\n",
    "        label.set_color('red')\n",
    "        label.set_fontweight('bold')\n",
    "    if label.get_text() == 'BPOD Trial starts':\n",
    "        label.set_color('green')\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim([16000, 18000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HiFi1_1_events.asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_timeseries_and_find_non_matching_events(time_series_1, time_series_2):\n",
    "    # Create event markers for each timeseries\n",
    "    events_1 = np.zeros(int(np.ceil(max(time_series_1))))\n",
    "    events_2 = np.zeros(int(np.ceil(max(time_series_2))))\n",
    "\n",
    "    # Mark the events\n",
    "    for event in time_series_1:\n",
    "        events_1[int(event)] = 1\n",
    "    for event in time_series_2:\n",
    "        events_2[int(event)] = 1\n",
    "\n",
    "    # Pad the shorter timeseries with zeros to make them the same length\n",
    "    if len(events_1) > len(events_2):\n",
    "        events_2 = np.pad(events_2, (0, len(events_1) - len(events_2)), 'constant')\n",
    "    elif len(events_2) > len(events_1):\n",
    "        events_1 = np.pad(events_1, (0, len(events_2) - len(events_1)), 'constant')\n",
    "\n",
    "    # Perform cross-correlation\n",
    "    correlation = correlate(events_1, events_2, mode='full')\n",
    "    lag = np.argmax(correlation) - (len(events_2) - 1)\n",
    "\n",
    "    # Shift the second series according to the lag\n",
    "    aligned_series_2 = np.roll(events_2, -lag)[:len(events_1)]\n",
    "\n",
    "    # Find the indices where events do not match\n",
    "    non_matching_indices = np.where((events_1 == 1) & (aligned_series_2 == 0))[0]\n",
    "\n",
    "    # Convert indices back to times\n",
    "    non_matching_times = time_series_1[np.isin(time_series_1, non_matching_indices)]\n",
    "\n",
    "    return non_matching_indices, non_matching_times, lag\n",
    "\n",
    "# Align the two timeseries and find non-matching events\n",
    "non_matching_indices, non_matching_times, alignment_lag = align_timeseries_and_find_non_matching_events(bpod_trial, aux_tone)\n",
    "\n",
    "non_matching_indices, non_matching_times, alignment_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_tone - aux_tone[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_trial-bpod_trial[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matching_times, alignment_lag = align_timeseries_and_find_non_matching_events(bpod_trial, aux_tone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import correlate\n",
    "\n",
    "def align_timeseries(series1, series2):\n",
    "    \"\"\"\n",
    "    This function aligns two timeseries data and finds non-matching events.\n",
    "\n",
    "    :param series1: First timeseries data as a numpy array.\n",
    "    :param series2: Second timeseries data as a numpy array.\n",
    "    :return: Indices of non-matching events in series1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform cross-correlation\n",
    "    correlation = correlate(series1, series2, mode='full')\n",
    "    # Find the lag that maximizes the cross-correlation\n",
    "    lag = np.argmax(correlation) - (len(series2) - 1)\n",
    "\n",
    "    # Align the series by shifting series2\n",
    "    aligned_series2 = np.roll(series2, -lag)\n",
    "\n",
    "    # Find non-matching events (where one series has an event and the other does not)\n",
    "    non_matching_events = np.where((series1 != 0) != (aligned_series2[:len(series1)] != 0))[0]\n",
    "\n",
    "    return non_matching_events, lag, aligned_series2\n",
    "\n",
    "# Call the alignment function\n",
    "non_matching_indices, calculated_lag, aligned_series2 = align_timeseries(bpod_trial, aux_tone)\n",
    "\n",
    "print(f\"Non-matching event indices in series1: {non_matching_indices}\")\n",
    "print(f\"Lag for alignment: {calculated_lag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_trial = bpod_object.session_data[\"TrialStartTimestamp\"]\n",
    "bpod_trial = np.concatenate(([1], bpod_trial))\n",
    "ones = np.ones_like(bpod_trial)\n",
    "\n",
    "ones2 = np.ones_like(aligned_series2)\n",
    "\n",
    "aux_tone = (event.Event & 'scan_id = \"' + scansi + '\"' & 'event_type LIKE \"%vis%\"').fetch(\"event_start_time\")\n",
    "onesfive = np.ones_like(aux_tone) -.5\n",
    "\n",
    "fig, axs = plt.subplots(2,figsize=(20, 6))\n",
    "axs[0].scatter(aux_tone, onesfive+0.5)\n",
    "axs[1].scatter(bpod_trial, ones)\n",
    "axs[0].scatter(aligned_series2, ones2, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the alignment\n",
    "alignment = dtw(aux_tone, bpod_trial, keep_internals=True)\n",
    "\n",
    "# Plot the original time series\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(aux_tone, label='aux_tone')\n",
    "plt.plot(bpod_trial, label='bpod_trial')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the warping path\n",
    "plt.subplot(3, 1, 2)\n",
    "alignment.plot(type=\"twoway\",offset=-2)\n",
    "\n",
    "# Plot the aligned time series\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(aux_tone[alignment.index1], label='aligned aux_tone')\n",
    "plt.plot(bpod_trial[alignment.index2], label='aligned bpod_trial')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dtw(s, t, window):\n",
    "    n, m = len(s), len(t)\n",
    "    w = np.max([window, abs(n-m)])\n",
    "    dtw_matrix = np.zeros((n+1, m+1))\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            dtw_matrix[i, j] = np.inf\n",
    "    dtw_matrix[0, 0] = 0\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        for j in range(np.max([1, i-w]), np.min([m, i+w])+1):\n",
    "            dtw_matrix[i, j] = 0\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        for j in range(np.max([1, i-w]), np.min([m, i+w])+1):\n",
    "            cost = abs(s[i-1] - t[j-1])\n",
    "            # take last min from a square box\n",
    "            last_min = np.min([dtw_matrix[i-1, j], dtw_matrix[i, j-1], dtw_matrix[i-1, j-1]])\n",
    "            dtw_matrix[i, j] = cost + last_min\n",
    "    return dtw_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_1 = aux_tone\n",
    "time_series_2 = bpod_trial\n",
    "dtw(time_series_1, time_series_2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dtw_alignment(ts_a, ts_b, d=lambda x, y: abs(x - y)):\n",
    "    \"\"\"Computes the Dynamic Time Warping (DTW) alignment between two time series.\n",
    "\n",
    "    Args:\n",
    "        ts_a: First time series (list or array).\n",
    "        ts_b: Second time series (list or array).\n",
    "        d: Distance function. Defaults to absolute difference.\n",
    "\n",
    "    Returns:\n",
    "        path: The optimal path for the alignment.\n",
    "        DTW: The full DTW matrix.\n",
    "    \"\"\"\n",
    "    # Create cost matrix with infinite values\n",
    "    DTW = np.full((len(ts_a) + 1, len(ts_b) + 1), np.inf)\n",
    "    DTW[0, 0] = 0\n",
    "\n",
    "    # Populate the cost matrix\n",
    "    for i in range(1, len(ts_a) + 1):\n",
    "        for j in range(1, len(ts_b) + 1):\n",
    "            cost = d(ts_a[i-1], ts_b[j-1])\n",
    "            DTW[i, j] = cost + min(DTW[i-1, j],    # insertion\n",
    "                                   DTW[i, j-1],    # deletion\n",
    "                                   DTW[i-1, j-1])  # match\n",
    "\n",
    "    # Backtrack to find the optimal path\n",
    "    path = []\n",
    "    i, j = len(ts_a), len(ts_b)\n",
    "    while i > 0 and j > 0:\n",
    "        path.append((i-1, j-1))\n",
    "        min_index = np.argmin((DTW[i-1, j], DTW[i, j-1], DTW[i-1, j-1]))\n",
    "        if min_index == 0:\n",
    "            i -= 1\n",
    "        elif min_index == 1:\n",
    "            j -= 1\n",
    "        else:  # min_index == 2\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "    # Include the start cell in the path\n",
    "    path.append((0, 0))\n",
    "\n",
    "    path.reverse()\n",
    "    return path, DTW\n",
    "\n",
    "# Example time series data\n",
    "ts_a = aux_tone\n",
    "ts_b = bpod_trial\n",
    "\n",
    "# Get the alignment and DTW matrix\n",
    "alignment_path, DTW_matrix = dtw_alignment(ts_a, ts_b)\n",
    "\n",
    "# The alignment path is returned as tuples of aligned indices\n",
    "alignment_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpod_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_tone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run image processing jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Processing.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.Curation().create1_from_processing_task({'session_id': sessi, 'scan_id': scansi, \"paramset_idx\": selected_s2pparms_index, \"manual_curation\": 0})\n",
    "\n",
    "imaging.MotionCorrection.populate(**populate_settings)\n",
    "\n",
    "imaging.Segmentation.populate(**populate_settings)\n",
    "\n",
    "imaging.MaskClassification.populate(**populate_settings)\n",
    "\n",
    "imaging.Fluorescence.populate(**populate_settings)\n",
    "\n",
    "imaging.Activity.populate(**populate_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overview_movies_rspace(curation_key):\n",
    "    # params_key = (imaging.ProcessingParamSet & 'paramset_idx = \"4\"').fetch('KEY')\n",
    "    # reg_tiffs_available = (imaging.ProcessingParamSet & params_key).fetch(\"params\")[0]['reg_tif']\n",
    "    from scipy.ndimage import mean\n",
    "    import tifffile\n",
    "    path = (scan.ScanPath & curation_key).fetch1(\"path\") + (\"/suite2p/plane0/reg_tif\")\n",
    "\n",
    "    # path = '/datajoint-data/data/jisooj/RN_OPI-1681_2023-02-15_scan9FGLEFJ3_sess9FGLEFJ3/suite2p_exp9FGLEFJ3/suite2p/plane0/reg_tif'\n",
    "    # Get a list of all tiff files in the folder\n",
    "    tiff_files = [os.path.join(path, f) for f in natsorted(os.listdir(path)) if f.endswith('.tif')]\n",
    "\n",
    "    # print(tiff_files)\n",
    "\n",
    "    # Load each tiff stack into a list of numpy arrays\n",
    "    stacks = []\n",
    "    for f in tiff_files:\n",
    "        with tifffile.TiffFile(f) as tif:\n",
    "            # Get the number of pages in the file\n",
    "            num_pages = len(tif.pages)\n",
    "            \n",
    "            # Create a numpy array to store all pages\n",
    "            stack = np.zeros((num_pages,) + tif.pages[0].shape, dtype=tif.pages[0].dtype)\n",
    "            \n",
    "            # Iterate over the pages and store them in the array\n",
    "            for i, page in enumerate(tif.pages):\n",
    "                stack[i] = page.asarray()\n",
    "\n",
    "        stacks.append(stack)\n",
    "\n",
    "    # Concatenate the stacks into a single numpy array along the z-axis\n",
    "    volume = np.concatenate(stacks, axis=0)\n",
    "\n",
    "    # delete registration tiff\n",
    "    for f in tiff_files:\n",
    "        os.remove(f) \n",
    "    \n",
    "    ### moving average filter\n",
    "    # Create a running Z mean projection of the volume\n",
    "\n",
    "    runav = 30\n",
    "    # running_z_projection = uniform_filter_mt(volume, size=(runav,xyrunav,xyrunav))\n",
    "    running_z_projection = sh.rolling_average_filter(volume, runav)\n",
    "\n",
    "    session_id = curation_key['session_id']\n",
    "    scan_id = curation_key['scan_id']\n",
    "\n",
    "    filename = os.path.join(path, 'registered_movie_' + session_id + '_' + scan_id + '_' + str(runav) + '_frame_runningaverage2' + '.mp4')\n",
    "\n",
    "    fps = 120   # frames per second - 120 default\n",
    "    p1 = 2       # percentile scaling low - 1 default\n",
    "    p2 = 99.998  # percentile scaling high - 99.995 default\n",
    "\n",
    "    rescaled_image_8bit = sh.make_stack_movie(running_z_projection, filename, fps, p1, p2)\n",
    "\n",
    "    tmpdir = dj.config['custom'].get('suite2p_fast_tmp')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_overview_movies_rspace(curation_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(model) + dj.Diagram(equipment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_setup_typestr = (scan.ScanInfo() & 'scan_id = \"' + scansi + '\"').fetch(\"userfunction_info\")[0]\n",
    "selected_DLCmodel = 'Topcam_2bin_with_scope'\n",
    "print('- - - -')\n",
    "print('DLC pose estimation:', scansi)\n",
    "\n",
    "# insert TOP movie into model table\n",
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0] \n",
    "moviepath = str(list(pathlib.Path((scan.ScanPath() & scan_key).fetch(\"path\")[0]).glob(\"*top*.mp4*\"))[0])\n",
    "\n",
    "key = {'session_id': scan_key[\"session_id\"],\n",
    "    'recording_id': scan_key[\"scan_id\"], \n",
    "    'camera': \"mini2p1_top\", # Currently 'scanner' due to in equipment tables\n",
    "    }\n",
    "model.VideoRecording.insert1(key, skip_duplicates=True)\n",
    "\n",
    "key.update({'file_path': moviepath,\n",
    "            'file_id': 0})  #INCREMENT FILE_ID WITH CAM NUMBER?\n",
    "\n",
    "model.VideoRecording.File.insert1(key, ignore_extra_fields=True, skip_duplicates=True)\n",
    "model.RecordingInfo.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.VideoRecording.File * model.VideoRecording() &  'recording_id = \"' + scansi + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key =  (model.VideoRecording & f'recording_id=\"{scansi}\"').fetch1('KEY')\n",
    "key.update({'model_name': selected_DLCmodel, 'task_mode': 'trigger'}) \n",
    "key      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT pose estimation task\n",
    "model.PoseEstimationTask.insert_estimation_task(key, key[\"model_name\"], analyze_videos_params={'save_as_csv':True, 'dynamic':(True,.5,60)}) # dynamic cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.PoseEstimationTask() * model.VideoRecording * model.VideoRecording.File()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.RecordingInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pose estimation\n",
    "model.PoseEstimation.populate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make labeled video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplabcut.utils.make_labeled_video import create_labeled_video\n",
    "import yaml\n",
    "from element_interface.utils import find_full_path\n",
    "from adamacs.paths import get_dlc_root_data_dir\n",
    "\n",
    "\n",
    "destfolder = model.PoseEstimationTask.infer_output_dir(key)\n",
    "destfolder\n",
    "\n",
    "video_path = find_full_path( # Fetch the full video path\n",
    "    get_dlc_root_data_dir(), ((model.VideoRecording.File & key).fetch1(\"file_path\"))\n",
    ")\n",
    "\n",
    "config_paths = sorted( # Of configs in the project path, defer to the datajoint-saved\n",
    "    list(\n",
    "        find_full_path(\n",
    "            get_dlc_root_data_dir(), ((model.Model & key).fetch1(\"project_path\"))\n",
    "        ).glob(\"*.y*ml\")\n",
    "    )\n",
    ")\n",
    "\n",
    "create_labeled_video( # Pass strings to label the video\n",
    "    config=str(config_paths[-1]),\n",
    "    videos=str(video_path),\n",
    "    destfolder=str(destfolder),\n",
    ")\n",
    "\n",
    "# list(list(pathlib.Path((model.VideoRecording.File & key).fetch1(\"file_path\")).parent.glob(\"device*\"))[0].glob(\"*.y*ml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "mp4_files = glob.glob(f\"{destfolder}/*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.PoseEstimation() * session.Session * session.SessionUser * subject.User()).fetch(format = \"frame\", order_by = \"session_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scansi = \"scan9FKSMYYS\"\n",
    "scan_key = (model.PoseEstimation & f'recording_id = \"{scansi}\"').fetch('KEY')[0] \n",
    "path = (model.VideoRecording.File & scan_key).fetch(\"file_path\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.PoseEstimation.BodyPartPosition() & scan_key & 'body_part = \"head_middle\"').fetch(\"x_pos\", \"y_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning. Use with caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging.ProcessingTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject.Subject.delete()\n",
    "# session.Session.delete()\n",
    "# imaging.Processing.delete()\n",
    "# imaging.Curation.delete()\n",
    "# event.Event.delete()\n",
    "# event.BehaviorRecording.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (session.Session & \"session_datetime >= '2023-10-01'\").fetch('KEY')\n",
    "# (session.Session & \"session_datetime >= '2023-10-01'\").delete()\n",
    "# (imaging.ProcessingTask & \"session_id LIKE 'sess9FKWT2RT%'\").delete()\n",
    "# (imaging.ProcessingTask  & key).delete()\n",
    "(model.PoseEstimationTask & key).delete()\n",
    "# (session.Session & \"session_id LIKE 'sess9FKWT2RT%'\").delete()\n",
    "# (subject.Subject & \"subject = 'ROS-1571'\").delete()\n",
    "# # subject.Subject.delete()\n",
    "# # session.Session.delete()\n",
    "# # imaging.Processing.delete()\n",
    "# # imaging.Curation.delete()\n",
    "# # event.Event.delete()\n",
    "# # event.BehaviorRecording.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.Session.drop()\n",
    "# scan.Scan.drop()\n",
    "# imaging.Processing.drop()\n",
    "# imaging.Curation.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datajoint-DLCbackup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

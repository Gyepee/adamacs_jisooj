{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# change to the upper level folder to detect dj_local_conf.json\n",
    "if os.path.basename(os.getcwd())=='notebooks': os.chdir('..')\n",
    "assert os.path.basename(os.getcwd())=='adamacs', (\"Please move to the main directory\")\n",
    "from adamacs.pipeline import subject, session, equipment, surgery, event, trial, imaging, behavior, scan, model\n",
    "from adamacs.ingest import session as isess\n",
    "from adamacs.helpers import stack_helpers as sh\n",
    "from adamacs.ingest import behavior as ibe\n",
    "import datajoint as dj\n",
    "from rspace_client.eln import eln\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dj.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some functions used here (will be hidden later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import cv2\n",
    "\n",
    "def get_closest_timestamps(series, target_timestamp):\n",
    "    # List to store the indices\n",
    "    indices = []\n",
    "\n",
    "    # For each timestamp in series1, find the closest timestamp in series2 and get its index\n",
    "    for t1 in series:\n",
    "        closest_index = closest_timestamp(target_timestamp, t1)\n",
    "        indices.append(closest_index)\n",
    "    return indices\n",
    "\n",
    "# Function to find closest timestamp\n",
    "def closest_timestamp(series, target_timestamp):\n",
    "    index = bisect.bisect_left(series, target_timestamp)\n",
    "    if index == 0:\n",
    "        return 0\n",
    "    if index == len(series):\n",
    "        return len(series)-1\n",
    "    before = series[index - 1]\n",
    "    after = series[index]\n",
    "    if after - target_timestamp < target_timestamp - before:\n",
    "       return index\n",
    "    else:\n",
    "       return index-1\n",
    "\n",
    "\n",
    "def resize_movie(movie, new_height, new_width):\n",
    "    # Get the number of frames and color channels\n",
    "    num_frames, _, _, num_channels = movie.shape\n",
    "    \n",
    "    # Initialize an empty array for the scaled movie\n",
    "    scaled_movie = np.empty((num_frames, new_height, new_width, num_channels), dtype=np.uint8)\n",
    "    \n",
    "    # Iterate through each frame\n",
    "    for i in tqdm(range(num_frames), desc=\"Resizing frames\"):\n",
    "        # Resize the frame and store it in the new array\n",
    "        scaled_movie[i] =  cv2.resize(movie[i], (new_width, new_height), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    # Return the scaled movie\n",
    "    return scaled_movie\n",
    "\n",
    "\n",
    "def resize_frame(frame, new_height, new_width):\n",
    "    return cv2.resize(frame, (new_width, new_height), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def resize_movie_mt(movie, new_height, new_width):\n",
    "    num_frames, _, _, num_channels = movie.shape\n",
    "    scaled_movie = np.empty((num_frames, new_height, new_width, num_channels), dtype=np.uint8)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for i, resized_frame in tqdm(enumerate(executor.map(resize_frame, movie, [new_height]*num_frames, [new_width]*num_frames)), total=num_frames, desc=\"Resizing frames\"):\n",
    "            scaled_movie[i] = resized_frame\n",
    "\n",
    "    return scaled_movie\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# Figure Style settings for notebook.\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "plot_params = {\n",
    "    'axes.facecolor': 'white',\n",
    "    'figure.facecolor': 'white',\n",
    "    'font.family': 'sans-serif',\n",
    "    # 'font.sans-serif': 'Helvetica Neue',\n",
    "    'font.size': 16,\n",
    "    'lines.color': 'black',\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.spines.top': True,\n",
    "    'axes.spines.right': True,\n",
    "    'axes.edgecolor': 'black',  \n",
    "    # 'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .5,\n",
    "    'figure.subplot.hspace': .5,\n",
    "    # 'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': True,\n",
    "    'xtick.major.bottom': True\n",
    "}\n",
    "\n",
    "map_params = {\n",
    "    'axes.facecolor': 'white',\n",
    "    'figure.facecolor': 'white',\n",
    "    'font.family': 'sans-serif',\n",
    "    # 'font.sans-serif': 'Helvetica Neue',\n",
    "    'font.size': 12,\n",
    "    'lines.color': 'black',\n",
    "    'xtick.direction': 'out',\n",
    "    'ytick.direction': 'out',\n",
    "    'xtick.color': 'black',\n",
    "    'ytick.color': 'black',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.edgecolor': 'black',  \n",
    "    # 'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .5,\n",
    "    'figure.subplot.hspace': .5,\n",
    "    # 'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': False,\n",
    "    'xtick.major.bottom': False\n",
    "}\n",
    "\n",
    "\n",
    "img_params = {\n",
    "    'axes.facecolor': 'black',\n",
    "    'figure.facecolor': 'black',\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'legend.frameon': False,\n",
    "    'figure.subplot.wspace': .01,\n",
    "    'figure.subplot.hspace': .01,\n",
    "    'figure.figsize': (18, 13),\n",
    "    'ytick.major.left': False,\n",
    "    'xtick.major.bottom': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recordings and example analysis for BonnBrain - JISOOJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jisoo - example scans\n",
    "\n",
    "# mini2p\n",
    "# sessi = sess9FJBPNSP\n",
    "\n",
    "\n",
    "# bench2p\n",
    "# sessi = sess9FH37WBZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench2p figures and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a key to be used across multiple tables\n",
    "\n",
    "scansi = \"scan9FKSN4XD\"\n",
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')\n",
    "sessi = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('session_id')[0]\n",
    "aux_setup_typestr = (scan.ScanInfo() & scan_key).fetch(\"userfunction_info\")[0] # check setup type (not needed)\n",
    "print(aux_setup_typestr)\n",
    "print((scan.ScanPath & scan_key).fetch(\"path\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session & 'subject = \"ROS-1604\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and show overview images from suite2p registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('ref_image')\n",
    "average_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('average_image')\n",
    "correlation_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('correlation_image')\n",
    "max_proj_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('max_proj_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some scaling values from pixel distribution\n",
    "scalemin = 1\n",
    "scalemax = 100\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin)  \n",
    "cmax = np.percentile(average_image,scalemax)\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "# Make figure with all templates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(ref_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Reference Image for Registration\");\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Registered Image, Mean Projection\");\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(max_proj_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(correlation_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Correlation Map\")\n",
    "plt.show(block=False)\n",
    "# plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just mean image\n",
    "scalemin = 1\n",
    "scalemax = 100\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin)  \n",
    "cmax = np.percentile(average_image,scalemax)\n",
    "\n",
    "\n",
    "# plt.subplot(1, 4, 1)\n",
    "plt.imshow(max_proj_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot treadmill data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract scan treadmill data from database using the scan_key from above\n",
    "treadmill = (behavior.TreadmillRecording.Channel() & scan_key).fetch(\"data\")[0]\n",
    "auxtime = (behavior.TreadmillRecording.Channel() & scan_key).fetch(\"time\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing window size (ms)\n",
    "window = 5000\n",
    "\n",
    "# convert voltage to degree\n",
    "treadmillnorm = (treadmill-np.min(treadmill)) / np.max(treadmill) * 360\n",
    "\n",
    "# compute running speed (see function above)\n",
    "angular_velocity_smoothed, unwrapped_angle_smoothed = ibe.compute_angular_velocity(auxtime, treadmillnorm, window)\n",
    "\n",
    "# get some values to scale running speed plot \n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "offset = 10\n",
    "ymin = np.percentile(angular_velocity_smoothed,scalemin)  - offset\n",
    "ymax = np.percentile(angular_velocity_smoothed,scalemax)  + offset\n",
    "\n",
    "# load plot styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15, 10))\n",
    "# plt.rcParams['agg.path.chunksize'] = 10000  # Add this line if it does not rende\n",
    "\n",
    "# Plotting the time series\n",
    "axes[0].plot(auxtime, treadmillnorm)\n",
    "axes[0].set_ylim([-10, 370])\n",
    "axes[0].set_ylabel(\"Wheel position \\n[degree]\")\n",
    "axes[0].set_xlabel(\"Time [s]\")\n",
    "\n",
    "axes[1].plot(auxtime[:-window+1],unwrapped_angle_smoothed )\n",
    "# axes[1].set_ylim([-10000, 10000])\n",
    "axes[1].set_ylabel(\"Unwrapped wheel position \\n[cumulative degree]\")\n",
    "axes[1].set_xlabel(\"Time [s]\")\n",
    "\n",
    "axes[2].plot(auxtime[:-window],angular_velocity_smoothed)\n",
    "axes[2].set_ylim([ymin, ymax])\n",
    "axes[2].set_ylabel(\"Running speed \\n[degree / s]\")\n",
    "axes[2].set_xlabel(\"Time [s]\")\n",
    "\n",
    "fig.suptitle(scan_key[\"scan_id\"], fontsize=16)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the fluorescence traces of this recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask positions of masks that are classified as cells and that are larger than a certain pixel size\n",
    "mask_xpix, mask_ypix = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & scan_key\n",
    "    & \"mask_npix > 1\"\n",
    ").fetch(\"mask_xpix\", \"mask_ypix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this query, we've fetched the coordinates of segmented masks. We can overlay these\n",
    "masks onto our average image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_image = np.zeros(np.shape(average_image), dtype=bool)\n",
    "for xpix, ypix in zip(mask_xpix, mask_ypix):\n",
    "    mask_image[ypix, xpix] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.contour(mask_image, colors=\"red\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example using queries - plot fluorescence and deconvolved activity\n",
    "traces:\n",
    "\n",
    "Here we fetch the primary key attributes of the entry with `curation_id=0` for the\n",
    "current session in the `imaging.Curation` table. \n",
    "\n",
    "Then, we fetch all cells that fit the\n",
    "restriction criteria from `imaging.Segmentation.Mask` and\n",
    "`imaging.MaskClassification.MaskType` as a `projection`. \n",
    "\n",
    "We then use this projection as\n",
    "a restriction to fetch and plot fluorescence and deconvolved activity traces from the\n",
    "`imaging.Fluorescence.Trace` and `imaging.Activity.Trace` tables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key = (imaging.Curation & scan_key & \"curation_id=1\").fetch1(\"KEY\")\n",
    "query_cells = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & curation_key\n",
    "    & \"mask_center_z=0\"\n",
    "    & \"mask_npix > 1\"\n",
    ").proj()\n",
    "\n",
    "# query_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropilcorr = True\n",
    "\n",
    "fluorescence_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "neuropil_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"neuropil_fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "if neuropilcorr:\n",
    "    print(\"DOING VANILLA NEUROPIL CORRECTION NOW!\")\n",
    "    fluorescence_traces = fluorescence_traces - 0.7 * neuropil_traces\n",
    "\n",
    "activity_traces = (imaging.Activity.Trace & query_cells).fetch(\n",
    "    \"activity_trace\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "sampling_rate = (scan.ScanInfo & curation_key).fetch1(\"fps\")\n",
    "\n",
    "# timebase_2p = np.r_[: fluorescence_traces[0].size] * 1 / sampling_rate\n",
    "\n",
    "timebase_2p = np.linspace(0, fluorescence_traces[0].size / sampling_rate, fluorescence_traces[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap import Rastermap\n",
    "from scipy import stats \n",
    "from scipy.stats import zscore\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# stack fluorescence for rastermap\n",
    "fluos = np.vstack(fluorescence_traces)\n",
    "\n",
    "nan_mask = np.isnan(fluos).any(axis=1)\n",
    "S = fluos[~nan_mask]\n",
    "S = zscore(S, axis=1)\n",
    "\n",
    "rmmodel = Rastermap(n_clusters=None, # None turns off clustering and sorts single neurons \n",
    "                  n_PCs=100, # use fewer PCs than neurons\n",
    "                  locality=0.15, # some locality in sorting (this is a value from 0-1)\n",
    "                  time_lag_window=15, # use future timepoints to compute correlation\n",
    "                  grid_upsample=0, # 0 turns off upsampling since we're using single neurons\n",
    "                ).fit(S)\n",
    "\n",
    "\n",
    "y = rmmodel.embedding # neurons x 1\n",
    "isort = rmmodel.isort\n",
    "\n",
    "# sort by embedding and smooth over neurons (uncomment)\n",
    "\n",
    "# Sfilt = gaussian_filter1d(S[isort], np.minimum(1,np.maximum(1,int(S.shape[0]*0.001))),axis=0)\n",
    "Sfilt = S[isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sorted data\n",
    "# load plot styles for display\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(Sfilt, vmin = -0.1, vmax=1, extent= [timebase_2p[0], timebase_2p[-1], 0, Sfilt.shape[0]], aspect='auto', cmap='gray_r')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('sorted boutons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "offset_scaler = 20 # We want to plot every cell with a little offset to the last one\n",
    "for no,trace in enumerate(Sfilt):\n",
    "    if no == 80: break # not more than 80\n",
    "\n",
    "    # get the neuropil corrected values for that trace:\n",
    "    # trace = Sfilt\n",
    "    ax.plot(timebase_2p,trace + (no*offset_scaler),lw=1,c='k',alpha=.8)\n",
    "\n",
    "ax.set_xlim(0,timebase_2p[-1])\n",
    "\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.set_title('Sorted z-scored traces')    \n",
    "\n",
    "ax.set_ylabel('Cells')\n",
    "ax.set_xlabel('Time [s]')\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='bench2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "aligned_wheel_indices = get_closest_timestamps(twoptimestamps,auxtime[:-window]) #smoothing windwo from above\n",
    "\n",
    "# use this to index into the wheelspeed\n",
    "angular_velocity_smoothed_2pref = angular_velocity_smoothed[aligned_wheel_indices]\n",
    "\n",
    "# both arrays have same shape now - now 2pdata and wheel speed can be plotted together on the 2ptimestamps\n",
    "print(np.shape(twoptimestamps))\n",
    "print(np.shape(angular_velocity_smoothed_2pref))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_colors = np.array([[0.55,0.55,0.55]])\n",
    "\n",
    "\n",
    "# timepoints to visualize\n",
    "tstart = 0\n",
    "tend =  timebase_2p[-1] - 150\n",
    "\n",
    "xmin = int(np.floor(tstart * sampling_rate))\n",
    "xmax = int(np.floor(tend * sampling_rate))\n",
    "\n",
    "# make figure with grid for easy plotting\n",
    "fig = plt.figure(figsize=(14,5), dpi=200)\n",
    "grid = plt.GridSpec(9, 20, figure=fig, wspace = 0.05, hspace = 0.3)\n",
    "\n",
    "# plot running speed\n",
    "ax = plt.subplot(grid[:2, :-1])\n",
    "ax.plot(angular_velocity_smoothed_2pref,  color=kp_colors[0])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "\n",
    "ax.tick_params(left=False, right=False, bottom=False, top=False,\n",
    "               labelleft=False, labelbottom=False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "ax.set_ylabel(\"treadmill\\nspeed\")\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "# ax.axis(\"off\")\n",
    "\n",
    "\n",
    "# ax.set_title(\"running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "# plot superneuron activity\n",
    "ax = plt.subplot(grid[2:, :-1])\n",
    "ax.imshow(Sfilt[:, xmin:xmax], cmap=\"gray_r\", vmin=-0.1, vmax=0.7,  extent= [timebase_2p[xmin], timebase_2p[xmax], 0, Sfilt.shape[0]], aspect=\"auto\")\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"sorted boutons\")\n",
    "\n",
    "plt.show()\n",
    "plt.savefig\n",
    "# ax = plt.subplot(grid[1:, -1])\n",
    "# ax.imshow(np.arange(0, len(sn))[:,np.newaxis], cmap=\"gist_ncar\", aspect=\"auto\")\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini2p figures and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a key to be used across multiple tables\n",
    "\n",
    "scansi = \"scan9FKSMSMZ\"\n",
    "scan_key = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('KEY')[0]\n",
    "curation_key = (imaging.Curation & scan_key & 'curation_id=1').fetch1('KEY')\n",
    "sessi = (scan.Scan & f'scan_id = \"{scansi}\"').fetch('session_id')[0]\n",
    "aux_setup_typestr = (scan.ScanInfo() & scan_key).fetch(\"userfunction_info\")[0] # check setup type (not needed)\n",
    "print(aux_setup_typestr)\n",
    "print((scan.ScanPath & scan_key).fetch(\"path\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and show overview images from suite2p registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('ref_image')\n",
    "average_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('average_image')\n",
    "correlation_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('correlation_image')\n",
    "max_proj_image = (imaging.MotionCorrection.Summary & curation_key & 'field_idx=0').fetch1('max_proj_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some scaling values from pixel distribution\n",
    "scalemin = 0\n",
    "scalemax = 100\n",
    "offset = 150\n",
    "\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin)  \n",
    "cmax = np.percentile(average_image,scalemax) + offset\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "# Make figure with all templates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(ref_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Reference Image for Registration\");\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.title(\"Registered Image, Mean Projection\");\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(max_proj_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Max Projection\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(correlation_image, cmap='gray')\n",
    "plt.title(\"Registered Image, Correlation Map\")\n",
    "plt.show(block=False)\n",
    "# plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just mean image\n",
    "scalemin = 5\n",
    "scalemax = 100\n",
    "\n",
    "cmin = np.percentile(average_image,scalemin) \n",
    "cmax = np.percentile(average_image,scalemax) + offset\n",
    "\n",
    "\n",
    "# plt.subplot(1, 4, 1)\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make cam-synchronized movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import skvideo.io\n",
    "\n",
    "# get the movie file name from the database\n",
    "topfile = (model.VideoRecording.File & scan_key).fetch('file_path')[0]\n",
    "\n",
    "#load to array\n",
    "videodata = skvideo.io.vread(str(topfile))\n",
    "# videodata = np.asarray([skvideo.io.vshape(frame)[0] for frame in videodata], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make DLC overlay video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.VideoRecording & scan_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEPLABCUT OVERLAY - CURRENTLY ONLY WORKING IN MY ENVIRONMENT. NEED TO CHECK VERSIONS\n",
    "dlc_scan_key = (model.PoseEstimation & f'recording_id = \"{scan_key[\"scan_id\"]}\"').fetch('KEY')[0] \n",
    "key = (model.VideoRecording & dlc_scan_key).fetch1('KEY')\n",
    "key.update({'model_name': 'Head_orientation-NK', 'task_mode': 'trigger'})\n",
    "\n",
    "\n",
    "from deeplabcut.utils.make_labeled_video import create_labeled_video\n",
    "import yaml\n",
    "from element_interface.utils import find_full_path\n",
    "from adamacs.paths import get_dlc_root_data_dir\n",
    "\n",
    "\n",
    "destfolder = model.PoseEstimationTask.infer_output_dir(dlc_scan_key)\n",
    "\n",
    "config_paths = sorted( # Of configs in the project path, defer to the datajoint-saved\n",
    "    list(\n",
    "        find_full_path(\n",
    "            get_dlc_root_data_dir(), ((model.Model & key).fetch1(\"project_path\"))\n",
    "        ).glob(\"*.y*ml\")\n",
    "    )\n",
    ")\n",
    "\n",
    "create_labeled_video( # Pass strings to label the video\n",
    "    config=str(config_paths[-1]),\n",
    "    videos=str(topfile),\n",
    "    destfolder=str(destfolder),\n",
    ")\n",
    "\n",
    "pattern = '*resnet*.mp4'\n",
    "files = list(directory.glob(pattern))\n",
    "\n",
    "# labeled_videofile = '/datajoint-data/data/tobiasr/JJ_WEZ-8873_2023-07-30_scan9FJBPNSP_sess9FJBPNSP/device_mini2p1_top_recording_scan9FJBPNSP_model_Head_orientation-NK/scan9FJBPNSP_top_video_2023-07-30T17_38_30DLC_resnet50_Head_orientationJul17shuffle1_90000_labeled.mp4'\n",
    "\n",
    "labeled_videodata = skvideo.io.vread(str(files[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import skvideo.io\n",
    "from deeplabcut.utils.make_labeled_video import create_labeled_video\n",
    "import yaml\n",
    "from element_interface.utils import find_full_path\n",
    "from adamacs.paths import get_dlc_root_data_dir\n",
    "\n",
    "dlc_scan_key = (model.PoseEstimation & f'recording_id = \"{scan_key[\"scan_id\"]}\"').fetch('KEY')[0] \n",
    "destfolder = model.PoseEstimationTask.infer_output_dir(dlc_scan_key)\n",
    "\n",
    "directory = Path(destfolder)\n",
    "pattern = '*resnet*.mp4'\n",
    "files = list(directory.glob(pattern))\n",
    "labeled_videodata = skvideo.io.vread(str(files[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load moving average registered Ca2+ imaging movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scandir = (scan.ScanPath & scan_key).fetch('path')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the registered moving average (blinking) movie data of the specified scanID\n",
    "\n",
    "scandir = (scan.ScanPath & scan_key).fetch('path')[0]\n",
    "\n",
    "directory = Path(scandir + \"/suite2p/plane0/reg_tif\")\n",
    "pattern = '*.mp4'\n",
    "files = list(directory.glob(pattern))\n",
    "blinkvideodata = skvideo.io.vread(str(files[0]))\n",
    "blinkvideodata = np.asarray([skvideo.io.vshape(frame)[0] for frame in blinkvideodata], dtype=np.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display camaravideo with slider\n",
    "sh.display_volume_z(videodata,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 2pstackvideo with slider\n",
    "sh.display_volume_z(blinkvideodata,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 2pstackvideo with slider\n",
    "sh.display_volume_z(labeled_videodata,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionts of the original movie (frames, x,y,rgb)\n",
    "print(videodata.shape)\n",
    "print(blinkvideodata.shape)\n",
    "print(labeled_videodata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " event.Event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the timestamp data and gate / offset cameraframes\n",
    "\n",
    "# from the event table get the main recording gate start / end timestamps.\n",
    "auxgatetimestamp_end = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_end_time')\n",
    "auxgatetimestamp_start = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# Then return camera start timestamps within the recording gate only \n",
    "cameratimestamps = (event.Event()  &  \"event_type='aux_cam'\" & f\"event_start_time>{auxgatetimestamp_start[0]}\" & f\"event_start_time<{auxgatetimestamp_end[0]}\" & scan_key).fetch('event_start_time')\n",
    "\n",
    "#  and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='mini2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoptimestamps =  twoptimestamps[:np.shape(blinkvideodata)[0]] # truncating 2p timestamps to number of 2p videoframes\n",
    "\n",
    "#  Zero camera timestamps on first 2p timestamp. (not necessary)\n",
    "# cameratimestamps = cameratimestamps - twoptimestamps[0]\n",
    "# twoptimestamps = twoptimestamps - twoptimestamps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the two recordings by finding the indices of the closest camera timestamp that fits the 2p frame timestamps by sorted list insertion (\"bisect\"). Be aware: camera frames can be double.\n",
    "aligned_cameraframes = get_closest_timestamps(twoptimestamps,cameratimestamps)\n",
    "\n",
    "# this should have the same shape as the 2p frames:\n",
    "print(np.shape(aligned_cameraframes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now use this camara frame indices to reslice the video (which now is aligned to the 2p frames on a frame-by-frame level)\n",
    "# resliced_cam_video = videodata[aligned_cameraframes]\n",
    "resliced_cam_video = labeled_videodata[aligned_cameraframes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display synchronized movie\n",
    "sh.display_volume_z(resliced_cam_video,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale camera movie to fit size of 2p movie (can take a lot of time and memory)\n",
    "rescaled_cam_movie = resize_movie(resliced_cam_video, np.shape(blinkvideodata)[1],np.shape(blinkvideodata)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(rescaled_cam_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate and display movies\n",
    "concatmovie = np.concatenate((blinkvideodata,rescaled_cam_movie), axis = 2)\n",
    "sh.display_volume_z(concatmovie,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as new movie (without rescaling)\n",
    "\n",
    "\n",
    "filename = str(directory) + '/aligned_stack_cam_movie.mp4'\n",
    "fps = (scan.ScanInfo & scan_key).fetch('fps')\n",
    "# p1 = 0\n",
    "# p2 = 100\n",
    "# trash = sh.make_stack_movie(concatmovie, filename, fps[0], p1, p2)\n",
    "\n",
    "codecset = 'libx264'\n",
    "import imageio\n",
    "import imageio.plugins.ffmpeg as ffmpeg\n",
    "\n",
    "# Create an imageio VideoWriter object to write the video\n",
    "writer = imageio.get_writer(filename, fps=fps[0], codec=codecset, output_params=['-crf', '19'])\n",
    "\n",
    "# # Calculate the 1st and 99th percentile\n",
    "# p1, p99 = np.percentile(running_z_projection[:500,:,:], (p1set, p2set))\n",
    "\n",
    "# # rescale to 8 bit\n",
    "# rescaled_image_8bit = rescale_image_multithreaded(running_z_projection, p1, p99)\n",
    "\n",
    "for page in concatmovie:\n",
    "    writer.append_data(page)\n",
    "\n",
    "# Close the video writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed up, add timestamps etc - all with fast ffmpeg operations\n",
    "\n",
    "import os\n",
    "\n",
    "spedby = 5\n",
    "setpts_value = 1/spedby # change this to your desired value\n",
    "newfps = fps[0]*spedby\n",
    "\n",
    "input_filename = filename\n",
    "# 2. Add timestamps\n",
    "\n",
    "output_filename = str(directory) + '/' + scansi + '_top_video_concatenated' + 'withtimestamps.mp4'\n",
    "command = f\"\"\"ffmpeg -y -i {input_filename} -vf \"drawtext=fontfile=/Library/Fonts/Arial.ttf:timecode='00\\\\\\\\:00\\\\\\\\:00\\\\\\\\:00':rate={fps[0]}:text='':fontsize=15:fontcolor=white:x=260:y=10:box=1:boxcolor=0x00000000@1\" -f mp4 {output_filename}\"\"\"\n",
    "\n",
    "os.system(command)\n",
    "\n",
    "\n",
    "input_filename = output_filename  # 'sped_up_video.mp4'\n",
    "output_filename = str(directory) + '/' +  scansi + '_top_video_concatenated_spedup_' + str(spedby) + 'fold_withtimestamps.mp4'\n",
    "\n",
    "command = f'ffmpeg -y -i {input_filename} -vf \"setpts={setpts_value}*PTS\" -r {newfps}  {output_filename}'\n",
    "\n",
    "\n",
    "\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the fluorescence traces of this recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask positions of masks that are classified as cells and that are larger than a certain pixel size\n",
    "mask_xpix, mask_ypix = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & scan_key\n",
    "    & \"mask_npix > 1\"\n",
    "    & \"curation_id = 1\"\n",
    ").fetch(\"mask_xpix\", \"mask_ypix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this query, we've fetched the coordinates of segmented masks. We can overlay these\n",
    "masks onto our average image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_image = np.zeros(np.shape(average_image), dtype=bool)\n",
    "for xpix, ypix in zip(mask_xpix, mask_ypix):\n",
    "    mask_image[ypix, xpix] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(img_params)\n",
    "\n",
    "plt.imshow(average_image, cmap='gray', vmin = cmin, vmax = cmax)\n",
    "plt.contour(mask_image, colors=\"red\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Normalize the average_image to [0, 1]\n",
    "average_image = (average_image - average_image.min()) / (average_image.max() - average_image.min())\n",
    "\n",
    "# Determine the number of masks\n",
    "num_masks = len(mask_xpix)\n",
    "\n",
    "# Generate random colormap using HSV\n",
    "np.random.seed(42)  # For reproducibility, remove this line for truly random colors every time\n",
    "hues = np.random.rand(num_masks)\n",
    "colors = [mcolors.hsv_to_rgb([hue, 1, 1]) for hue in hues]\n",
    "\n",
    "# # Create a color overlay using a perceptually uniform colormap\n",
    "# colormap = plt.cm.viridis  # You can also use 'plasma', 'cividis', etc.\n",
    "# colors = [colormap(i) for i in np.linspace(0, 1, num_masks)]\n",
    "\n",
    "\n",
    "# Start with the grayscale image as the base\n",
    "color_overlay = np.repeat(average_image[..., np.newaxis], 3, axis=2)\n",
    "\n",
    "# Define an alpha factor for overall translucency (e.g., 0.5 for 50% transparency)\n",
    "alpha_factor = 0.18\n",
    "\n",
    "# Loop through masks, add color overlays\n",
    "for (xpix, ypix), color in zip(zip(mask_xpix, mask_ypix), colors):\n",
    "    for i in range(3):  # R, G, B channels\n",
    "        color_overlay[ypix, xpix, i] = color_overlay[ypix, xpix, i] * (1 - alpha_factor) + color[i] * alpha_factor\n",
    "\n",
    "# Display using Matplotlib\n",
    "plt.imshow(color_overlay)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more example using queries - plot fluorescence and deconvolved activity\n",
    "traces:\n",
    "\n",
    "Here we fetch the primary key attributes of the entry with `curation_id=0` for the\n",
    "current session in the `imaging.Curation` table. \n",
    "\n",
    "Then, we fetch all cells that fit the\n",
    "restriction criteria from `imaging.Segmentation.Mask` and\n",
    "`imaging.MaskClassification.MaskType` as a `projection`. \n",
    "\n",
    "We then use this projection as\n",
    "a restriction to fetch and plot fluorescence and deconvolved activity traces from the\n",
    "`imaging.Fluorescence.Trace` and `imaging.Activity.Trace` tables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_key = (imaging.Curation & scan_key & \"curation_id=1\").fetch1(\"KEY\")\n",
    "query_cells = (\n",
    "    imaging.Segmentation.Mask * imaging.MaskClassification.MaskType\n",
    "    & curation_key\n",
    "    & \"mask_center_z=0\"\n",
    "    & \"mask_npix > 1\"\n",
    ").proj()\n",
    "\n",
    "# query_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropilcorr = True\n",
    "\n",
    "fluorescence_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "neuropil_traces = (imaging.Fluorescence.Trace & query_cells).fetch(\n",
    "    \"neuropil_fluorescence\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "if neuropilcorr:\n",
    "    print(\"DOING VANILLA NEUROPIL CORRECTION NOW!\")\n",
    "    fluorescence_traces = fluorescence_traces - 0.7 * neuropil_traces\n",
    "\n",
    "activity_traces = (imaging.Activity.Trace & query_cells).fetch(\n",
    "    \"activity_trace\", order_by=\"mask\"\n",
    ")\n",
    "\n",
    "sampling_rate = (scan.ScanInfo & curation_key).fetch1(\"fps\")\n",
    "\n",
    "# timebase_2p = np.r_[: fluorescence_traces[0].size] * 1 / sampling_rate\n",
    "\n",
    "timebase_2p = np.linspace(0, fluorescence_traces[0].size / sampling_rate, fluorescence_traces[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap import Rastermap\n",
    "from scipy import stats \n",
    "from scipy.stats import zscore\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# stack fluorescence for rastermap\n",
    "fluos = np.vstack(fluorescence_traces)\n",
    "\n",
    "nan_mask = np.isnan(fluos).any(axis=1)\n",
    "S = fluos[~nan_mask]\n",
    "S = zscore(S, axis=1)\n",
    "\n",
    "rmmodel = Rastermap(n_clusters=None, # None turns off clustering and sorts single neurons \n",
    "                  n_PCs=80, # use fewer PCs than neurons\n",
    "                  locality=0.15, # some locality in sorting (this is a value from 0-1)\n",
    "                  time_lag_window=15, # use future timepoints to compute correlation\n",
    "                  grid_upsample=0, # 0 turns off upsampling since we're using single neurons\n",
    "                ).fit(S)\n",
    "\n",
    "\n",
    "y = rmmodel.embedding # neurons x 1\n",
    "isort = rmmodel.isort\n",
    "\n",
    "# sort by embedding and smooth over neurons (uncomment)\n",
    "\n",
    "# Sfilt = gaussian_filter1d(S[isort], np.minimum(1,np.maximum(1,int(S.shape[0]*0.001))),axis=0)\n",
    "Sfilt = S[isort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sorted data\n",
    "# load plot styles for display\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(Sfilt, vmin = -0.1, vmax=1, extent= [timebase_2p[0], timebase_2p[-1], 0, Sfilt.shape[0]], aspect='auto', cmap='gray_r')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('sorted neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarized event visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "figure = plt.figure(figsize=(5,6))\n",
    "ax = figure.add_subplot(111)\n",
    "\n",
    "thresh = 4\n",
    "event_matrix = np.array([item > thresh for item in Sfilt], dtype=bool).astype(int)\n",
    "\n",
    "\n",
    "offset_scaler = 1.5 # We want to plot every cell with a little offset to the last one\n",
    "for no,trace in enumerate(event_matrix):\n",
    "    if no == 80: break # not more than 80\n",
    "\n",
    "    # get the neuropil corrected values for that trace:\n",
    "    # trace = Sfilt\n",
    "    ax.plot(timebase_2p,trace + (no*offset_scaler),lw=1,c='k',alpha=.8)\n",
    "    # ax.plot(timebase_2p,trace + (no*offset_scaler),lw=1,c='r',alpha=.8)\n",
    "\n",
    "\n",
    "ax.set_xlim(0,timebase_2p[-1])\n",
    "\n",
    "ax.get_yaxis().set_ticks([])\n",
    "ax.set_title('Sorted binarized z-scored traces - very preliminary')    \n",
    "\n",
    "ax.set_ylabel('Cells')\n",
    "ax.set_xlabel('Time [s]')\n",
    "sns.despine(left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the timestamp data and gate / offset cameraframes\n",
    "\n",
    "# from the event table get the main recording gate start / end timestamps.\n",
    "auxgatetimestamp_end = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_end_time')\n",
    "auxgatetimestamp_start = (event.Event()  &  \"event_type='main_track_gate'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# Then return camera start timestamps within the recording gate only \n",
    "cameratimestamps = (event.Event()  &  \"event_type='aux_cam'\" & f\"event_start_time>{auxgatetimestamp_start[0]}\" & f\"event_start_time<{auxgatetimestamp_end[0]}\" & scan_key).fetch('event_start_time')\n",
    "\n",
    "#  and 2p timestamps (which will always be in the recording gate).\n",
    "twoptimestamps = (event.Event()  &  \"event_type='mini2p_frames'\" &  scan_key ).fetch('event_start_time')\n",
    "\n",
    "# align the two recordings by finding the indices of the closest camera timestamp that fits the 2p frame timestamps by sorted list insertion (\"bisect\"). Be aware: camera frames can be double.\n",
    "aligned_cameraframes = get_closest_timestamps(twoptimestamps,cameratimestamps)\n",
    "\n",
    "# this should have the same shape as the 2p frames:\n",
    "print(np.shape(aligned_cameraframes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the timestamps of the video synchronization from above are the one to use for synchronized plotting of positions etc: aligned_cameraframes\n",
    "print(np.shape(cameratimestamps))\n",
    "print(np.shape(aligned_cameraframes))\n",
    "print(np.shape(twoptimestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now do some positional plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc_scan_key = (model.PoseEstimation & f'recording_id = \"{scan_key[\"scan_id\"]}\"').fetch('KEY')[0] \n",
    "path = (model.VideoRecording.File & scan_key).fetch(\"file_path\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce dataframe to xy coordinates\n",
    "\n",
    "df=model.PoseEstimation.get_trajectory(dlc_scan_key)\n",
    "df_xy = df.iloc[:,df.columns.get_level_values(2).isin([\"x\",\"y\"])]['Head_orientation-NK']\n",
    "# df_xy.mean()\n",
    "# df_xy\n",
    "df_xy.plot().legend(loc='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat = df_xy.copy()\n",
    "df_flat.columns = df_flat.columns.map('_'.join)\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "df_flat.plot(x='body_middle_x',y='body_middle_y',ax=ax)\n",
    "df_flat.plot(x='head_middle_x',y='head_middle_y', ax=ax)\n",
    "df_flat.plot(x='tail_x',y='tail_y', ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.title(scan_key)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot events over position\n",
    "\n",
    "position = df_flat[['body_middle_x', 'body_middle_y']].values\n",
    "position = position[aligned_cameraframes].T/10 # synchronize to 2pframes and translate for opexebo - THIS IS A GUESSTIMATE!  pretending 1px = 1mm NEEDS CALIBRATION - tracking needs to be in xy real-world coordinates (in cm)\n",
    "\n",
    "\n",
    "total_cells = np.shape(event_matrix)[0] # Change this to the desired number of cells\n",
    "\n",
    "# Determine the grid dimensions (for a roughly square arrangement)\n",
    "nrows = int(np.ceil(np.sqrt(total_cells)))\n",
    "ncols = int(np.ceil(total_cells / nrows))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 12))\n",
    "fig.subplots_adjust(hspace=0.2) # Add some space between the subplots\n",
    "\n",
    "# If axs is not already a 2D array (e.g., if total_cells is a perfect square), make it one\n",
    "if total_cells != nrows * ncols:\n",
    "    axs = axs.reshape(-1)\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(map_params)\n",
    "\n",
    "for cell in range(total_cells):\n",
    "    ax = axs[cell]\n",
    "\n",
    "    # Plotting the line plot first\n",
    "    ax.plot(position[0], position[1], color='grey')\n",
    "\n",
    "    # spike events at position\n",
    "    spikes_at_pos = np.vstack((position[0, event_matrix[cell].astype(\"bool\")], position[1, event_matrix[cell].astype(\"bool\")]))\n",
    "    \n",
    "    # Then plotting the scatter plot so that it's on top of the line\n",
    "    ax.scatter(spikes_at_pos[0], spikes_at_pos[1], color='red', zorder=2)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(scan_key[\"scan_id\"] + \" - Cell \" + str(cell+1))\n",
    "\n",
    "# Remove any extra subplots\n",
    "for cell in range(total_cells, nrows * ncols):\n",
    "    fig.delaxes(axs[cell])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make masked spatial occupancy map - OPEXEBO\n",
    "\n",
    "import opexebo\n",
    "\n",
    "arena_size = 100 # in cm - NEEDS CALIBRATED TRACKING COORDS!\n",
    "arena_shape = \"circle\"\n",
    "bin_width =  4 # cm\n",
    "\n",
    "masked_occupancy_map, coverage, bin_edges = opexebo.analysis.spatial_occupancy(timebase_2p, position, arena_size, arena_shape = arena_shape, bin_width = bin_width)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.imshow(masked_occupancy_map)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('time / bin [s]')\n",
    "plt.title(f'spatial occupancy - {scan_key[\"scan_id\"]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spike events at position\n",
    "spikes_at_pos = np.vstack((position[0, event_matrix[cell].astype(\"bool\")], position[1, event_matrix[cell].astype(\"bool\")]))\n",
    "\n",
    "# time at postiion\n",
    "time_at_pos = (timebase_2p[event_matrix[cell].astype(\"bool\")])\n",
    "\n",
    "# spikes_tracking [t,x,y]\n",
    "spikes_tracking = np.vstack((time_at_pos, spikes_at_pos))\n",
    "\n",
    "# make ratemap\n",
    "rate_map = opexebo.analysis.rate_map(masked_occupancy_map, spikes_tracking, arena_size, arena_shape = arena_shape, bin_width = bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rate maps - OPEXEBO\n",
    "\n",
    "# Determine the grid dimensions (for a roughly square arrangement)\n",
    "nrows = int(np.ceil(np.sqrt(total_cells)))\n",
    "ncols = int(np.ceil(total_cells / nrows))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 12))\n",
    "fig.subplots_adjust(hspace=0.2) # Add some space between the subplots\n",
    "\n",
    "# If axs is not already a 2D array (e.g., if total_cells is a perfect square), make it one\n",
    "if total_cells != nrows * ncols:\n",
    "    axs = axs.reshape(-1)\n",
    "\n",
    "# load image styles for display\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(map_params)\n",
    "\n",
    "for cell in range(total_cells):\n",
    "    ax = axs[cell]\n",
    "\n",
    "    # spike events at position\n",
    "    spikes_at_pos = np.vstack((position[0, event_matrix[cell].astype(\"bool\")], position[1, event_matrix[cell].astype(\"bool\")]))\n",
    "    \n",
    "    # time at postiion\n",
    "    time_at_pos = (timebase_2p[event_matrix[cell].astype(\"bool\")])\n",
    "    \n",
    "    # spikes_tracking [t,x,y]\n",
    "    spikes_tracking = np.vstack((time_at_pos, spikes_at_pos))\n",
    "    \n",
    "    # make ratemap\n",
    "    rate_map = opexebo.analysis.rate_map(masked_occupancy_map, spikes_tracking, arena_size, arena_shape = arena_shape, bin_width = bin_width)\n",
    "    \n",
    "    # Then plotting the scatter plot so that it's on top of the line\n",
    "    im = ax.imshow(rate_map)\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(scan_key[\"scan_id\"] + \" - \" + str(cell+1))\n",
    "    # cbar = plt.colorbar(im, ax=ax) # Pass the image object and the ax to plt.colorbar\n",
    "    # cbar.set_label('events / bin [s]')\n",
    "\n",
    "# Remove any extra subplots\n",
    "for cell in range(total_cells, nrows * ncols):\n",
    "    fig.delaxes(axs[cell])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speed tuning - freely moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get running speed - OPEXEBO\n",
    "new_speed = opexebo.analysis.calc_speed(timebase_2p, position[0], position[1], moving_average = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "mpl.rcParams.update(plot_params)\n",
    "\n",
    "# get some scaling values from pixel distribution\n",
    "scalemin = 0\n",
    "scalemax = 99.7\n",
    "\n",
    "cmin = np.percentile(new_speed,scalemin)  \n",
    "cmax = np.percentile(new_speed,scalemax)\n",
    "\n",
    "kp_colors = np.array([[0.55,0.55,0.55]])\n",
    "\n",
    "\n",
    "# timepoints to visualize\n",
    "tstart = 0\n",
    "tend =  timebase_2p[-1] - 150\n",
    "\n",
    "xmin = int(np.floor(tstart * sampling_rate))\n",
    "xmax = int(np.floor(tend * sampling_rate))\n",
    "\n",
    "# make figure with grid for easy plotting\n",
    "fig = plt.figure(figsize=(8,5), dpi=200)\n",
    "grid = plt.GridSpec(9, 20, figure=fig, wspace = 0.05, hspace = 0.3)\n",
    "\n",
    "# plot running speed\n",
    "ax = plt.subplot(grid[:2, :-1])\n",
    "ax.plot(new_speed,  color=kp_colors[0])\n",
    "ax.set_xlim([0, xmax-xmin])\n",
    "ax.set_ylim([cmin, cmax])\n",
    "\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"freely moving running speed\", color=kp_colors[0])\n",
    "# ax.set_xlabel(\"running speed\")\n",
    "\n",
    "\n",
    "# plot superneuron activity\n",
    "ax = plt.subplot(grid[2:, :-1])\n",
    "ax.imshow(Sfilt[:, xmin:xmax], cmap=\"gray_r\", vmin=-0.1, vmax=0.8,  extent= [timebase_2p[xmin], timebase_2p[xmax], 0, Sfilt.shape[0]], aspect=\"auto\")\n",
    "ax.set_xlabel(\"time [s]\")\n",
    "ax.set_ylabel(\"sorted cells\")\n",
    "\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()\n",
    "\n",
    "# ax = plt.subplot(grid[1:, -1])\n",
    "# ax.imshow(np.arange(0, len(sn))[:,np.newaxis], cmap=\"gist_ncar\", aspect=\"auto\")\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datajoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
